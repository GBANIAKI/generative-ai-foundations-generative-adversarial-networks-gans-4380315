{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "# For this example we will use pytorch to manage the construction of the neural networks and the training\n",
    "# torchvision is a module that is part of pytorch that supports vision datasets and it will be where we will source the mnist - handwritten digits - data\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f43eef95a30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a seed will determine which data elements are selected. To replicate results keep the same seed.\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a check if there is a gpu available for training. At the moment we are assuming that it is not available.\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the GPU is not available means we will set the device to cpu and set up some parameters\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\")\n",
    "#This is the width of the latent space matrix\n",
    "nz = 100\n",
    "# This is the generator matrix shape\n",
    "ngf = 64\n",
    "# This is the descrimator matrix shape\n",
    "ndf = 64\n",
    "# This is the number of color channels - other datasets may have 3 if they are color\n",
    "nc = 1\n",
    "# The number of sample to process per pass\n",
    "batch_size = 64\n",
    "# the number of CPU workers to work on the dataset\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root='data', download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5,), (0.5,)),\n",
    "                      ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "# The weights will need to be initialised based on the layer type to some value before training. These could be imported from past training steps.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Generator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Discrimator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function from pytorches established modules\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Set up the initial noise of the latent space to sample from.\n",
    "# Set the label of a real and fake sample to 0,1\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Create the optimiser which will dynamically change the parameters of the learning function over time to imporve the training process\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0005, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/938] Loss_D: 3.6217 Loss_G: 0.4947 D(x): 0.0826 D(G(z)): 0.0081 / 0.7190\n",
      "[0/10][1/938] Loss_D: 2.0329 Loss_G: 3.1704 D(x): 0.9794 D(G(z)): 0.7354 / 0.1071\n",
      "[0/10][2/938] Loss_D: 0.7622 Loss_G: 4.0870 D(x): 0.7603 D(G(z)): 0.2793 / 0.0382\n",
      "[0/10][3/938] Loss_D: 1.0286 Loss_G: 1.5070 D(x): 0.5317 D(G(z)): 0.1199 / 0.2877\n",
      "[0/10][4/938] Loss_D: 2.0775 Loss_G: 4.8690 D(x): 0.8323 D(G(z)): 0.7683 / 0.0168\n",
      "[0/10][5/938] Loss_D: 2.0847 Loss_G: 1.8401 D(x): 0.2510 D(G(z)): 0.0500 / 0.2443\n",
      "[0/10][6/938] Loss_D: 0.8845 Loss_G: 2.7199 D(x): 0.8546 D(G(z)): 0.4258 / 0.1100\n",
      "[0/10][7/938] Loss_D: 0.5826 Loss_G: 4.2385 D(x): 0.8654 D(G(z)): 0.2909 / 0.0254\n",
      "[0/10][8/938] Loss_D: 0.6425 Loss_G: 2.9364 D(x): 0.7070 D(G(z)): 0.1282 / 0.0780\n",
      "[0/10][9/938] Loss_D: 0.6700 Loss_G: 3.5044 D(x): 0.8291 D(G(z)): 0.2968 / 0.0467\n",
      "[0/10][10/938] Loss_D: 0.6866 Loss_G: 4.9298 D(x): 0.8543 D(G(z)): 0.3465 / 0.0123\n",
      "[0/10][11/938] Loss_D: 0.9371 Loss_G: 2.1509 D(x): 0.5382 D(G(z)): 0.1166 / 0.2014\n",
      "[0/10][12/938] Loss_D: 1.0844 Loss_G: 5.3550 D(x): 0.8120 D(G(z)): 0.4937 / 0.0077\n",
      "[0/10][13/938] Loss_D: 0.5711 Loss_G: 3.4844 D(x): 0.6504 D(G(z)): 0.0425 / 0.0707\n",
      "[0/10][14/938] Loss_D: 0.4750 Loss_G: 3.4418 D(x): 0.8497 D(G(z)): 0.2055 / 0.0529\n",
      "[0/10][15/938] Loss_D: 0.6311 Loss_G: 4.9333 D(x): 0.8478 D(G(z)): 0.3112 / 0.0113\n",
      "[0/10][16/938] Loss_D: 0.4840 Loss_G: 3.0414 D(x): 0.7359 D(G(z)): 0.1034 / 0.0805\n",
      "[0/10][17/938] Loss_D: 1.0750 Loss_G: 5.1854 D(x): 0.7963 D(G(z)): 0.4711 / 0.0138\n",
      "[0/10][18/938] Loss_D: 1.4826 Loss_G: 0.9158 D(x): 0.3809 D(G(z)): 0.0661 / 0.4663\n",
      "[0/10][19/938] Loss_D: 1.6655 Loss_G: 10.4341 D(x): 0.9721 D(G(z)): 0.7508 / 0.0001\n",
      "[0/10][20/938] Loss_D: 0.7415 Loss_G: 8.5246 D(x): 0.5641 D(G(z)): 0.0011 / 0.0008\n",
      "[0/10][21/938] Loss_D: 0.3963 Loss_G: 2.0203 D(x): 0.8081 D(G(z)): 0.0984 / 0.1909\n",
      "[0/10][22/938] Loss_D: 1.4870 Loss_G: 6.5901 D(x): 0.9450 D(G(z)): 0.6562 / 0.0028\n",
      "[0/10][23/938] Loss_D: 1.2170 Loss_G: 3.5398 D(x): 0.4380 D(G(z)): 0.0153 / 0.0612\n",
      "[0/10][24/938] Loss_D: 0.3545 Loss_G: 2.6811 D(x): 0.8631 D(G(z)): 0.1430 / 0.1013\n",
      "[0/10][25/938] Loss_D: 0.6155 Loss_G: 5.2351 D(x): 0.8899 D(G(z)): 0.3463 / 0.0097\n",
      "[0/10][26/938] Loss_D: 0.3967 Loss_G: 4.0150 D(x): 0.7630 D(G(z)): 0.0692 / 0.0333\n",
      "[0/10][27/938] Loss_D: 0.4472 Loss_G: 4.2875 D(x): 0.8736 D(G(z)): 0.2119 / 0.0257\n",
      "[0/10][28/938] Loss_D: 0.5257 Loss_G: 3.5602 D(x): 0.7882 D(G(z)): 0.1531 / 0.0489\n",
      "[0/10][29/938] Loss_D: 0.2809 Loss_G: 3.7933 D(x): 0.8961 D(G(z)): 0.1394 / 0.0347\n",
      "[0/10][30/938] Loss_D: 0.3373 Loss_G: 4.1642 D(x): 0.8813 D(G(z)): 0.1450 / 0.0296\n",
      "[0/10][31/938] Loss_D: 0.2601 Loss_G: 3.7758 D(x): 0.8871 D(G(z)): 0.1112 / 0.0311\n",
      "[0/10][32/938] Loss_D: 0.3531 Loss_G: 3.9705 D(x): 0.8767 D(G(z)): 0.1701 / 0.0271\n",
      "[0/10][33/938] Loss_D: 0.3816 Loss_G: 2.8720 D(x): 0.8067 D(G(z)): 0.1083 / 0.0781\n",
      "[0/10][34/938] Loss_D: 0.4560 Loss_G: 2.7521 D(x): 0.8240 D(G(z)): 0.1741 / 0.0864\n",
      "[0/10][35/938] Loss_D: 0.4877 Loss_G: 4.9451 D(x): 0.8987 D(G(z)): 0.2825 / 0.0117\n",
      "[0/10][36/938] Loss_D: 0.4254 Loss_G: 2.9379 D(x): 0.7345 D(G(z)): 0.0372 / 0.0847\n",
      "[0/10][37/938] Loss_D: 0.3786 Loss_G: 4.0351 D(x): 0.9146 D(G(z)): 0.2079 / 0.0282\n",
      "[0/10][38/938] Loss_D: 0.3445 Loss_G: 3.1576 D(x): 0.8243 D(G(z)): 0.0954 / 0.0597\n",
      "[0/10][39/938] Loss_D: 0.4168 Loss_G: 4.0630 D(x): 0.8652 D(G(z)): 0.2080 / 0.0242\n",
      "[0/10][40/938] Loss_D: 0.2488 Loss_G: 4.2683 D(x): 0.8919 D(G(z)): 0.1081 / 0.0265\n",
      "[0/10][41/938] Loss_D: 0.3232 Loss_G: 2.8626 D(x): 0.8290 D(G(z)): 0.0890 / 0.0840\n",
      "[0/10][42/938] Loss_D: 0.4968 Loss_G: 6.6218 D(x): 0.9182 D(G(z)): 0.3027 / 0.0030\n",
      "[0/10][43/938] Loss_D: 1.4660 Loss_G: 0.6952 D(x): 0.3528 D(G(z)): 0.0160 / 0.5869\n",
      "[0/10][44/938] Loss_D: 2.2069 Loss_G: 12.0577 D(x): 0.9886 D(G(z)): 0.8149 / 0.0000\n",
      "[0/10][45/938] Loss_D: 3.2427 Loss_G: 4.9175 D(x): 0.1423 D(G(z)): 0.0008 / 0.0238\n",
      "[0/10][46/938] Loss_D: 0.4765 Loss_G: 2.8256 D(x): 0.8489 D(G(z)): 0.1843 / 0.0953\n",
      "[0/10][47/938] Loss_D: 0.6924 Loss_G: 3.0500 D(x): 0.7972 D(G(z)): 0.2782 / 0.0718\n",
      "[0/10][48/938] Loss_D: 0.5822 Loss_G: 3.4284 D(x): 0.8191 D(G(z)): 0.2678 / 0.0479\n",
      "[0/10][49/938] Loss_D: 0.7985 Loss_G: 2.2779 D(x): 0.7083 D(G(z)): 0.2394 / 0.1418\n",
      "[0/10][50/938] Loss_D: 0.9956 Loss_G: 4.1179 D(x): 0.7575 D(G(z)): 0.4160 / 0.0357\n",
      "[0/10][51/938] Loss_D: 0.9378 Loss_G: 1.0779 D(x): 0.5358 D(G(z)): 0.1111 / 0.5082\n",
      "[0/10][52/938] Loss_D: 1.6947 Loss_G: 7.4535 D(x): 0.9845 D(G(z)): 0.6746 / 0.0018\n",
      "[0/10][53/938] Loss_D: 2.0825 Loss_G: 1.3592 D(x): 0.2230 D(G(z)): 0.0148 / 0.3526\n",
      "[0/10][54/938] Loss_D: 1.1866 Loss_G: 3.3927 D(x): 0.9459 D(G(z)): 0.5862 / 0.0650\n",
      "[0/10][55/938] Loss_D: 0.6935 Loss_G: 3.1266 D(x): 0.7233 D(G(z)): 0.2147 / 0.0695\n",
      "[0/10][56/938] Loss_D: 1.3200 Loss_G: 0.7948 D(x): 0.4874 D(G(z)): 0.2708 / 0.5139\n",
      "[0/10][57/938] Loss_D: 1.4646 Loss_G: 5.6128 D(x): 0.9112 D(G(z)): 0.6910 / 0.0052\n",
      "[0/10][58/938] Loss_D: 1.4760 Loss_G: 2.5385 D(x): 0.3257 D(G(z)): 0.0134 / 0.1431\n",
      "[0/10][59/938] Loss_D: 0.4548 Loss_G: 1.9097 D(x): 0.8821 D(G(z)): 0.2246 / 0.2015\n",
      "[0/10][60/938] Loss_D: 0.9077 Loss_G: 5.1762 D(x): 0.9516 D(G(z)): 0.4749 / 0.0105\n",
      "[0/10][61/938] Loss_D: 0.8599 Loss_G: 2.4110 D(x): 0.5324 D(G(z)): 0.0538 / 0.1477\n",
      "[0/10][62/938] Loss_D: 0.8448 Loss_G: 2.3390 D(x): 0.8026 D(G(z)): 0.3591 / 0.1536\n",
      "[0/10][63/938] Loss_D: 0.6190 Loss_G: 3.7736 D(x): 0.8508 D(G(z)): 0.3123 / 0.0349\n",
      "[0/10][64/938] Loss_D: 0.7455 Loss_G: 1.9146 D(x): 0.5894 D(G(z)): 0.0626 / 0.2200\n",
      "[0/10][65/938] Loss_D: 0.6973 Loss_G: 2.6028 D(x): 0.8187 D(G(z)): 0.3231 / 0.1104\n",
      "[0/10][66/938] Loss_D: 0.5320 Loss_G: 3.4556 D(x): 0.8296 D(G(z)): 0.2466 / 0.0498\n",
      "[0/10][67/938] Loss_D: 0.4538 Loss_G: 2.9626 D(x): 0.7884 D(G(z)): 0.1413 / 0.0750\n",
      "[0/10][68/938] Loss_D: 0.6630 Loss_G: 2.4598 D(x): 0.7543 D(G(z)): 0.2158 / 0.1386\n",
      "[0/10][69/938] Loss_D: 0.4880 Loss_G: 2.8798 D(x): 0.8515 D(G(z)): 0.2359 / 0.0745\n",
      "[0/10][70/938] Loss_D: 0.4491 Loss_G: 3.2415 D(x): 0.8374 D(G(z)): 0.2029 / 0.0579\n",
      "[0/10][71/938] Loss_D: 0.4640 Loss_G: 2.4820 D(x): 0.7975 D(G(z)): 0.1409 / 0.1083\n",
      "[0/10][72/938] Loss_D: 0.4929 Loss_G: 3.7536 D(x): 0.8843 D(G(z)): 0.2828 / 0.0317\n",
      "[0/10][73/938] Loss_D: 0.4642 Loss_G: 2.6678 D(x): 0.7418 D(G(z)): 0.0843 / 0.1013\n",
      "[0/10][74/938] Loss_D: 0.4317 Loss_G: 3.5718 D(x): 0.9074 D(G(z)): 0.2437 / 0.0445\n",
      "[0/10][75/938] Loss_D: 0.5055 Loss_G: 3.6021 D(x): 0.8108 D(G(z)): 0.2050 / 0.0333\n",
      "[0/10][76/938] Loss_D: 0.3778 Loss_G: 2.6295 D(x): 0.7870 D(G(z)): 0.0984 / 0.0919\n",
      "[0/10][77/938] Loss_D: 0.4713 Loss_G: 4.0580 D(x): 0.8782 D(G(z)): 0.2455 / 0.0243\n",
      "[0/10][78/938] Loss_D: 0.5971 Loss_G: 2.8463 D(x): 0.7341 D(G(z)): 0.1500 / 0.1075\n",
      "[0/10][79/938] Loss_D: 0.4733 Loss_G: 3.8597 D(x): 0.8530 D(G(z)): 0.2104 / 0.0356\n",
      "[0/10][80/938] Loss_D: 0.3764 Loss_G: 3.2028 D(x): 0.8229 D(G(z)): 0.1327 / 0.0676\n",
      "[0/10][81/938] Loss_D: 0.3817 Loss_G: 3.6694 D(x): 0.8631 D(G(z)): 0.1726 / 0.0368\n",
      "[0/10][82/938] Loss_D: 0.3891 Loss_G: 2.6530 D(x): 0.8025 D(G(z)): 0.1083 / 0.1124\n",
      "[0/10][83/938] Loss_D: 0.4381 Loss_G: 4.7157 D(x): 0.9110 D(G(z)): 0.2478 / 0.0147\n",
      "[0/10][84/938] Loss_D: 0.5927 Loss_G: 1.0558 D(x): 0.6312 D(G(z)): 0.0531 / 0.4257\n",
      "[0/10][85/938] Loss_D: 1.4841 Loss_G: 9.4797 D(x): 0.9755 D(G(z)): 0.6701 / 0.0008\n",
      "[0/10][86/938] Loss_D: 4.2378 Loss_G: 1.9732 D(x): 0.0458 D(G(z)): 0.0028 / 0.2917\n",
      "[0/10][87/938] Loss_D: 0.7457 Loss_G: 2.6737 D(x): 0.8988 D(G(z)): 0.3505 / 0.1169\n",
      "[0/10][88/938] Loss_D: 0.7203 Loss_G: 5.1503 D(x): 0.8946 D(G(z)): 0.3886 / 0.0131\n",
      "[0/10][89/938] Loss_D: 1.3204 Loss_G: 0.5021 D(x): 0.3747 D(G(z)): 0.0668 / 0.6559\n",
      "[0/10][90/938] Loss_D: 1.7670 Loss_G: 4.7097 D(x): 0.9858 D(G(z)): 0.7613 / 0.0210\n",
      "[0/10][91/938] Loss_D: 0.8150 Loss_G: 3.3994 D(x): 0.5946 D(G(z)): 0.1170 / 0.0856\n",
      "[0/10][92/938] Loss_D: 0.6693 Loss_G: 2.3040 D(x): 0.7434 D(G(z)): 0.2210 / 0.1492\n",
      "[0/10][93/938] Loss_D: 1.0200 Loss_G: 4.2757 D(x): 0.7664 D(G(z)): 0.4183 / 0.0252\n",
      "[0/10][94/938] Loss_D: 0.8409 Loss_G: 1.2151 D(x): 0.5304 D(G(z)): 0.0638 / 0.3817\n",
      "[0/10][95/938] Loss_D: 1.1740 Loss_G: 6.1866 D(x): 0.9482 D(G(z)): 0.5955 / 0.0037\n",
      "[0/10][96/938] Loss_D: 0.7373 Loss_G: 3.3638 D(x): 0.5665 D(G(z)): 0.0271 / 0.0724\n",
      "[0/10][97/938] Loss_D: 0.5962 Loss_G: 2.2966 D(x): 0.8194 D(G(z)): 0.2322 / 0.1681\n",
      "[0/10][98/938] Loss_D: 0.5826 Loss_G: 4.1474 D(x): 0.8811 D(G(z)): 0.3197 / 0.0246\n",
      "[0/10][99/938] Loss_D: 0.5972 Loss_G: 2.5949 D(x): 0.6867 D(G(z)): 0.0958 / 0.1112\n",
      "[0/10][100/938] Loss_D: 0.5523 Loss_G: 2.9831 D(x): 0.8245 D(G(z)): 0.2563 / 0.0667\n",
      "[0/10][101/938] Loss_D: 0.4165 Loss_G: 2.5878 D(x): 0.7911 D(G(z)): 0.1308 / 0.0951\n",
      "[0/10][102/938] Loss_D: 0.4428 Loss_G: 3.1014 D(x): 0.8580 D(G(z)): 0.2243 / 0.0566\n",
      "[0/10][103/938] Loss_D: 0.3243 Loss_G: 3.0300 D(x): 0.8533 D(G(z)): 0.1315 / 0.0592\n",
      "[0/10][104/938] Loss_D: 0.4824 Loss_G: 2.3783 D(x): 0.7886 D(G(z)): 0.1500 / 0.1252\n",
      "[0/10][105/938] Loss_D: 0.3359 Loss_G: 3.4685 D(x): 0.9178 D(G(z)): 0.2006 / 0.0460\n",
      "[0/10][106/938] Loss_D: 0.3257 Loss_G: 3.0844 D(x): 0.8355 D(G(z)): 0.1090 / 0.0737\n",
      "[0/10][107/938] Loss_D: 0.3250 Loss_G: 2.6054 D(x): 0.8527 D(G(z)): 0.1336 / 0.0946\n",
      "[0/10][108/938] Loss_D: 0.4301 Loss_G: 3.8655 D(x): 0.9184 D(G(z)): 0.2531 / 0.0285\n",
      "[0/10][109/938] Loss_D: 0.4422 Loss_G: 1.9038 D(x): 0.7133 D(G(z)): 0.0487 / 0.2161\n",
      "[0/10][110/938] Loss_D: 0.6766 Loss_G: 5.7054 D(x): 0.9424 D(G(z)): 0.4013 / 0.0067\n",
      "[0/10][111/938] Loss_D: 0.7772 Loss_G: 2.2705 D(x): 0.5510 D(G(z)): 0.0192 / 0.1887\n",
      "[0/10][112/938] Loss_D: 0.4743 Loss_G: 3.5099 D(x): 0.9522 D(G(z)): 0.3071 / 0.0388\n",
      "[0/10][113/938] Loss_D: 0.2601 Loss_G: 3.4540 D(x): 0.8643 D(G(z)): 0.0928 / 0.0512\n",
      "[0/10][114/938] Loss_D: 0.3013 Loss_G: 2.4582 D(x): 0.8363 D(G(z)): 0.0866 / 0.1172\n",
      "[0/10][115/938] Loss_D: 0.2816 Loss_G: 3.3752 D(x): 0.9208 D(G(z)): 0.1603 / 0.0531\n",
      "[0/10][116/938] Loss_D: 0.2227 Loss_G: 3.4191 D(x): 0.8976 D(G(z)): 0.0981 / 0.0449\n",
      "[0/10][117/938] Loss_D: 0.3042 Loss_G: 2.1550 D(x): 0.8108 D(G(z)): 0.0674 / 0.1513\n",
      "[0/10][118/938] Loss_D: 0.4395 Loss_G: 4.8484 D(x): 0.9614 D(G(z)): 0.3001 / 0.0135\n",
      "[0/10][119/938] Loss_D: 0.9423 Loss_G: 0.9958 D(x): 0.4870 D(G(z)): 0.0282 / 0.4572\n",
      "[0/10][120/938] Loss_D: 1.2230 Loss_G: 14.2586 D(x): 0.9839 D(G(z)): 0.6108 / 0.0000\n",
      "[0/10][121/938] Loss_D: 11.5453 Loss_G: 3.1096 D(x): 0.0001 D(G(z)): 0.0004 / 0.1200\n",
      "[0/10][122/938] Loss_D: 0.4940 Loss_G: 3.4876 D(x): 0.8259 D(G(z)): 0.1896 / 0.0662\n",
      "[0/10][123/938] Loss_D: 1.1253 Loss_G: 0.0997 D(x): 0.4614 D(G(z)): 0.1009 / 0.9103\n",
      "[0/10][124/938] Loss_D: 2.6598 Loss_G: 4.5537 D(x): 0.9956 D(G(z)): 0.8954 / 0.0308\n",
      "[0/10][125/938] Loss_D: 0.8363 Loss_G: 2.3069 D(x): 0.5800 D(G(z)): 0.1040 / 0.1867\n",
      "[0/10][126/938] Loss_D: 0.6148 Loss_G: 3.1808 D(x): 0.8982 D(G(z)): 0.3249 / 0.0770\n",
      "[0/10][127/938] Loss_D: 0.7361 Loss_G: 2.0743 D(x): 0.6836 D(G(z)): 0.2382 / 0.1734\n",
      "[0/10][128/938] Loss_D: 0.8135 Loss_G: 3.5983 D(x): 0.8155 D(G(z)): 0.3940 / 0.0394\n",
      "[0/10][129/938] Loss_D: 0.9573 Loss_G: 0.6375 D(x): 0.5113 D(G(z)): 0.1186 / 0.5924\n",
      "[0/10][130/938] Loss_D: 1.6729 Loss_G: 6.2682 D(x): 0.9602 D(G(z)): 0.7309 / 0.0043\n",
      "[0/10][131/938] Loss_D: 2.4912 Loss_G: 1.4323 D(x): 0.1359 D(G(z)): 0.0103 / 0.3302\n",
      "[0/10][132/938] Loss_D: 0.8410 Loss_G: 2.0987 D(x): 0.8849 D(G(z)): 0.4393 / 0.1782\n",
      "[0/10][133/938] Loss_D: 0.6180 Loss_G: 3.3334 D(x): 0.8117 D(G(z)): 0.2928 / 0.0539\n",
      "[0/10][134/938] Loss_D: 0.5526 Loss_G: 2.4577 D(x): 0.7207 D(G(z)): 0.1369 / 0.1295\n",
      "[0/10][135/938] Loss_D: 0.4699 Loss_G: 2.5394 D(x): 0.8340 D(G(z)): 0.2083 / 0.1155\n",
      "[0/10][136/938] Loss_D: 0.4554 Loss_G: 2.6809 D(x): 0.8253 D(G(z)): 0.2005 / 0.0957\n",
      "[0/10][137/938] Loss_D: 0.5691 Loss_G: 3.1470 D(x): 0.8327 D(G(z)): 0.2637 / 0.0686\n",
      "[0/10][138/938] Loss_D: 0.7313 Loss_G: 1.5011 D(x): 0.6310 D(G(z)): 0.1532 / 0.2898\n",
      "[0/10][139/938] Loss_D: 0.7700 Loss_G: 3.2228 D(x): 0.8617 D(G(z)): 0.4049 / 0.0561\n",
      "[0/10][140/938] Loss_D: 0.7115 Loss_G: 1.9133 D(x): 0.6298 D(G(z)): 0.1497 / 0.1830\n",
      "[0/10][141/938] Loss_D: 0.7542 Loss_G: 3.9103 D(x): 0.8707 D(G(z)): 0.3984 / 0.0276\n",
      "[0/10][142/938] Loss_D: 0.8872 Loss_G: 1.3988 D(x): 0.5194 D(G(z)): 0.0786 / 0.3187\n",
      "[0/10][143/938] Loss_D: 0.6104 Loss_G: 3.7503 D(x): 0.9489 D(G(z)): 0.3863 / 0.0347\n",
      "[0/10][144/938] Loss_D: 0.4151 Loss_G: 3.0766 D(x): 0.7895 D(G(z)): 0.1374 / 0.0683\n",
      "[0/10][145/938] Loss_D: 0.4457 Loss_G: 1.9559 D(x): 0.7947 D(G(z)): 0.1638 / 0.1759\n",
      "[0/10][146/938] Loss_D: 0.8153 Loss_G: 3.9685 D(x): 0.8553 D(G(z)): 0.4241 / 0.0278\n",
      "[0/10][147/938] Loss_D: 1.1738 Loss_G: 0.6288 D(x): 0.3902 D(G(z)): 0.0615 / 0.6087\n",
      "[0/10][148/938] Loss_D: 1.8073 Loss_G: 5.5681 D(x): 0.9824 D(G(z)): 0.7388 / 0.0099\n",
      "[0/10][149/938] Loss_D: 1.3618 Loss_G: 1.4917 D(x): 0.3477 D(G(z)): 0.0475 / 0.3062\n",
      "[0/10][150/938] Loss_D: 0.8114 Loss_G: 2.6580 D(x): 0.8954 D(G(z)): 0.4166 / 0.0989\n",
      "[0/10][151/938] Loss_D: 0.5429 Loss_G: 3.0650 D(x): 0.8075 D(G(z)): 0.2421 / 0.0648\n",
      "[0/10][152/938] Loss_D: 0.7753 Loss_G: 1.2498 D(x): 0.5775 D(G(z)): 0.1058 / 0.3399\n",
      "[0/10][153/938] Loss_D: 0.7407 Loss_G: 3.7891 D(x): 0.9112 D(G(z)): 0.4311 / 0.0393\n",
      "[0/10][154/938] Loss_D: 0.7170 Loss_G: 1.3069 D(x): 0.5717 D(G(z)): 0.0687 / 0.3240\n",
      "[0/10][155/938] Loss_D: 0.8290 Loss_G: 3.8237 D(x): 0.9359 D(G(z)): 0.4877 / 0.0289\n",
      "[0/10][156/938] Loss_D: 0.7661 Loss_G: 1.8092 D(x): 0.5807 D(G(z)): 0.0943 / 0.2112\n",
      "[0/10][157/938] Loss_D: 0.4154 Loss_G: 1.9638 D(x): 0.8502 D(G(z)): 0.2035 / 0.1745\n",
      "[0/10][158/938] Loss_D: 0.4577 Loss_G: 3.3338 D(x): 0.8937 D(G(z)): 0.2724 / 0.0491\n",
      "[0/10][159/938] Loss_D: 0.5162 Loss_G: 1.7784 D(x): 0.6843 D(G(z)): 0.0913 / 0.2103\n",
      "[0/10][160/938] Loss_D: 0.5627 Loss_G: 2.4757 D(x): 0.8361 D(G(z)): 0.2738 / 0.1161\n",
      "[0/10][161/938] Loss_D: 0.3608 Loss_G: 2.8885 D(x): 0.8488 D(G(z)): 0.1600 / 0.0708\n",
      "[0/10][162/938] Loss_D: 0.4256 Loss_G: 2.5491 D(x): 0.8162 D(G(z)): 0.1709 / 0.1012\n",
      "[0/10][163/938] Loss_D: 0.3437 Loss_G: 2.2660 D(x): 0.8323 D(G(z)): 0.1312 / 0.1352\n",
      "[0/10][164/938] Loss_D: 0.3846 Loss_G: 2.5304 D(x): 0.8509 D(G(z)): 0.1799 / 0.0972\n",
      "[0/10][165/938] Loss_D: 0.4250 Loss_G: 2.5941 D(x): 0.8282 D(G(z)): 0.1884 / 0.0943\n",
      "[0/10][166/938] Loss_D: 0.3724 Loss_G: 2.4419 D(x): 0.8344 D(G(z)): 0.1538 / 0.1052\n",
      "[0/10][167/938] Loss_D: 0.4512 Loss_G: 2.2506 D(x): 0.8023 D(G(z)): 0.1740 / 0.1327\n",
      "[0/10][168/938] Loss_D: 0.4140 Loss_G: 2.9576 D(x): 0.8548 D(G(z)): 0.2091 / 0.0648\n",
      "[0/10][169/938] Loss_D: 0.4431 Loss_G: 1.8064 D(x): 0.7545 D(G(z)): 0.1197 / 0.1964\n",
      "[0/10][170/938] Loss_D: 0.5562 Loss_G: 4.5750 D(x): 0.9088 D(G(z)): 0.3325 / 0.0174\n",
      "[0/10][171/938] Loss_D: 0.9348 Loss_G: 1.4165 D(x): 0.4804 D(G(z)): 0.0411 / 0.3092\n",
      "[0/10][172/938] Loss_D: 0.8832 Loss_G: 4.2055 D(x): 0.8543 D(G(z)): 0.4350 / 0.0243\n",
      "[0/10][173/938] Loss_D: 1.3216 Loss_G: 0.0944 D(x): 0.3646 D(G(z)): 0.0903 / 0.9163\n",
      "[0/10][174/938] Loss_D: 3.1163 Loss_G: 6.2190 D(x): 0.9933 D(G(z)): 0.8957 / 0.0045\n",
      "[0/10][175/938] Loss_D: 2.1831 Loss_G: 0.8115 D(x): 0.1893 D(G(z)): 0.0184 / 0.5331\n",
      "[0/10][176/938] Loss_D: 1.4234 Loss_G: 3.2775 D(x): 0.9408 D(G(z)): 0.6008 / 0.0684\n",
      "[0/10][177/938] Loss_D: 0.9370 Loss_G: 1.9867 D(x): 0.5844 D(G(z)): 0.2219 / 0.2142\n",
      "[0/10][178/938] Loss_D: 0.8343 Loss_G: 1.5321 D(x): 0.6778 D(G(z)): 0.2679 / 0.2759\n",
      "[0/10][179/938] Loss_D: 0.9124 Loss_G: 3.3387 D(x): 0.7931 D(G(z)): 0.4326 / 0.0476\n",
      "[0/10][180/938] Loss_D: 0.9418 Loss_G: 1.1184 D(x): 0.5133 D(G(z)): 0.1138 / 0.3903\n",
      "[0/10][181/938] Loss_D: 1.2190 Loss_G: 4.0706 D(x): 0.8795 D(G(z)): 0.5820 / 0.0293\n",
      "[0/10][182/938] Loss_D: 1.1680 Loss_G: 1.1923 D(x): 0.4083 D(G(z)): 0.0561 / 0.3844\n",
      "[0/10][183/938] Loss_D: 0.7998 Loss_G: 3.0855 D(x): 0.9460 D(G(z)): 0.4676 / 0.0780\n",
      "[0/10][184/938] Loss_D: 0.4519 Loss_G: 2.5551 D(x): 0.7521 D(G(z)): 0.1256 / 0.0996\n",
      "[0/10][185/938] Loss_D: 0.4868 Loss_G: 1.9304 D(x): 0.7786 D(G(z)): 0.1675 / 0.1885\n",
      "[0/10][186/938] Loss_D: 0.5796 Loss_G: 2.9597 D(x): 0.8722 D(G(z)): 0.3271 / 0.0684\n",
      "[0/10][187/938] Loss_D: 0.7303 Loss_G: 1.1184 D(x): 0.6154 D(G(z)): 0.1360 / 0.3702\n",
      "[0/10][188/938] Loss_D: 0.6238 Loss_G: 3.4985 D(x): 0.9193 D(G(z)): 0.3867 / 0.0423\n",
      "[0/10][189/938] Loss_D: 0.5969 Loss_G: 1.6096 D(x): 0.6410 D(G(z)): 0.0838 / 0.2339\n",
      "[0/10][190/938] Loss_D: 0.8725 Loss_G: 4.2869 D(x): 0.8716 D(G(z)): 0.4455 / 0.0222\n",
      "[0/10][191/938] Loss_D: 1.3421 Loss_G: 0.4402 D(x): 0.3412 D(G(z)): 0.0424 / 0.6987\n",
      "[0/10][192/938] Loss_D: 1.6770 Loss_G: 5.7824 D(x): 0.9639 D(G(z)): 0.6796 / 0.0059\n",
      "[0/10][193/938] Loss_D: 2.6785 Loss_G: 0.1897 D(x): 0.1236 D(G(z)): 0.0156 / 0.8469\n",
      "[0/10][194/938] Loss_D: 2.6717 Loss_G: 4.1621 D(x): 0.9953 D(G(z)): 0.8722 / 0.0526\n",
      "[0/10][195/938] Loss_D: 1.0587 Loss_G: 2.7808 D(x): 0.5173 D(G(z)): 0.1489 / 0.1154\n",
      "[0/10][196/938] Loss_D: 0.9988 Loss_G: 0.6571 D(x): 0.5234 D(G(z)): 0.1925 / 0.5823\n",
      "[0/10][197/938] Loss_D: 1.4754 Loss_G: 3.5440 D(x): 0.9224 D(G(z)): 0.6656 / 0.0503\n",
      "[0/10][198/938] Loss_D: 0.9012 Loss_G: 2.1014 D(x): 0.5301 D(G(z)): 0.1030 / 0.1634\n",
      "[0/10][199/938] Loss_D: 0.7103 Loss_G: 1.6812 D(x): 0.7453 D(G(z)): 0.2873 / 0.2365\n",
      "[0/10][200/938] Loss_D: 0.8275 Loss_G: 2.8174 D(x): 0.8029 D(G(z)): 0.4011 / 0.0846\n",
      "[0/10][201/938] Loss_D: 0.7766 Loss_G: 1.4561 D(x): 0.5774 D(G(z)): 0.1333 / 0.2826\n",
      "[0/10][202/938] Loss_D: 0.6895 Loss_G: 2.5456 D(x): 0.8534 D(G(z)): 0.3481 / 0.1060\n",
      "[0/10][203/938] Loss_D: 0.5651 Loss_G: 2.5589 D(x): 0.7719 D(G(z)): 0.2310 / 0.1083\n",
      "[0/10][204/938] Loss_D: 0.7532 Loss_G: 1.0933 D(x): 0.6114 D(G(z)): 0.1731 / 0.3895\n",
      "[0/10][205/938] Loss_D: 0.8854 Loss_G: 3.0957 D(x): 0.8842 D(G(z)): 0.4562 / 0.0713\n",
      "[0/10][206/938] Loss_D: 0.5517 Loss_G: 2.2881 D(x): 0.6962 D(G(z)): 0.1151 / 0.1475\n",
      "[0/10][207/938] Loss_D: 0.5752 Loss_G: 1.5300 D(x): 0.7481 D(G(z)): 0.1923 / 0.2671\n",
      "[0/10][208/938] Loss_D: 0.6601 Loss_G: 3.3469 D(x): 0.9057 D(G(z)): 0.3827 / 0.0486\n",
      "[0/10][209/938] Loss_D: 0.8297 Loss_G: 1.1257 D(x): 0.5413 D(G(z)): 0.1147 / 0.3807\n",
      "[0/10][210/938] Loss_D: 0.5551 Loss_G: 2.7701 D(x): 0.9085 D(G(z)): 0.3368 / 0.0864\n",
      "[0/10][211/938] Loss_D: 0.5293 Loss_G: 2.2123 D(x): 0.7554 D(G(z)): 0.1780 / 0.1282\n",
      "[0/10][212/938] Loss_D: 0.5349 Loss_G: 1.7873 D(x): 0.7532 D(G(z)): 0.1821 / 0.2014\n",
      "[0/10][213/938] Loss_D: 0.4916 Loss_G: 2.7771 D(x): 0.8572 D(G(z)): 0.2620 / 0.0812\n",
      "[0/10][214/938] Loss_D: 0.4722 Loss_G: 2.0343 D(x): 0.7537 D(G(z)): 0.1446 / 0.1607\n",
      "[0/10][215/938] Loss_D: 0.4830 Loss_G: 1.8067 D(x): 0.7914 D(G(z)): 0.1872 / 0.1983\n",
      "[0/10][216/938] Loss_D: 0.6186 Loss_G: 3.1597 D(x): 0.8533 D(G(z)): 0.3295 / 0.0527\n",
      "[0/10][217/938] Loss_D: 0.5808 Loss_G: 1.2902 D(x): 0.6486 D(G(z)): 0.1000 / 0.3215\n",
      "[0/10][218/938] Loss_D: 0.5880 Loss_G: 3.3422 D(x): 0.9205 D(G(z)): 0.3616 / 0.0454\n",
      "[0/10][219/938] Loss_D: 0.5994 Loss_G: 1.5216 D(x): 0.6322 D(G(z)): 0.0757 / 0.2675\n",
      "[0/10][220/938] Loss_D: 0.5354 Loss_G: 3.6139 D(x): 0.9177 D(G(z)): 0.3338 / 0.0360\n",
      "[0/10][221/938] Loss_D: 0.8331 Loss_G: 0.7602 D(x): 0.5497 D(G(z)): 0.1223 / 0.5021\n",
      "[0/10][222/938] Loss_D: 1.0935 Loss_G: 5.4027 D(x): 0.9638 D(G(z)): 0.6004 / 0.0066\n",
      "[0/10][223/938] Loss_D: 2.3979 Loss_G: 0.3747 D(x): 0.1576 D(G(z)): 0.0133 / 0.7191\n",
      "[0/10][224/938] Loss_D: 1.6817 Loss_G: 5.2029 D(x): 0.9849 D(G(z)): 0.7545 / 0.0088\n",
      "[0/10][225/938] Loss_D: 2.0586 Loss_G: 0.6268 D(x): 0.2115 D(G(z)): 0.0386 / 0.6211\n",
      "[0/10][226/938] Loss_D: 1.3824 Loss_G: 2.7329 D(x): 0.9150 D(G(z)): 0.6220 / 0.1001\n",
      "[0/10][227/938] Loss_D: 0.7340 Loss_G: 2.4497 D(x): 0.6850 D(G(z)): 0.2044 / 0.1169\n",
      "[0/10][228/938] Loss_D: 0.9435 Loss_G: 1.1589 D(x): 0.5962 D(G(z)): 0.2196 / 0.3762\n",
      "[0/10][229/938] Loss_D: 1.0425 Loss_G: 2.8341 D(x): 0.8366 D(G(z)): 0.5141 / 0.0777\n",
      "[0/10][230/938] Loss_D: 1.0738 Loss_G: 1.1588 D(x): 0.4892 D(G(z)): 0.1819 / 0.3499\n",
      "[0/10][231/938] Loss_D: 0.7567 Loss_G: 2.5122 D(x): 0.8397 D(G(z)): 0.3967 / 0.1039\n",
      "[0/10][232/938] Loss_D: 0.7222 Loss_G: 1.7008 D(x): 0.6375 D(G(z)): 0.1737 / 0.2272\n",
      "[0/10][233/938] Loss_D: 0.7845 Loss_G: 2.0560 D(x): 0.7715 D(G(z)): 0.3580 / 0.1523\n",
      "[0/10][234/938] Loss_D: 0.6521 Loss_G: 1.5064 D(x): 0.6689 D(G(z)): 0.1758 / 0.2683\n",
      "[0/10][235/938] Loss_D: 0.6055 Loss_G: 3.2876 D(x): 0.8946 D(G(z)): 0.3656 / 0.0501\n",
      "[0/10][236/938] Loss_D: 0.7746 Loss_G: 0.9912 D(x): 0.5791 D(G(z)): 0.1034 / 0.4009\n",
      "[0/10][237/938] Loss_D: 0.8544 Loss_G: 3.5944 D(x): 0.9406 D(G(z)): 0.5018 / 0.0363\n",
      "[0/10][238/938] Loss_D: 0.8539 Loss_G: 1.2660 D(x): 0.5048 D(G(z)): 0.0535 / 0.3261\n",
      "[0/10][239/938] Loss_D: 0.7034 Loss_G: 2.6348 D(x): 0.9040 D(G(z)): 0.4210 / 0.0911\n",
      "[0/10][240/938] Loss_D: 0.4947 Loss_G: 2.3179 D(x): 0.7411 D(G(z)): 0.1452 / 0.1277\n",
      "[0/10][241/938] Loss_D: 0.6342 Loss_G: 1.6730 D(x): 0.7334 D(G(z)): 0.2365 / 0.2345\n",
      "[0/10][242/938] Loss_D: 0.5418 Loss_G: 3.1285 D(x): 0.8666 D(G(z)): 0.2955 / 0.0575\n",
      "[0/10][243/938] Loss_D: 0.6930 Loss_G: 1.4550 D(x): 0.6203 D(G(z)): 0.1418 / 0.2739\n",
      "[0/10][244/938] Loss_D: 0.5863 Loss_G: 2.8021 D(x): 0.8839 D(G(z)): 0.3432 / 0.0758\n",
      "[0/10][245/938] Loss_D: 0.5561 Loss_G: 1.5978 D(x): 0.6746 D(G(z)): 0.1182 / 0.2434\n",
      "[0/10][246/938] Loss_D: 0.4970 Loss_G: 2.5492 D(x): 0.8573 D(G(z)): 0.2679 / 0.0926\n",
      "[0/10][247/938] Loss_D: 0.4004 Loss_G: 2.2475 D(x): 0.8026 D(G(z)): 0.1452 / 0.1321\n",
      "[0/10][248/938] Loss_D: 0.4545 Loss_G: 2.4393 D(x): 0.8190 D(G(z)): 0.1974 / 0.1190\n",
      "[0/10][249/938] Loss_D: 0.3446 Loss_G: 2.9816 D(x): 0.8815 D(G(z)): 0.1756 / 0.0656\n",
      "[0/10][250/938] Loss_D: 0.5236 Loss_G: 1.6124 D(x): 0.7307 D(G(z)): 0.1556 / 0.2385\n",
      "[0/10][251/938] Loss_D: 0.4200 Loss_G: 2.6827 D(x): 0.8508 D(G(z)): 0.2032 / 0.0949\n",
      "[0/10][252/938] Loss_D: 0.3619 Loss_G: 3.0204 D(x): 0.8724 D(G(z)): 0.1858 / 0.0601\n",
      "[0/10][253/938] Loss_D: 0.5101 Loss_G: 1.7976 D(x): 0.7543 D(G(z)): 0.1712 / 0.2109\n",
      "[0/10][254/938] Loss_D: 0.5596 Loss_G: 3.7809 D(x): 0.8912 D(G(z)): 0.3181 / 0.0308\n",
      "[0/10][255/938] Loss_D: 0.6498 Loss_G: 0.9926 D(x): 0.5880 D(G(z)): 0.0417 / 0.4227\n",
      "[0/10][256/938] Loss_D: 0.8155 Loss_G: 4.9324 D(x): 0.9622 D(G(z)): 0.4911 / 0.0101\n",
      "[0/10][257/938] Loss_D: 1.6703 Loss_G: 0.4026 D(x): 0.2516 D(G(z)): 0.0229 / 0.6896\n",
      "[0/10][258/938] Loss_D: 1.4570 Loss_G: 7.0325 D(x): 0.9822 D(G(z)): 0.7267 / 0.0012\n",
      "[0/10][259/938] Loss_D: 4.8349 Loss_G: 0.7092 D(x): 0.0118 D(G(z)): 0.0031 / 0.5630\n",
      "[0/10][260/938] Loss_D: 1.6486 Loss_G: 2.2353 D(x): 0.9276 D(G(z)): 0.7098 / 0.1471\n",
      "[0/10][261/938] Loss_D: 1.1853 Loss_G: 1.3705 D(x): 0.4929 D(G(z)): 0.2488 / 0.3073\n",
      "[0/10][262/938] Loss_D: 1.2686 Loss_G: 1.6862 D(x): 0.6639 D(G(z)): 0.4784 / 0.2316\n",
      "[0/10][263/938] Loss_D: 1.1335 Loss_G: 1.3625 D(x): 0.5878 D(G(z)): 0.3076 / 0.3071\n",
      "[0/10][264/938] Loss_D: 1.1360 Loss_G: 2.2512 D(x): 0.7067 D(G(z)): 0.4900 / 0.1368\n",
      "[0/10][265/938] Loss_D: 1.2309 Loss_G: 0.6297 D(x): 0.4134 D(G(z)): 0.1611 / 0.5845\n",
      "[0/10][266/938] Loss_D: 1.7011 Loss_G: 3.7163 D(x): 0.8820 D(G(z)): 0.7395 / 0.0444\n",
      "[0/10][267/938] Loss_D: 1.4024 Loss_G: 1.2121 D(x): 0.3654 D(G(z)): 0.0770 / 0.3464\n",
      "[0/10][268/938] Loss_D: 1.0348 Loss_G: 1.5701 D(x): 0.7534 D(G(z)): 0.4656 / 0.2375\n",
      "[0/10][269/938] Loss_D: 0.8647 Loss_G: 2.0957 D(x): 0.7044 D(G(z)): 0.3501 / 0.1593\n",
      "[0/10][270/938] Loss_D: 0.8202 Loss_G: 1.4416 D(x): 0.6369 D(G(z)): 0.2454 / 0.2700\n",
      "[0/10][271/938] Loss_D: 0.8218 Loss_G: 1.8657 D(x): 0.7325 D(G(z)): 0.3653 / 0.1822\n",
      "[0/10][272/938] Loss_D: 0.9214 Loss_G: 1.6462 D(x): 0.6401 D(G(z)): 0.3162 / 0.2259\n",
      "[0/10][273/938] Loss_D: 0.7237 Loss_G: 1.8331 D(x): 0.7125 D(G(z)): 0.2750 / 0.1983\n",
      "[0/10][274/938] Loss_D: 0.6487 Loss_G: 2.8792 D(x): 0.8019 D(G(z)): 0.3163 / 0.0761\n",
      "[0/10][275/938] Loss_D: 0.7707 Loss_G: 1.1067 D(x): 0.6143 D(G(z)): 0.1438 / 0.3826\n",
      "[0/10][276/938] Loss_D: 0.8554 Loss_G: 3.1104 D(x): 0.8681 D(G(z)): 0.4490 / 0.0571\n",
      "[0/10][277/938] Loss_D: 0.8858 Loss_G: 1.0286 D(x): 0.5076 D(G(z)): 0.0993 / 0.4278\n",
      "[0/10][278/938] Loss_D: 0.8736 Loss_G: 3.4776 D(x): 0.9180 D(G(z)): 0.4951 / 0.0453\n",
      "[0/10][279/938] Loss_D: 0.7368 Loss_G: 1.5591 D(x): 0.5914 D(G(z)): 0.1032 / 0.2608\n",
      "[0/10][280/938] Loss_D: 0.7922 Loss_G: 3.2301 D(x): 0.8664 D(G(z)): 0.4372 / 0.0523\n",
      "[0/10][281/938] Loss_D: 0.7022 Loss_G: 1.3705 D(x): 0.5796 D(G(z)): 0.0757 / 0.2929\n",
      "[0/10][282/938] Loss_D: 0.8310 Loss_G: 3.5419 D(x): 0.9078 D(G(z)): 0.4686 / 0.0456\n",
      "[0/10][283/938] Loss_D: 0.9224 Loss_G: 1.1627 D(x): 0.5149 D(G(z)): 0.0870 / 0.3669\n",
      "[0/10][284/938] Loss_D: 0.7933 Loss_G: 2.9780 D(x): 0.9209 D(G(z)): 0.4545 / 0.0736\n",
      "[0/10][285/938] Loss_D: 0.6343 Loss_G: 2.1142 D(x): 0.6721 D(G(z)): 0.1511 / 0.1496\n",
      "[0/10][286/938] Loss_D: 0.5211 Loss_G: 1.9087 D(x): 0.7924 D(G(z)): 0.2188 / 0.1768\n",
      "[0/10][287/938] Loss_D: 0.5467 Loss_G: 2.7572 D(x): 0.8336 D(G(z)): 0.2829 / 0.0773\n",
      "[0/10][288/938] Loss_D: 0.6500 Loss_G: 0.8682 D(x): 0.6242 D(G(z)): 0.1228 / 0.4541\n",
      "[0/10][289/938] Loss_D: 0.8195 Loss_G: 4.4080 D(x): 0.9542 D(G(z)): 0.5044 / 0.0156\n",
      "[0/10][290/938] Loss_D: 1.6240 Loss_G: 0.6077 D(x): 0.2549 D(G(z)): 0.0324 / 0.5906\n",
      "[0/10][291/938] Loss_D: 1.1419 Loss_G: 4.9525 D(x): 0.9483 D(G(z)): 0.5726 / 0.0113\n",
      "[0/10][292/938] Loss_D: 2.2508 Loss_G: 0.1461 D(x): 0.1739 D(G(z)): 0.0444 / 0.8827\n",
      "[0/10][293/938] Loss_D: 2.9123 Loss_G: 2.7973 D(x): 0.9886 D(G(z)): 0.8389 / 0.1244\n",
      "[0/10][294/938] Loss_D: 0.9869 Loss_G: 2.9886 D(x): 0.6233 D(G(z)): 0.2495 / 0.0767\n",
      "[0/10][295/938] Loss_D: 1.1013 Loss_G: 0.8420 D(x): 0.4759 D(G(z)): 0.1510 / 0.4849\n",
      "[0/10][296/938] Loss_D: 1.0460 Loss_G: 2.2892 D(x): 0.8802 D(G(z)): 0.5267 / 0.1383\n",
      "[0/10][297/938] Loss_D: 0.7136 Loss_G: 2.1273 D(x): 0.6702 D(G(z)): 0.1950 / 0.1463\n",
      "[0/10][298/938] Loss_D: 0.6331 Loss_G: 1.2732 D(x): 0.6924 D(G(z)): 0.1916 / 0.3197\n",
      "[0/10][299/938] Loss_D: 0.8971 Loss_G: 3.2047 D(x): 0.8834 D(G(z)): 0.4845 / 0.0619\n",
      "[0/10][300/938] Loss_D: 0.9618 Loss_G: 1.1245 D(x): 0.4860 D(G(z)): 0.1087 / 0.3871\n",
      "[0/10][301/938] Loss_D: 0.8050 Loss_G: 2.2558 D(x): 0.8576 D(G(z)): 0.4150 / 0.1404\n",
      "[0/10][302/938] Loss_D: 0.5552 Loss_G: 2.5705 D(x): 0.7596 D(G(z)): 0.2001 / 0.1104\n",
      "[0/10][303/938] Loss_D: 0.8245 Loss_G: 1.1743 D(x): 0.6173 D(G(z)): 0.2237 / 0.3459\n",
      "[0/10][304/938] Loss_D: 0.6902 Loss_G: 2.5021 D(x): 0.8247 D(G(z)): 0.3461 / 0.1101\n",
      "[0/10][305/938] Loss_D: 0.5559 Loss_G: 2.4161 D(x): 0.7664 D(G(z)): 0.2156 / 0.1165\n",
      "[0/10][306/938] Loss_D: 0.6248 Loss_G: 1.2178 D(x): 0.6700 D(G(z)): 0.1621 / 0.3297\n",
      "[0/10][307/938] Loss_D: 0.7810 Loss_G: 3.0708 D(x): 0.8463 D(G(z)): 0.4130 / 0.0592\n",
      "[0/10][308/938] Loss_D: 0.5503 Loss_G: 1.6739 D(x): 0.6755 D(G(z)): 0.1231 / 0.2110\n",
      "[0/10][309/938] Loss_D: 0.5411 Loss_G: 1.7485 D(x): 0.7997 D(G(z)): 0.2426 / 0.2035\n",
      "[0/10][310/938] Loss_D: 0.5908 Loss_G: 2.7768 D(x): 0.8442 D(G(z)): 0.3033 / 0.0805\n",
      "[0/10][311/938] Loss_D: 0.4522 Loss_G: 2.0830 D(x): 0.7515 D(G(z)): 0.1288 / 0.1566\n",
      "[0/10][312/938] Loss_D: 0.5052 Loss_G: 1.9552 D(x): 0.8005 D(G(z)): 0.2147 / 0.1686\n",
      "[0/10][313/938] Loss_D: 0.5254 Loss_G: 2.7205 D(x): 0.8404 D(G(z)): 0.2650 / 0.0910\n",
      "[0/10][314/938] Loss_D: 0.5102 Loss_G: 1.7279 D(x): 0.7085 D(G(z)): 0.1099 / 0.2152\n",
      "[0/10][315/938] Loss_D: 0.5227 Loss_G: 3.1702 D(x): 0.9026 D(G(z)): 0.3137 / 0.0561\n",
      "[0/10][316/938] Loss_D: 0.6325 Loss_G: 1.1154 D(x): 0.6299 D(G(z)): 0.0815 / 0.3900\n",
      "[0/10][317/938] Loss_D: 0.7949 Loss_G: 3.6117 D(x): 0.9265 D(G(z)): 0.4725 / 0.0329\n",
      "[0/10][318/938] Loss_D: 0.5844 Loss_G: 1.6821 D(x): 0.6300 D(G(z)): 0.0682 / 0.2290\n",
      "[0/10][319/938] Loss_D: 0.5916 Loss_G: 2.7239 D(x): 0.8836 D(G(z)): 0.3139 / 0.0864\n",
      "[0/10][320/938] Loss_D: 0.4876 Loss_G: 1.8981 D(x): 0.7200 D(G(z)): 0.0994 / 0.1838\n",
      "[0/10][321/938] Loss_D: 0.4560 Loss_G: 2.9785 D(x): 0.8974 D(G(z)): 0.2678 / 0.0675\n",
      "[0/10][322/938] Loss_D: 0.4908 Loss_G: 1.4213 D(x): 0.6980 D(G(z)): 0.0831 / 0.2784\n",
      "[0/10][323/938] Loss_D: 0.6655 Loss_G: 4.0391 D(x): 0.9272 D(G(z)): 0.4059 / 0.0238\n",
      "[0/10][324/938] Loss_D: 0.8451 Loss_G: 1.2542 D(x): 0.5059 D(G(z)): 0.0532 / 0.3511\n",
      "[0/10][325/938] Loss_D: 0.6485 Loss_G: 3.6086 D(x): 0.8928 D(G(z)): 0.3775 / 0.0395\n",
      "[0/10][326/938] Loss_D: 0.9489 Loss_G: 0.3888 D(x): 0.4834 D(G(z)): 0.0959 / 0.7129\n",
      "[0/10][327/938] Loss_D: 2.0439 Loss_G: 5.2519 D(x): 0.9750 D(G(z)): 0.8181 / 0.0108\n",
      "[0/10][328/938] Loss_D: 2.1720 Loss_G: 0.6739 D(x): 0.1738 D(G(z)): 0.0217 / 0.5891\n",
      "[0/10][329/938] Loss_D: 1.3962 Loss_G: 2.8991 D(x): 0.9448 D(G(z)): 0.6235 / 0.1081\n",
      "[0/10][330/938] Loss_D: 0.9079 Loss_G: 2.7395 D(x): 0.6356 D(G(z)): 0.2357 / 0.0980\n",
      "[0/10][331/938] Loss_D: 0.7984 Loss_G: 1.8941 D(x): 0.6851 D(G(z)): 0.2374 / 0.1928\n",
      "[0/10][332/938] Loss_D: 0.6337 Loss_G: 2.3190 D(x): 0.7955 D(G(z)): 0.2778 / 0.1315\n",
      "[0/10][333/938] Loss_D: 0.6833 Loss_G: 1.3083 D(x): 0.6900 D(G(z)): 0.2011 / 0.3069\n",
      "[0/10][334/938] Loss_D: 0.7848 Loss_G: 3.8100 D(x): 0.8760 D(G(z)): 0.4196 / 0.0335\n",
      "[0/10][335/938] Loss_D: 0.8670 Loss_G: 1.1300 D(x): 0.5186 D(G(z)): 0.0673 / 0.3745\n",
      "[0/10][336/938] Loss_D: 0.8387 Loss_G: 3.8879 D(x): 0.9159 D(G(z)): 0.4703 / 0.0374\n",
      "[0/10][337/938] Loss_D: 0.7472 Loss_G: 1.3684 D(x): 0.5751 D(G(z)): 0.0905 / 0.3142\n",
      "[0/10][338/938] Loss_D: 0.5908 Loss_G: 2.1141 D(x): 0.8576 D(G(z)): 0.3198 / 0.1525\n",
      "[0/10][339/938] Loss_D: 0.5556 Loss_G: 2.5442 D(x): 0.7867 D(G(z)): 0.2371 / 0.0949\n",
      "[0/10][340/938] Loss_D: 0.4350 Loss_G: 2.0921 D(x): 0.7693 D(G(z)): 0.1276 / 0.1519\n",
      "[0/10][341/938] Loss_D: 0.4593 Loss_G: 2.7306 D(x): 0.8648 D(G(z)): 0.2470 / 0.0799\n",
      "[0/10][342/938] Loss_D: 0.5174 Loss_G: 1.8236 D(x): 0.7562 D(G(z)): 0.1770 / 0.1924\n",
      "[0/10][343/938] Loss_D: 0.4311 Loss_G: 2.6592 D(x): 0.8498 D(G(z)): 0.2099 / 0.0940\n",
      "[0/10][344/938] Loss_D: 0.4716 Loss_G: 1.9747 D(x): 0.7799 D(G(z)): 0.1689 / 0.1632\n",
      "[0/10][345/938] Loss_D: 0.4209 Loss_G: 2.9210 D(x): 0.8432 D(G(z)): 0.1933 / 0.0718\n",
      "[0/10][346/938] Loss_D: 0.6239 Loss_G: 1.2211 D(x): 0.6905 D(G(z)): 0.1862 / 0.3331\n",
      "[0/10][347/938] Loss_D: 0.5421 Loss_G: 4.0587 D(x): 0.9069 D(G(z)): 0.3175 / 0.0253\n",
      "[0/10][348/938] Loss_D: 0.8920 Loss_G: 1.0320 D(x): 0.4967 D(G(z)): 0.0824 / 0.4551\n",
      "[0/10][349/938] Loss_D: 0.8258 Loss_G: 3.4768 D(x): 0.8120 D(G(z)): 0.3901 / 0.0443\n",
      "[0/10][350/938] Loss_D: 0.9817 Loss_G: 0.2427 D(x): 0.5139 D(G(z)): 0.1409 / 0.7999\n",
      "[0/10][351/938] Loss_D: 2.1648 Loss_G: 6.5431 D(x): 0.9819 D(G(z)): 0.8175 / 0.0072\n",
      "[0/10][352/938] Loss_D: 3.1174 Loss_G: 1.0778 D(x): 0.1374 D(G(z)): 0.0477 / 0.4665\n",
      "[0/10][353/938] Loss_D: 1.5123 Loss_G: 2.2584 D(x): 0.8323 D(G(z)): 0.5833 / 0.1487\n",
      "[0/10][354/938] Loss_D: 0.8881 Loss_G: 1.9433 D(x): 0.6370 D(G(z)): 0.2771 / 0.1999\n",
      "[0/10][355/938] Loss_D: 0.8629 Loss_G: 1.2297 D(x): 0.6339 D(G(z)): 0.2288 / 0.3582\n",
      "[0/10][356/938] Loss_D: 0.8284 Loss_G: 2.5911 D(x): 0.8382 D(G(z)): 0.4200 / 0.0935\n",
      "[0/10][357/938] Loss_D: 0.8159 Loss_G: 1.5475 D(x): 0.6264 D(G(z)): 0.2223 / 0.2592\n",
      "[0/10][358/938] Loss_D: 0.6898 Loss_G: 2.5561 D(x): 0.8046 D(G(z)): 0.3164 / 0.0955\n",
      "[0/10][359/938] Loss_D: 0.7085 Loss_G: 1.0965 D(x): 0.6312 D(G(z)): 0.1568 / 0.3788\n",
      "[0/10][360/938] Loss_D: 0.9383 Loss_G: 4.8600 D(x): 0.9368 D(G(z)): 0.4957 / 0.0144\n",
      "[0/10][361/938] Loss_D: 1.5010 Loss_G: 0.4595 D(x): 0.3115 D(G(z)): 0.0460 / 0.6812\n",
      "[0/10][362/938] Loss_D: 1.3003 Loss_G: 3.7704 D(x): 0.9726 D(G(z)): 0.6462 / 0.0364\n",
      "[0/10][363/938] Loss_D: 1.0330 Loss_G: 1.3946 D(x): 0.5139 D(G(z)): 0.1266 / 0.3008\n",
      "[0/10][364/938] Loss_D: 0.6978 Loss_G: 1.7217 D(x): 0.7927 D(G(z)): 0.3246 / 0.2022\n",
      "[0/10][365/938] Loss_D: 0.5567 Loss_G: 2.5828 D(x): 0.8222 D(G(z)): 0.2670 / 0.0957\n",
      "[0/10][366/938] Loss_D: 0.5950 Loss_G: 1.6555 D(x): 0.6753 D(G(z)): 0.1414 / 0.2406\n",
      "[0/10][367/938] Loss_D: 0.7167 Loss_G: 3.0469 D(x): 0.8734 D(G(z)): 0.4095 / 0.0557\n",
      "[0/10][368/938] Loss_D: 0.7600 Loss_G: 1.2707 D(x): 0.5620 D(G(z)): 0.0852 / 0.3372\n",
      "[0/10][369/938] Loss_D: 0.7200 Loss_G: 2.9904 D(x): 0.9120 D(G(z)): 0.4105 / 0.0685\n",
      "[0/10][370/938] Loss_D: 0.8057 Loss_G: 1.2725 D(x): 0.5743 D(G(z)): 0.1463 / 0.3312\n",
      "[0/10][371/938] Loss_D: 0.8993 Loss_G: 3.8138 D(x): 0.8862 D(G(z)): 0.4830 / 0.0325\n",
      "[0/10][372/938] Loss_D: 1.3034 Loss_G: 0.6509 D(x): 0.3668 D(G(z)): 0.0519 / 0.5603\n",
      "[0/10][373/938] Loss_D: 1.0787 Loss_G: 4.4862 D(x): 0.9763 D(G(z)): 0.6031 / 0.0157\n",
      "[0/10][374/938] Loss_D: 1.1178 Loss_G: 1.1239 D(x): 0.3962 D(G(z)): 0.0401 / 0.3872\n",
      "[0/10][375/938] Loss_D: 0.8803 Loss_G: 1.9584 D(x): 0.8406 D(G(z)): 0.4290 / 0.1943\n",
      "[0/10][376/938] Loss_D: 0.5792 Loss_G: 2.7951 D(x): 0.8183 D(G(z)): 0.2760 / 0.0822\n",
      "[0/10][377/938] Loss_D: 0.7847 Loss_G: 1.6035 D(x): 0.6496 D(G(z)): 0.2332 / 0.2276\n",
      "[0/10][378/938] Loss_D: 0.5620 Loss_G: 1.4363 D(x): 0.7355 D(G(z)): 0.1876 / 0.2823\n",
      "[0/10][379/938] Loss_D: 0.7133 Loss_G: 3.1365 D(x): 0.8967 D(G(z)): 0.4149 / 0.0568\n",
      "[0/10][380/938] Loss_D: 0.9083 Loss_G: 0.7834 D(x): 0.4949 D(G(z)): 0.1145 / 0.4997\n",
      "[0/10][381/938] Loss_D: 0.9651 Loss_G: 3.7112 D(x): 0.9195 D(G(z)): 0.5288 / 0.0379\n",
      "[0/10][382/938] Loss_D: 0.9107 Loss_G: 1.1273 D(x): 0.4766 D(G(z)): 0.0627 / 0.3830\n",
      "[0/10][383/938] Loss_D: 0.7131 Loss_G: 2.8449 D(x): 0.9118 D(G(z)): 0.4164 / 0.0785\n",
      "[0/10][384/938] Loss_D: 0.4958 Loss_G: 2.3265 D(x): 0.7480 D(G(z)): 0.1483 / 0.1270\n",
      "[0/10][385/938] Loss_D: 0.5645 Loss_G: 1.6987 D(x): 0.7491 D(G(z)): 0.2019 / 0.2154\n",
      "[0/10][386/938] Loss_D: 0.4726 Loss_G: 2.3994 D(x): 0.8398 D(G(z)): 0.2250 / 0.1159\n",
      "[0/10][387/938] Loss_D: 0.4770 Loss_G: 2.5374 D(x): 0.8059 D(G(z)): 0.1987 / 0.1004\n",
      "[0/10][388/938] Loss_D: 0.4857 Loss_G: 1.7983 D(x): 0.7469 D(G(z)): 0.1314 / 0.2123\n",
      "[0/10][389/938] Loss_D: 0.5633 Loss_G: 3.6758 D(x): 0.9025 D(G(z)): 0.3409 / 0.0365\n",
      "[0/10][390/938] Loss_D: 0.7453 Loss_G: 1.3312 D(x): 0.6062 D(G(z)): 0.0882 / 0.3068\n",
      "[0/10][391/938] Loss_D: 0.6610 Loss_G: 3.5932 D(x): 0.8911 D(G(z)): 0.3958 / 0.0329\n",
      "[0/10][392/938] Loss_D: 0.8962 Loss_G: 0.6619 D(x): 0.4746 D(G(z)): 0.0499 / 0.5756\n",
      "[0/10][393/938] Loss_D: 1.2918 Loss_G: 6.1399 D(x): 0.9737 D(G(z)): 0.6609 / 0.0039\n",
      "[0/10][394/938] Loss_D: 2.1382 Loss_G: 0.4030 D(x): 0.1934 D(G(z)): 0.0278 / 0.7111\n",
      "[0/10][395/938] Loss_D: 1.4382 Loss_G: 3.8722 D(x): 0.9601 D(G(z)): 0.6505 / 0.0314\n",
      "[0/10][396/938] Loss_D: 1.5536 Loss_G: 0.4783 D(x): 0.3032 D(G(z)): 0.0714 / 0.6623\n",
      "[0/10][397/938] Loss_D: 1.9317 Loss_G: 3.4410 D(x): 0.9546 D(G(z)): 0.7567 / 0.0500\n",
      "[0/10][398/938] Loss_D: 1.2712 Loss_G: 1.3254 D(x): 0.4226 D(G(z)): 0.1208 / 0.3531\n",
      "[0/10][399/938] Loss_D: 0.6073 Loss_G: 2.0243 D(x): 0.8708 D(G(z)): 0.3344 / 0.1605\n",
      "[0/10][400/938] Loss_D: 0.8562 Loss_G: 3.0011 D(x): 0.7986 D(G(z)): 0.3749 / 0.0666\n",
      "[0/10][401/938] Loss_D: 0.9975 Loss_G: 0.8476 D(x): 0.4643 D(G(z)): 0.1010 / 0.5014\n",
      "[0/10][402/938] Loss_D: 1.0724 Loss_G: 2.9302 D(x): 0.9244 D(G(z)): 0.5241 / 0.0803\n",
      "[0/10][403/938] Loss_D: 0.6110 Loss_G: 2.6266 D(x): 0.7176 D(G(z)): 0.1893 / 0.1005\n",
      "[0/10][404/938] Loss_D: 0.5767 Loss_G: 1.2275 D(x): 0.6667 D(G(z)): 0.1239 / 0.3296\n",
      "[0/10][405/938] Loss_D: 0.8101 Loss_G: 3.0728 D(x): 0.8774 D(G(z)): 0.4403 / 0.0616\n",
      "[0/10][406/938] Loss_D: 0.8565 Loss_G: 1.3434 D(x): 0.5534 D(G(z)): 0.1267 / 0.3130\n",
      "[0/10][407/938] Loss_D: 0.5438 Loss_G: 2.3482 D(x): 0.8754 D(G(z)): 0.2971 / 0.1249\n",
      "[0/10][408/938] Loss_D: 0.5951 Loss_G: 1.5806 D(x): 0.7103 D(G(z)): 0.1826 / 0.2485\n",
      "[0/10][409/938] Loss_D: 0.6161 Loss_G: 3.0997 D(x): 0.8721 D(G(z)): 0.3483 / 0.0584\n",
      "[0/10][410/938] Loss_D: 0.7387 Loss_G: 1.0275 D(x): 0.5481 D(G(z)): 0.0716 / 0.4049\n",
      "[0/10][411/938] Loss_D: 1.0299 Loss_G: 5.2921 D(x): 0.9524 D(G(z)): 0.5772 / 0.0076\n",
      "[0/10][412/938] Loss_D: 2.3675 Loss_G: 0.8930 D(x): 0.1445 D(G(z)): 0.0347 / 0.4985\n",
      "[0/10][413/938] Loss_D: 1.0112 Loss_G: 2.9339 D(x): 0.8884 D(G(z)): 0.5215 / 0.0636\n",
      "[0/10][414/938] Loss_D: 1.0402 Loss_G: 0.8289 D(x): 0.4906 D(G(z)): 0.1602 / 0.4810\n",
      "[0/10][415/938] Loss_D: 1.2267 Loss_G: 3.4884 D(x): 0.8805 D(G(z)): 0.5879 / 0.0458\n",
      "[0/10][416/938] Loss_D: 1.2291 Loss_G: 0.6948 D(x): 0.3985 D(G(z)): 0.1001 / 0.5488\n",
      "[0/10][417/938] Loss_D: 1.0628 Loss_G: 2.7437 D(x): 0.8603 D(G(z)): 0.5368 / 0.0820\n",
      "[0/10][418/938] Loss_D: 0.8839 Loss_G: 1.8518 D(x): 0.6255 D(G(z)): 0.2266 / 0.2041\n",
      "[0/10][419/938] Loss_D: 0.7949 Loss_G: 1.9611 D(x): 0.7377 D(G(z)): 0.3255 / 0.1768\n",
      "[0/10][420/938] Loss_D: 0.8831 Loss_G: 1.6275 D(x): 0.6552 D(G(z)): 0.2960 / 0.2432\n",
      "[0/10][421/938] Loss_D: 0.8632 Loss_G: 1.7124 D(x): 0.7081 D(G(z)): 0.3342 / 0.2204\n",
      "[0/10][422/938] Loss_D: 0.7608 Loss_G: 2.8826 D(x): 0.8015 D(G(z)): 0.3810 / 0.0706\n",
      "[0/10][423/938] Loss_D: 0.9448 Loss_G: 0.7957 D(x): 0.4985 D(G(z)): 0.1181 / 0.5053\n",
      "[0/10][424/938] Loss_D: 1.2146 Loss_G: 3.5999 D(x): 0.8923 D(G(z)): 0.6056 / 0.0528\n",
      "[0/10][425/938] Loss_D: 1.2058 Loss_G: 0.7794 D(x): 0.3906 D(G(z)): 0.0662 / 0.5044\n",
      "[0/10][426/938] Loss_D: 1.1086 Loss_G: 2.7923 D(x): 0.8842 D(G(z)): 0.5600 / 0.0893\n",
      "[0/10][427/938] Loss_D: 0.7199 Loss_G: 1.8019 D(x): 0.6342 D(G(z)): 0.1599 / 0.2055\n",
      "[0/10][428/938] Loss_D: 0.7835 Loss_G: 1.6213 D(x): 0.7305 D(G(z)): 0.3131 / 0.2275\n",
      "[0/10][429/938] Loss_D: 0.4909 Loss_G: 2.1792 D(x): 0.8064 D(G(z)): 0.2197 / 0.1329\n",
      "[0/10][430/938] Loss_D: 0.6993 Loss_G: 1.8976 D(x): 0.7276 D(G(z)): 0.2713 / 0.1807\n",
      "[0/10][431/938] Loss_D: 0.6208 Loss_G: 2.0134 D(x): 0.7491 D(G(z)): 0.2419 / 0.1676\n",
      "[0/10][432/938] Loss_D: 0.7174 Loss_G: 1.2348 D(x): 0.6690 D(G(z)): 0.2310 / 0.3213\n",
      "[0/10][433/938] Loss_D: 0.8334 Loss_G: 3.9777 D(x): 0.8982 D(G(z)): 0.4752 / 0.0246\n",
      "[0/10][434/938] Loss_D: 1.3142 Loss_G: 0.5705 D(x): 0.3408 D(G(z)): 0.0483 / 0.6102\n",
      "[0/10][435/938] Loss_D: 0.9812 Loss_G: 3.0661 D(x): 0.9158 D(G(z)): 0.5029 / 0.0640\n",
      "[0/10][436/938] Loss_D: 0.8964 Loss_G: 1.0402 D(x): 0.5561 D(G(z)): 0.1799 / 0.4201\n",
      "[0/10][437/938] Loss_D: 0.9700 Loss_G: 3.5040 D(x): 0.8972 D(G(z)): 0.5081 / 0.0514\n",
      "[0/10][438/938] Loss_D: 0.7979 Loss_G: 1.7748 D(x): 0.5817 D(G(z)): 0.1053 / 0.2425\n",
      "[0/10][439/938] Loss_D: 0.7599 Loss_G: 1.5321 D(x): 0.7367 D(G(z)): 0.2925 / 0.2619\n",
      "[0/10][440/938] Loss_D: 0.7084 Loss_G: 2.5913 D(x): 0.8124 D(G(z)): 0.3340 / 0.1001\n",
      "[0/10][441/938] Loss_D: 0.7434 Loss_G: 1.6540 D(x): 0.6679 D(G(z)): 0.2335 / 0.2225\n",
      "[0/10][442/938] Loss_D: 0.6218 Loss_G: 2.5327 D(x): 0.8301 D(G(z)): 0.3101 / 0.1031\n",
      "[0/10][443/938] Loss_D: 0.6196 Loss_G: 1.5013 D(x): 0.6806 D(G(z)): 0.1492 / 0.2536\n",
      "[0/10][444/938] Loss_D: 0.6574 Loss_G: 3.0769 D(x): 0.8466 D(G(z)): 0.3440 / 0.0635\n",
      "[0/10][445/938] Loss_D: 0.7495 Loss_G: 0.9028 D(x): 0.5896 D(G(z)): 0.1202 / 0.4466\n",
      "[0/10][446/938] Loss_D: 0.9136 Loss_G: 4.1883 D(x): 0.9343 D(G(z)): 0.5297 / 0.0178\n",
      "[0/10][447/938] Loss_D: 1.1789 Loss_G: 0.4843 D(x): 0.3688 D(G(z)): 0.0496 / 0.6472\n",
      "[0/10][448/938] Loss_D: 1.4055 Loss_G: 4.2782 D(x): 0.9491 D(G(z)): 0.6750 / 0.0207\n",
      "[0/10][449/938] Loss_D: 1.1026 Loss_G: 1.0726 D(x): 0.4134 D(G(z)): 0.0652 / 0.4001\n",
      "[0/10][450/938] Loss_D: 0.6145 Loss_G: 2.7031 D(x): 0.8956 D(G(z)): 0.3432 / 0.0983\n",
      "[0/10][451/938] Loss_D: 0.6706 Loss_G: 2.1925 D(x): 0.7322 D(G(z)): 0.2238 / 0.1488\n",
      "[0/10][452/938] Loss_D: 0.6303 Loss_G: 2.3565 D(x): 0.7675 D(G(z)): 0.2684 / 0.1260\n",
      "[0/10][453/938] Loss_D: 0.7109 Loss_G: 1.7948 D(x): 0.7092 D(G(z)): 0.2429 / 0.2081\n",
      "[0/10][454/938] Loss_D: 0.5299 Loss_G: 2.3332 D(x): 0.8009 D(G(z)): 0.2328 / 0.1301\n",
      "[0/10][455/938] Loss_D: 0.6057 Loss_G: 2.2970 D(x): 0.7787 D(G(z)): 0.2453 / 0.1224\n",
      "[0/10][456/938] Loss_D: 0.6281 Loss_G: 1.6879 D(x): 0.7152 D(G(z)): 0.2054 / 0.2216\n",
      "[0/10][457/938] Loss_D: 0.5619 Loss_G: 2.7782 D(x): 0.8317 D(G(z)): 0.2726 / 0.0756\n",
      "[0/10][458/938] Loss_D: 0.4300 Loss_G: 1.9809 D(x): 0.7431 D(G(z)): 0.1043 / 0.1775\n",
      "[0/10][459/938] Loss_D: 0.4468 Loss_G: 2.7250 D(x): 0.8773 D(G(z)): 0.2470 / 0.0807\n",
      "[0/10][460/938] Loss_D: 0.4221 Loss_G: 2.4326 D(x): 0.8031 D(G(z)): 0.1593 / 0.1167\n",
      "[0/10][461/938] Loss_D: 0.5397 Loss_G: 2.3888 D(x): 0.8108 D(G(z)): 0.2351 / 0.1131\n",
      "[0/10][462/938] Loss_D: 0.6205 Loss_G: 1.7577 D(x): 0.7040 D(G(z)): 0.1768 / 0.2151\n",
      "[0/10][463/938] Loss_D: 0.6499 Loss_G: 3.4367 D(x): 0.8468 D(G(z)): 0.3276 / 0.0461\n",
      "[0/10][464/938] Loss_D: 0.7387 Loss_G: 1.0206 D(x): 0.6076 D(G(z)): 0.1251 / 0.3982\n",
      "[0/10][465/938] Loss_D: 0.7369 Loss_G: 5.4976 D(x): 0.9580 D(G(z)): 0.4502 / 0.0066\n",
      "[0/10][466/938] Loss_D: 1.4639 Loss_G: 0.2774 D(x): 0.2932 D(G(z)): 0.0208 / 0.7772\n",
      "[0/10][467/938] Loss_D: 1.7410 Loss_G: 5.6306 D(x): 0.9860 D(G(z)): 0.7643 / 0.0099\n",
      "[0/10][468/938] Loss_D: 2.0483 Loss_G: 0.7770 D(x): 0.2759 D(G(z)): 0.0425 / 0.5554\n",
      "[0/10][469/938] Loss_D: 1.8455 Loss_G: 4.2019 D(x): 0.9186 D(G(z)): 0.7011 / 0.0277\n",
      "[0/10][470/938] Loss_D: 1.6481 Loss_G: 0.6763 D(x): 0.2863 D(G(z)): 0.0575 / 0.5586\n",
      "[0/10][471/938] Loss_D: 1.2912 Loss_G: 3.4261 D(x): 0.9339 D(G(z)): 0.6306 / 0.0481\n",
      "[0/10][472/938] Loss_D: 1.0149 Loss_G: 1.7011 D(x): 0.5135 D(G(z)): 0.1247 / 0.2566\n",
      "[0/10][473/938] Loss_D: 0.9194 Loss_G: 2.1452 D(x): 0.8130 D(G(z)): 0.4260 / 0.1585\n",
      "[0/10][474/938] Loss_D: 0.8319 Loss_G: 1.7292 D(x): 0.6381 D(G(z)): 0.2467 / 0.2264\n",
      "[0/10][475/938] Loss_D: 0.7791 Loss_G: 2.1743 D(x): 0.7528 D(G(z)): 0.3293 / 0.1519\n",
      "[0/10][476/938] Loss_D: 0.7798 Loss_G: 2.3900 D(x): 0.7182 D(G(z)): 0.3096 / 0.1274\n",
      "[0/10][477/938] Loss_D: 0.8298 Loss_G: 1.0869 D(x): 0.5966 D(G(z)): 0.1805 / 0.3887\n",
      "[0/10][478/938] Loss_D: 0.7827 Loss_G: 3.8263 D(x): 0.9255 D(G(z)): 0.4506 / 0.0430\n",
      "[0/10][479/938] Loss_D: 0.9373 Loss_G: 1.3182 D(x): 0.5221 D(G(z)): 0.1266 / 0.3449\n",
      "[0/10][480/938] Loss_D: 0.6429 Loss_G: 2.6419 D(x): 0.8929 D(G(z)): 0.3515 / 0.1009\n",
      "[0/10][481/938] Loss_D: 0.7226 Loss_G: 1.9665 D(x): 0.6968 D(G(z)): 0.2399 / 0.1731\n",
      "[0/10][482/938] Loss_D: 0.7459 Loss_G: 1.5473 D(x): 0.6755 D(G(z)): 0.2537 / 0.2679\n",
      "[0/10][483/938] Loss_D: 0.6344 Loss_G: 2.7215 D(x): 0.8425 D(G(z)): 0.3183 / 0.0975\n",
      "[0/10][484/938] Loss_D: 0.8086 Loss_G: 1.5228 D(x): 0.6264 D(G(z)): 0.1992 / 0.2613\n",
      "[0/10][485/938] Loss_D: 0.6503 Loss_G: 3.1063 D(x): 0.8634 D(G(z)): 0.3479 / 0.0621\n",
      "[0/10][486/938] Loss_D: 0.6964 Loss_G: 1.3987 D(x): 0.6061 D(G(z)): 0.1114 / 0.3096\n",
      "[0/10][487/938] Loss_D: 0.5778 Loss_G: 2.7292 D(x): 0.8926 D(G(z)): 0.3397 / 0.0778\n",
      "[0/10][488/938] Loss_D: 0.4688 Loss_G: 2.8993 D(x): 0.7964 D(G(z)): 0.1850 / 0.0780\n",
      "[0/10][489/938] Loss_D: 0.7617 Loss_G: 0.8037 D(x): 0.5861 D(G(z)): 0.1319 / 0.4860\n",
      "[0/10][490/938] Loss_D: 1.1384 Loss_G: 4.8863 D(x): 0.9408 D(G(z)): 0.6042 / 0.0125\n",
      "[0/10][491/938] Loss_D: 1.7805 Loss_G: 0.7641 D(x): 0.2400 D(G(z)): 0.0503 / 0.5189\n",
      "[0/10][492/938] Loss_D: 0.9617 Loss_G: 3.9994 D(x): 0.9244 D(G(z)): 0.5348 / 0.0250\n",
      "[0/10][493/938] Loss_D: 1.7335 Loss_G: 0.2169 D(x): 0.2536 D(G(z)): 0.0625 / 0.8209\n",
      "[0/10][494/938] Loss_D: 2.2778 Loss_G: 3.6285 D(x): 0.9857 D(G(z)): 0.8433 / 0.0424\n",
      "[0/10][495/938] Loss_D: 0.9061 Loss_G: 2.5260 D(x): 0.5938 D(G(z)): 0.1970 / 0.1157\n",
      "[0/10][496/938] Loss_D: 0.9686 Loss_G: 0.6629 D(x): 0.5224 D(G(z)): 0.1473 / 0.5588\n",
      "[0/10][497/938] Loss_D: 1.7198 Loss_G: 4.9679 D(x): 0.9391 D(G(z)): 0.7623 / 0.0160\n",
      "[0/10][498/938] Loss_D: 1.8767 Loss_G: 1.6423 D(x): 0.2612 D(G(z)): 0.0293 / 0.2719\n",
      "[0/10][499/938] Loss_D: 0.8993 Loss_G: 1.3824 D(x): 0.7936 D(G(z)): 0.4012 / 0.3200\n",
      "[0/10][500/938] Loss_D: 1.1448 Loss_G: 2.4804 D(x): 0.7269 D(G(z)): 0.4352 / 0.1143\n",
      "[0/10][501/938] Loss_D: 0.8875 Loss_G: 1.6614 D(x): 0.6156 D(G(z)): 0.2020 / 0.2339\n",
      "[0/10][502/938] Loss_D: 0.6084 Loss_G: 2.1009 D(x): 0.8072 D(G(z)): 0.2862 / 0.1523\n",
      "[0/10][503/938] Loss_D: 0.8369 Loss_G: 1.7715 D(x): 0.6859 D(G(z)): 0.3127 / 0.1945\n",
      "[0/10][504/938] Loss_D: 0.7104 Loss_G: 1.3945 D(x): 0.6632 D(G(z)): 0.2192 / 0.2754\n",
      "[0/10][505/938] Loss_D: 0.6073 Loss_G: 2.5426 D(x): 0.8538 D(G(z)): 0.3245 / 0.1068\n",
      "[0/10][506/938] Loss_D: 0.6609 Loss_G: 1.6482 D(x): 0.6823 D(G(z)): 0.2021 / 0.2166\n",
      "[0/10][507/938] Loss_D: 0.4869 Loss_G: 1.9878 D(x): 0.8203 D(G(z)): 0.2312 / 0.1566\n",
      "[0/10][508/938] Loss_D: 0.6255 Loss_G: 1.8344 D(x): 0.7417 D(G(z)): 0.2300 / 0.1900\n",
      "[0/10][509/938] Loss_D: 0.7244 Loss_G: 2.9644 D(x): 0.8284 D(G(z)): 0.3794 / 0.0674\n",
      "[0/10][510/938] Loss_D: 1.0305 Loss_G: 0.7098 D(x): 0.4550 D(G(z)): 0.0828 / 0.5291\n",
      "[0/10][511/938] Loss_D: 1.1486 Loss_G: 3.7667 D(x): 0.9346 D(G(z)): 0.6044 / 0.0305\n",
      "[0/10][512/938] Loss_D: 1.2067 Loss_G: 0.7155 D(x): 0.3854 D(G(z)): 0.0733 / 0.5411\n",
      "[0/10][513/938] Loss_D: 1.1640 Loss_G: 3.5711 D(x): 0.9122 D(G(z)): 0.5972 / 0.0414\n",
      "[0/10][514/938] Loss_D: 1.0853 Loss_G: 1.2937 D(x): 0.4390 D(G(z)): 0.0892 / 0.3464\n",
      "[0/10][515/938] Loss_D: 0.7762 Loss_G: 2.5674 D(x): 0.8925 D(G(z)): 0.4288 / 0.0997\n",
      "[0/10][516/938] Loss_D: 0.8603 Loss_G: 1.5444 D(x): 0.6099 D(G(z)): 0.2249 / 0.2727\n",
      "[0/10][517/938] Loss_D: 0.6762 Loss_G: 2.3084 D(x): 0.8351 D(G(z)): 0.3504 / 0.1208\n",
      "[0/10][518/938] Loss_D: 0.6744 Loss_G: 1.7344 D(x): 0.6575 D(G(z)): 0.1760 / 0.2178\n",
      "[0/10][519/938] Loss_D: 0.6614 Loss_G: 2.6643 D(x): 0.8602 D(G(z)): 0.3639 / 0.0915\n",
      "[0/10][520/938] Loss_D: 0.6810 Loss_G: 1.7426 D(x): 0.6582 D(G(z)): 0.1747 / 0.2182\n",
      "[0/10][521/938] Loss_D: 0.4523 Loss_G: 1.7758 D(x): 0.7834 D(G(z)): 0.1647 / 0.1970\n",
      "[0/10][522/938] Loss_D: 0.7071 Loss_G: 4.1676 D(x): 0.9029 D(G(z)): 0.4158 / 0.0232\n",
      "[0/10][523/938] Loss_D: 1.0752 Loss_G: 0.9102 D(x): 0.4360 D(G(z)): 0.0635 / 0.4768\n",
      "[0/10][524/938] Loss_D: 0.7164 Loss_G: 3.0636 D(x): 0.9053 D(G(z)): 0.4069 / 0.0651\n",
      "[0/10][525/938] Loss_D: 0.6387 Loss_G: 2.4457 D(x): 0.7279 D(G(z)): 0.2182 / 0.1359\n",
      "[0/10][526/938] Loss_D: 0.8738 Loss_G: 1.4895 D(x): 0.6518 D(G(z)): 0.2733 / 0.2598\n",
      "[0/10][527/938] Loss_D: 0.4736 Loss_G: 2.4093 D(x): 0.8216 D(G(z)): 0.2137 / 0.1146\n",
      "[0/10][528/938] Loss_D: 0.5985 Loss_G: 3.3916 D(x): 0.8523 D(G(z)): 0.3165 / 0.0433\n",
      "[0/10][529/938] Loss_D: 0.9588 Loss_G: 0.7471 D(x): 0.4845 D(G(z)): 0.0858 / 0.5085\n",
      "[0/10][530/938] Loss_D: 1.0907 Loss_G: 6.4003 D(x): 0.9601 D(G(z)): 0.5892 / 0.0037\n",
      "[0/10][531/938] Loss_D: 3.0743 Loss_G: 0.4199 D(x): 0.0773 D(G(z)): 0.0144 / 0.7737\n",
      "[0/10][532/938] Loss_D: 2.1907 Loss_G: 3.6759 D(x): 0.9499 D(G(z)): 0.7517 / 0.0347\n",
      "[0/10][533/938] Loss_D: 1.1678 Loss_G: 0.9155 D(x): 0.4249 D(G(z)): 0.0869 / 0.4560\n",
      "[0/10][534/938] Loss_D: 0.9661 Loss_G: 2.6200 D(x): 0.8637 D(G(z)): 0.5018 / 0.1022\n",
      "[0/10][535/938] Loss_D: 0.8904 Loss_G: 1.9578 D(x): 0.6444 D(G(z)): 0.2431 / 0.1955\n",
      "[0/10][536/938] Loss_D: 0.8766 Loss_G: 0.7628 D(x): 0.5629 D(G(z)): 0.2026 / 0.5182\n",
      "[0/10][537/938] Loss_D: 1.3941 Loss_G: 3.2524 D(x): 0.9034 D(G(z)): 0.6524 / 0.0590\n",
      "[0/10][538/938] Loss_D: 1.2803 Loss_G: 1.0669 D(x): 0.3953 D(G(z)): 0.1201 / 0.4028\n",
      "[0/10][539/938] Loss_D: 0.9558 Loss_G: 1.9584 D(x): 0.8074 D(G(z)): 0.4356 / 0.1815\n",
      "[0/10][540/938] Loss_D: 0.6571 Loss_G: 2.3490 D(x): 0.7707 D(G(z)): 0.2821 / 0.1242\n",
      "[0/10][541/938] Loss_D: 0.7649 Loss_G: 1.5601 D(x): 0.6684 D(G(z)): 0.2403 / 0.2423\n",
      "[0/10][542/938] Loss_D: 0.9686 Loss_G: 1.5382 D(x): 0.6642 D(G(z)): 0.3466 / 0.2746\n",
      "[0/10][543/938] Loss_D: 0.8350 Loss_G: 2.2944 D(x): 0.7638 D(G(z)): 0.3463 / 0.1238\n",
      "[0/10][544/938] Loss_D: 0.6165 Loss_G: 1.5793 D(x): 0.6688 D(G(z)): 0.1335 / 0.2572\n",
      "[0/10][545/938] Loss_D: 0.8413 Loss_G: 3.4321 D(x): 0.8778 D(G(z)): 0.4592 / 0.0432\n",
      "[0/10][546/938] Loss_D: 0.9567 Loss_G: 0.9869 D(x): 0.4720 D(G(z)): 0.0877 / 0.4191\n",
      "[0/10][547/938] Loss_D: 0.9330 Loss_G: 2.7577 D(x): 0.8552 D(G(z)): 0.4763 / 0.0812\n",
      "[0/10][548/938] Loss_D: 0.5668 Loss_G: 2.2431 D(x): 0.7324 D(G(z)): 0.1807 / 0.1281\n",
      "[0/10][549/938] Loss_D: 0.7575 Loss_G: 1.3082 D(x): 0.6671 D(G(z)): 0.2355 / 0.3029\n",
      "[0/10][550/938] Loss_D: 0.6631 Loss_G: 3.0290 D(x): 0.8532 D(G(z)): 0.3538 / 0.0668\n",
      "[0/10][551/938] Loss_D: 0.8701 Loss_G: 1.1006 D(x): 0.5645 D(G(z)): 0.1638 / 0.3967\n",
      "[0/10][552/938] Loss_D: 0.9700 Loss_G: 3.8663 D(x): 0.9189 D(G(z)): 0.5310 / 0.0337\n",
      "[0/10][553/938] Loss_D: 0.9437 Loss_G: 1.3808 D(x): 0.4783 D(G(z)): 0.0567 / 0.3059\n",
      "[0/10][554/938] Loss_D: 0.7579 Loss_G: 2.6672 D(x): 0.9249 D(G(z)): 0.4402 / 0.0940\n",
      "[0/10][555/938] Loss_D: 0.7800 Loss_G: 1.1506 D(x): 0.5567 D(G(z)): 0.1006 / 0.3682\n",
      "[0/10][556/938] Loss_D: 0.7463 Loss_G: 2.4955 D(x): 0.8908 D(G(z)): 0.4087 / 0.1067\n",
      "[0/10][557/938] Loss_D: 0.5751 Loss_G: 2.4830 D(x): 0.7511 D(G(z)): 0.2083 / 0.1067\n",
      "[0/10][558/938] Loss_D: 0.5747 Loss_G: 1.6574 D(x): 0.7096 D(G(z)): 0.1690 / 0.2184\n",
      "[0/10][559/938] Loss_D: 0.5422 Loss_G: 2.1686 D(x): 0.8143 D(G(z)): 0.2555 / 0.1373\n",
      "[0/10][560/938] Loss_D: 0.4391 Loss_G: 1.9611 D(x): 0.8003 D(G(z)): 0.1689 / 0.1646\n",
      "[0/10][561/938] Loss_D: 0.4855 Loss_G: 2.7851 D(x): 0.8565 D(G(z)): 0.2552 / 0.0768\n",
      "[0/10][562/938] Loss_D: 0.5104 Loss_G: 2.0641 D(x): 0.7499 D(G(z)): 0.1558 / 0.1542\n",
      "[0/10][563/938] Loss_D: 0.5017 Loss_G: 2.3902 D(x): 0.8261 D(G(z)): 0.2289 / 0.1246\n",
      "[0/10][564/938] Loss_D: 0.5995 Loss_G: 1.5036 D(x): 0.6997 D(G(z)): 0.1722 / 0.2648\n",
      "[0/10][565/938] Loss_D: 0.7518 Loss_G: 4.7569 D(x): 0.8823 D(G(z)): 0.4098 / 0.0122\n",
      "[0/10][566/938] Loss_D: 1.7468 Loss_G: 0.3673 D(x): 0.2465 D(G(z)): 0.0227 / 0.7242\n",
      "[0/10][567/938] Loss_D: 1.7412 Loss_G: 6.2193 D(x): 0.9724 D(G(z)): 0.7306 / 0.0038\n",
      "[0/10][568/938] Loss_D: 3.1679 Loss_G: 0.2349 D(x): 0.0915 D(G(z)): 0.0138 / 0.8144\n",
      "[0/10][569/938] Loss_D: 2.5370 Loss_G: 3.6110 D(x): 0.9659 D(G(z)): 0.8505 / 0.0518\n",
      "[0/10][570/938] Loss_D: 1.4748 Loss_G: 1.2819 D(x): 0.3571 D(G(z)): 0.1228 / 0.3570\n",
      "[0/10][571/938] Loss_D: 0.8950 Loss_G: 1.6616 D(x): 0.7984 D(G(z)): 0.4145 / 0.2460\n",
      "[0/10][572/938] Loss_D: 0.9023 Loss_G: 2.0170 D(x): 0.7221 D(G(z)): 0.3585 / 0.1732\n",
      "[0/10][573/938] Loss_D: 1.0201 Loss_G: 1.6663 D(x): 0.6306 D(G(z)): 0.3096 / 0.2573\n",
      "[0/10][574/938] Loss_D: 1.0841 Loss_G: 1.2139 D(x): 0.6149 D(G(z)): 0.3127 / 0.3672\n",
      "[0/10][575/938] Loss_D: 1.0529 Loss_G: 2.9134 D(x): 0.8337 D(G(z)): 0.5103 / 0.0819\n",
      "[0/10][576/938] Loss_D: 1.3074 Loss_G: 0.8947 D(x): 0.3747 D(G(z)): 0.1515 / 0.4663\n",
      "[0/10][577/938] Loss_D: 1.0661 Loss_G: 2.6911 D(x): 0.8962 D(G(z)): 0.5642 / 0.0846\n",
      "[0/10][578/938] Loss_D: 0.7632 Loss_G: 1.7814 D(x): 0.5974 D(G(z)): 0.1527 / 0.2072\n",
      "[0/10][579/938] Loss_D: 0.6269 Loss_G: 1.8955 D(x): 0.7926 D(G(z)): 0.2913 / 0.1793\n",
      "[0/10][580/938] Loss_D: 0.7418 Loss_G: 2.0686 D(x): 0.7191 D(G(z)): 0.2840 / 0.1615\n",
      "[0/10][581/938] Loss_D: 0.7208 Loss_G: 2.1912 D(x): 0.7456 D(G(z)): 0.3023 / 0.1411\n",
      "[0/10][582/938] Loss_D: 0.9306 Loss_G: 1.0299 D(x): 0.5731 D(G(z)): 0.1991 / 0.4073\n",
      "[0/10][583/938] Loss_D: 1.0561 Loss_G: 3.8079 D(x): 0.8827 D(G(z)): 0.5371 / 0.0350\n",
      "[0/10][584/938] Loss_D: 1.1609 Loss_G: 0.9203 D(x): 0.3932 D(G(z)): 0.0620 / 0.4487\n",
      "[0/10][585/938] Loss_D: 0.9131 Loss_G: 2.5987 D(x): 0.9034 D(G(z)): 0.4994 / 0.0938\n",
      "[0/10][586/938] Loss_D: 0.7019 Loss_G: 1.7325 D(x): 0.6364 D(G(z)): 0.1673 / 0.2066\n",
      "[0/10][587/938] Loss_D: 0.9442 Loss_G: 1.6782 D(x): 0.6876 D(G(z)): 0.3741 / 0.2270\n",
      "[0/10][588/938] Loss_D: 0.7288 Loss_G: 2.1250 D(x): 0.7506 D(G(z)): 0.3029 / 0.1522\n",
      "[0/10][589/938] Loss_D: 0.6307 Loss_G: 2.0520 D(x): 0.7442 D(G(z)): 0.2324 / 0.1597\n",
      "[0/10][590/938] Loss_D: 0.6305 Loss_G: 1.5712 D(x): 0.7039 D(G(z)): 0.2088 / 0.2426\n",
      "[0/10][591/938] Loss_D: 0.7725 Loss_G: 2.0133 D(x): 0.7680 D(G(z)): 0.3369 / 0.1634\n",
      "[0/10][592/938] Loss_D: 0.5483 Loss_G: 2.6648 D(x): 0.8090 D(G(z)): 0.2563 / 0.0886\n",
      "[0/10][593/938] Loss_D: 0.6567 Loss_G: 1.4942 D(x): 0.6620 D(G(z)): 0.1537 / 0.2904\n",
      "[0/10][594/938] Loss_D: 0.6052 Loss_G: 2.7866 D(x): 0.8822 D(G(z)): 0.3440 / 0.0767\n",
      "[0/10][595/938] Loss_D: 0.5220 Loss_G: 1.9463 D(x): 0.7038 D(G(z)): 0.1163 / 0.1831\n",
      "[0/10][596/938] Loss_D: 0.5303 Loss_G: 2.4942 D(x): 0.8555 D(G(z)): 0.2824 / 0.0998\n",
      "[0/10][597/938] Loss_D: 0.6622 Loss_G: 2.0661 D(x): 0.7368 D(G(z)): 0.2543 / 0.1488\n",
      "[0/10][598/938] Loss_D: 0.4624 Loss_G: 2.7677 D(x): 0.8261 D(G(z)): 0.2200 / 0.0735\n",
      "[0/10][599/938] Loss_D: 0.6422 Loss_G: 0.7847 D(x): 0.6333 D(G(z)): 0.1210 / 0.4976\n",
      "[0/10][600/938] Loss_D: 1.1731 Loss_G: 5.1703 D(x): 0.9032 D(G(z)): 0.5960 / 0.0123\n",
      "[0/10][601/938] Loss_D: 1.4707 Loss_G: 0.7211 D(x): 0.3192 D(G(z)): 0.0462 / 0.5785\n",
      "[0/10][602/938] Loss_D: 1.1594 Loss_G: 3.7035 D(x): 0.9197 D(G(z)): 0.5781 / 0.0328\n",
      "[0/10][603/938] Loss_D: 0.8438 Loss_G: 1.4847 D(x): 0.5408 D(G(z)): 0.1120 / 0.2849\n",
      "[0/10][604/938] Loss_D: 0.6725 Loss_G: 1.2659 D(x): 0.7080 D(G(z)): 0.2328 / 0.3284\n",
      "[0/10][605/938] Loss_D: 0.9651 Loss_G: 4.0731 D(x): 0.9339 D(G(z)): 0.5176 / 0.0248\n",
      "[0/10][606/938] Loss_D: 1.1993 Loss_G: 1.0372 D(x): 0.4049 D(G(z)): 0.0558 / 0.4077\n",
      "[0/10][607/938] Loss_D: 0.9690 Loss_G: 3.2222 D(x): 0.9014 D(G(z)): 0.5034 / 0.0555\n",
      "[0/10][608/938] Loss_D: 1.1336 Loss_G: 0.8314 D(x): 0.4373 D(G(z)): 0.1178 / 0.4782\n",
      "[0/10][609/938] Loss_D: 1.1193 Loss_G: 4.2241 D(x): 0.9518 D(G(z)): 0.5975 / 0.0257\n",
      "[0/10][610/938] Loss_D: 1.1118 Loss_G: 1.4041 D(x): 0.4314 D(G(z)): 0.0693 / 0.2821\n",
      "[0/10][611/938] Loss_D: 0.7743 Loss_G: 3.2761 D(x): 0.9385 D(G(z)): 0.4563 / 0.0474\n",
      "[0/10][612/938] Loss_D: 0.7539 Loss_G: 1.2892 D(x): 0.5681 D(G(z)): 0.0906 / 0.3296\n",
      "[0/10][613/938] Loss_D: 1.0838 Loss_G: 3.7190 D(x): 0.8903 D(G(z)): 0.5509 / 0.0330\n",
      "[0/10][614/938] Loss_D: 1.4355 Loss_G: 0.5715 D(x): 0.3268 D(G(z)): 0.0835 / 0.6048\n",
      "[0/10][615/938] Loss_D: 0.8876 Loss_G: 3.3384 D(x): 0.9369 D(G(z)): 0.5034 / 0.0506\n",
      "[0/10][616/938] Loss_D: 0.6208 Loss_G: 2.3021 D(x): 0.6776 D(G(z)): 0.1498 / 0.1315\n",
      "[0/10][617/938] Loss_D: 0.7350 Loss_G: 0.9142 D(x): 0.6086 D(G(z)): 0.1482 / 0.4449\n",
      "[0/10][618/938] Loss_D: 1.0499 Loss_G: 4.0932 D(x): 0.9599 D(G(z)): 0.5730 / 0.0245\n",
      "[0/10][619/938] Loss_D: 0.9891 Loss_G: 1.3584 D(x): 0.4896 D(G(z)): 0.1207 / 0.3071\n",
      "[0/10][620/938] Loss_D: 0.8143 Loss_G: 2.4734 D(x): 0.8401 D(G(z)): 0.4059 / 0.1070\n",
      "[0/10][621/938] Loss_D: 0.6728 Loss_G: 1.9363 D(x): 0.6767 D(G(z)): 0.2014 / 0.1790\n",
      "[0/10][622/938] Loss_D: 0.7466 Loss_G: 2.3508 D(x): 0.7740 D(G(z)): 0.3383 / 0.1251\n",
      "[0/10][623/938] Loss_D: 0.6395 Loss_G: 1.8619 D(x): 0.6945 D(G(z)): 0.1996 / 0.2028\n",
      "[0/10][624/938] Loss_D: 0.5220 Loss_G: 2.5328 D(x): 0.8249 D(G(z)): 0.2424 / 0.0978\n",
      "[0/10][625/938] Loss_D: 0.5747 Loss_G: 1.8227 D(x): 0.7196 D(G(z)): 0.1663 / 0.2101\n",
      "[0/10][626/938] Loss_D: 0.5077 Loss_G: 2.8351 D(x): 0.8681 D(G(z)): 0.2857 / 0.0712\n",
      "[0/10][627/938] Loss_D: 0.5596 Loss_G: 1.5759 D(x): 0.7127 D(G(z)): 0.1537 / 0.2352\n",
      "[0/10][628/938] Loss_D: 0.6331 Loss_G: 2.8751 D(x): 0.8705 D(G(z)): 0.3632 / 0.0652\n",
      "[0/10][629/938] Loss_D: 0.6079 Loss_G: 1.8050 D(x): 0.6375 D(G(z)): 0.1012 / 0.2069\n",
      "[0/10][630/938] Loss_D: 0.6773 Loss_G: 1.9768 D(x): 0.7784 D(G(z)): 0.2964 / 0.1627\n",
      "[0/10][631/938] Loss_D: 0.4208 Loss_G: 2.8497 D(x): 0.8444 D(G(z)): 0.1970 / 0.0705\n",
      "[0/10][632/938] Loss_D: 0.4133 Loss_G: 1.7713 D(x): 0.7696 D(G(z)): 0.1140 / 0.2143\n",
      "[0/10][633/938] Loss_D: 0.5718 Loss_G: 3.5416 D(x): 0.8951 D(G(z)): 0.3380 / 0.0368\n",
      "[0/10][634/938] Loss_D: 0.6369 Loss_G: 1.4326 D(x): 0.6239 D(G(z)): 0.0942 / 0.3017\n",
      "[0/10][635/938] Loss_D: 0.6622 Loss_G: 2.8785 D(x): 0.8561 D(G(z)): 0.3537 / 0.0734\n",
      "[0/10][636/938] Loss_D: 0.5762 Loss_G: 2.3821 D(x): 0.7303 D(G(z)): 0.1646 / 0.1180\n",
      "[0/10][637/938] Loss_D: 0.5829 Loss_G: 1.7629 D(x): 0.7364 D(G(z)): 0.1917 / 0.2183\n",
      "[0/10][638/938] Loss_D: 0.5840 Loss_G: 3.7996 D(x): 0.8980 D(G(z)): 0.3424 / 0.0297\n",
      "[0/10][639/938] Loss_D: 0.9444 Loss_G: 1.0234 D(x): 0.4753 D(G(z)): 0.0495 / 0.4199\n",
      "[0/10][640/938] Loss_D: 1.1775 Loss_G: 6.8453 D(x): 0.9647 D(G(z)): 0.6370 / 0.0014\n",
      "[0/10][641/938] Loss_D: 4.7704 Loss_G: 0.3569 D(x): 0.0144 D(G(z)): 0.0039 / 0.7258\n",
      "[0/10][642/938] Loss_D: 1.7619 Loss_G: 5.0858 D(x): 0.9708 D(G(z)): 0.7506 / 0.0128\n",
      "[0/10][643/938] Loss_D: 2.3797 Loss_G: 0.0828 D(x): 0.1620 D(G(z)): 0.0519 / 0.9246\n",
      "[0/10][644/938] Loss_D: 3.1571 Loss_G: 4.2673 D(x): 0.9843 D(G(z)): 0.9127 / 0.0227\n",
      "[0/10][645/938] Loss_D: 1.4008 Loss_G: 1.1854 D(x): 0.3773 D(G(z)): 0.1127 / 0.3845\n",
      "[0/10][646/938] Loss_D: 1.1234 Loss_G: 2.4146 D(x): 0.8419 D(G(z)): 0.5171 / 0.1252\n",
      "[0/10][647/938] Loss_D: 0.9478 Loss_G: 1.6433 D(x): 0.5776 D(G(z)): 0.2321 / 0.2426\n",
      "[0/10][648/938] Loss_D: 0.9205 Loss_G: 1.8226 D(x): 0.6758 D(G(z)): 0.3451 / 0.2053\n",
      "[0/10][649/938] Loss_D: 0.7140 Loss_G: 2.7584 D(x): 0.8069 D(G(z)): 0.3479 / 0.0854\n",
      "[0/10][650/938] Loss_D: 0.7417 Loss_G: 1.4979 D(x): 0.6217 D(G(z)): 0.1649 / 0.2688\n",
      "[0/10][651/938] Loss_D: 1.0665 Loss_G: 3.1003 D(x): 0.8113 D(G(z)): 0.5172 / 0.0603\n",
      "[0/10][652/938] Loss_D: 1.0132 Loss_G: 1.1110 D(x): 0.4695 D(G(z)): 0.0975 / 0.3868\n",
      "[0/10][653/938] Loss_D: 1.1082 Loss_G: 2.8770 D(x): 0.8232 D(G(z)): 0.5440 / 0.0730\n",
      "[0/10][654/938] Loss_D: 0.7083 Loss_G: 1.8666 D(x): 0.6052 D(G(z)): 0.0907 / 0.1967\n",
      "[0/10][655/938] Loss_D: 0.5239 Loss_G: 2.3124 D(x): 0.8645 D(G(z)): 0.2878 / 0.1169\n",
      "[0/10][656/938] Loss_D: 0.6332 Loss_G: 2.0434 D(x): 0.7381 D(G(z)): 0.2319 / 0.1571\n",
      "[0/10][657/938] Loss_D: 0.6579 Loss_G: 2.6027 D(x): 0.8124 D(G(z)): 0.3163 / 0.0995\n",
      "[0/10][658/938] Loss_D: 0.8312 Loss_G: 1.0724 D(x): 0.5927 D(G(z)): 0.1866 / 0.4028\n",
      "[0/10][659/938] Loss_D: 0.9558 Loss_G: 3.2848 D(x): 0.8611 D(G(z)): 0.4980 / 0.0480\n",
      "[0/10][660/938] Loss_D: 1.0237 Loss_G: 1.1941 D(x): 0.4633 D(G(z)): 0.0787 / 0.3579\n",
      "[0/10][661/938] Loss_D: 0.8479 Loss_G: 3.5252 D(x): 0.8778 D(G(z)): 0.4477 / 0.0404\n",
      "[0/10][662/938] Loss_D: 1.0080 Loss_G: 0.7999 D(x): 0.4622 D(G(z)): 0.1154 / 0.5338\n",
      "[0/10][663/938] Loss_D: 1.1998 Loss_G: 2.9803 D(x): 0.9196 D(G(z)): 0.5924 / 0.0674\n",
      "[0/10][664/938] Loss_D: 0.9519 Loss_G: 1.7509 D(x): 0.5515 D(G(z)): 0.2003 / 0.2225\n",
      "[0/10][665/938] Loss_D: 0.6127 Loss_G: 1.9780 D(x): 0.7819 D(G(z)): 0.2579 / 0.1617\n",
      "[0/10][666/938] Loss_D: 0.6877 Loss_G: 1.9269 D(x): 0.7437 D(G(z)): 0.2602 / 0.1760\n",
      "[0/10][667/938] Loss_D: 0.7079 Loss_G: 2.2958 D(x): 0.7581 D(G(z)): 0.3064 / 0.1325\n",
      "[0/10][668/938] Loss_D: 0.5228 Loss_G: 2.3443 D(x): 0.7852 D(G(z)): 0.2077 / 0.1196\n",
      "[0/10][669/938] Loss_D: 0.6879 Loss_G: 2.2351 D(x): 0.7363 D(G(z)): 0.2736 / 0.1309\n",
      "[0/10][670/938] Loss_D: 0.6743 Loss_G: 1.5566 D(x): 0.6924 D(G(z)): 0.1839 / 0.2512\n",
      "[0/10][671/938] Loss_D: 0.7315 Loss_G: 3.0967 D(x): 0.8663 D(G(z)): 0.3961 / 0.0634\n",
      "[0/10][672/938] Loss_D: 0.9664 Loss_G: 0.8353 D(x): 0.4722 D(G(z)): 0.0843 / 0.4897\n",
      "[0/10][673/938] Loss_D: 1.0075 Loss_G: 4.0538 D(x): 0.9332 D(G(z)): 0.5590 / 0.0232\n",
      "[0/10][674/938] Loss_D: 1.1860 Loss_G: 0.9215 D(x): 0.3773 D(G(z)): 0.0533 / 0.4579\n",
      "[0/10][675/938] Loss_D: 1.3145 Loss_G: 2.9181 D(x): 0.9047 D(G(z)): 0.6534 / 0.0687\n",
      "[0/10][676/938] Loss_D: 1.0202 Loss_G: 1.4099 D(x): 0.4846 D(G(z)): 0.1562 / 0.2968\n",
      "[0/10][677/938] Loss_D: 0.9081 Loss_G: 2.4157 D(x): 0.7908 D(G(z)): 0.4282 / 0.1231\n",
      "[0/10][678/938] Loss_D: 0.8148 Loss_G: 1.7556 D(x): 0.6132 D(G(z)): 0.1878 / 0.2013\n",
      "[0/10][679/938] Loss_D: 0.6539 Loss_G: 3.0539 D(x): 0.8482 D(G(z)): 0.3283 / 0.0661\n",
      "[0/10][680/938] Loss_D: 0.5993 Loss_G: 1.7687 D(x): 0.6970 D(G(z)): 0.1695 / 0.2100\n",
      "[0/10][681/938] Loss_D: 0.6993 Loss_G: 2.8646 D(x): 0.8355 D(G(z)): 0.3530 / 0.0748\n",
      "[0/10][682/938] Loss_D: 0.7196 Loss_G: 1.2533 D(x): 0.5770 D(G(z)): 0.1016 / 0.3309\n",
      "[0/10][683/938] Loss_D: 0.7024 Loss_G: 3.4406 D(x): 0.9126 D(G(z)): 0.4153 / 0.0411\n",
      "[0/10][684/938] Loss_D: 0.4726 Loss_G: 1.9997 D(x): 0.7182 D(G(z)): 0.1037 / 0.1644\n",
      "[0/10][685/938] Loss_D: 0.5105 Loss_G: 2.3894 D(x): 0.8532 D(G(z)): 0.2735 / 0.1092\n",
      "[0/10][686/938] Loss_D: 0.6927 Loss_G: 1.8666 D(x): 0.6897 D(G(z)): 0.2132 / 0.1875\n",
      "[0/10][687/938] Loss_D: 0.7246 Loss_G: 3.3551 D(x): 0.8190 D(G(z)): 0.3622 / 0.0439\n",
      "[0/10][688/938] Loss_D: 0.7848 Loss_G: 1.0025 D(x): 0.5327 D(G(z)): 0.0758 / 0.4185\n",
      "[0/10][689/938] Loss_D: 0.8868 Loss_G: 4.6477 D(x): 0.9302 D(G(z)): 0.4924 / 0.0141\n",
      "[0/10][690/938] Loss_D: 1.5781 Loss_G: 0.2203 D(x): 0.3080 D(G(z)): 0.1105 / 0.8115\n",
      "[0/10][691/938] Loss_D: 1.5304 Loss_G: 4.5669 D(x): 0.9692 D(G(z)): 0.6928 / 0.0205\n",
      "[0/10][692/938] Loss_D: 1.0294 Loss_G: 1.4580 D(x): 0.4974 D(G(z)): 0.0893 / 0.2871\n",
      "[0/10][693/938] Loss_D: 0.6840 Loss_G: 2.7456 D(x): 0.8469 D(G(z)): 0.3354 / 0.0904\n",
      "[0/10][694/938] Loss_D: 0.6983 Loss_G: 1.7426 D(x): 0.6806 D(G(z)): 0.1879 / 0.2191\n",
      "[0/10][695/938] Loss_D: 0.6872 Loss_G: 3.0662 D(x): 0.8506 D(G(z)): 0.3703 / 0.0579\n",
      "[0/10][696/938] Loss_D: 0.8160 Loss_G: 1.2912 D(x): 0.5775 D(G(z)): 0.1465 / 0.3328\n",
      "[0/10][697/938] Loss_D: 0.5687 Loss_G: 2.5692 D(x): 0.8545 D(G(z)): 0.3035 / 0.0970\n",
      "[0/10][698/938] Loss_D: 0.5427 Loss_G: 2.2930 D(x): 0.7620 D(G(z)): 0.1999 / 0.1248\n",
      "[0/10][699/938] Loss_D: 0.6226 Loss_G: 2.5409 D(x): 0.8111 D(G(z)): 0.2932 / 0.1032\n",
      "[0/10][700/938] Loss_D: 0.7972 Loss_G: 1.0736 D(x): 0.5892 D(G(z)): 0.1582 / 0.3863\n",
      "[0/10][701/938] Loss_D: 0.9266 Loss_G: 3.8677 D(x): 0.9169 D(G(z)): 0.5175 / 0.0299\n",
      "[0/10][702/938] Loss_D: 0.8320 Loss_G: 1.6461 D(x): 0.5341 D(G(z)): 0.0831 / 0.2506\n",
      "[0/10][703/938] Loss_D: 0.6036 Loss_G: 2.1671 D(x): 0.8046 D(G(z)): 0.2671 / 0.1563\n",
      "[0/10][704/938] Loss_D: 0.5287 Loss_G: 3.4222 D(x): 0.8479 D(G(z)): 0.2757 / 0.0420\n",
      "[0/10][705/938] Loss_D: 0.7893 Loss_G: 0.5395 D(x): 0.5711 D(G(z)): 0.1128 / 0.6118\n",
      "[0/10][706/938] Loss_D: 1.4959 Loss_G: 5.2479 D(x): 0.9523 D(G(z)): 0.7111 / 0.0084\n",
      "[0/10][707/938] Loss_D: 2.6858 Loss_G: 0.3708 D(x): 0.1085 D(G(z)): 0.0247 / 0.7334\n",
      "[0/10][708/938] Loss_D: 1.5991 Loss_G: 3.8391 D(x): 0.9467 D(G(z)): 0.7000 / 0.0353\n",
      "[0/10][709/938] Loss_D: 1.1236 Loss_G: 1.2853 D(x): 0.4226 D(G(z)): 0.1097 / 0.3574\n",
      "[0/10][710/938] Loss_D: 1.0690 Loss_G: 2.1782 D(x): 0.7689 D(G(z)): 0.4670 / 0.1486\n",
      "[0/10][711/938] Loss_D: 0.9156 Loss_G: 2.0285 D(x): 0.6390 D(G(z)): 0.3065 / 0.1605\n",
      "[0/10][712/938] Loss_D: 0.8662 Loss_G: 1.7945 D(x): 0.6635 D(G(z)): 0.2967 / 0.2052\n",
      "[0/10][713/938] Loss_D: 0.8469 Loss_G: 1.8050 D(x): 0.6918 D(G(z)): 0.3039 / 0.2012\n",
      "[0/10][714/938] Loss_D: 0.6483 Loss_G: 2.5546 D(x): 0.7983 D(G(z)): 0.2844 / 0.1093\n",
      "[0/10][715/938] Loss_D: 0.7044 Loss_G: 1.7192 D(x): 0.6727 D(G(z)): 0.2186 / 0.2253\n",
      "[0/10][716/938] Loss_D: 0.8382 Loss_G: 2.8319 D(x): 0.7858 D(G(z)): 0.3895 / 0.0770\n",
      "[0/10][717/938] Loss_D: 0.6937 Loss_G: 1.6051 D(x): 0.6391 D(G(z)): 0.1444 / 0.2290\n",
      "[0/10][718/938] Loss_D: 0.6291 Loss_G: 2.5847 D(x): 0.8332 D(G(z)): 0.3315 / 0.0932\n",
      "[0/10][719/938] Loss_D: 0.5154 Loss_G: 2.3804 D(x): 0.7600 D(G(z)): 0.1829 / 0.1174\n",
      "[0/10][720/938] Loss_D: 0.4563 Loss_G: 2.0864 D(x): 0.8036 D(G(z)): 0.1888 / 0.1490\n",
      "[0/10][721/938] Loss_D: 0.5614 Loss_G: 2.5270 D(x): 0.8050 D(G(z)): 0.2592 / 0.0952\n",
      "[0/10][722/938] Loss_D: 0.6699 Loss_G: 1.4586 D(x): 0.6693 D(G(z)): 0.1819 / 0.2702\n",
      "[0/10][723/938] Loss_D: 0.6627 Loss_G: 3.4831 D(x): 0.8917 D(G(z)): 0.3723 / 0.0394\n",
      "[0/10][724/938] Loss_D: 0.6421 Loss_G: 1.6272 D(x): 0.6214 D(G(z)): 0.0940 / 0.2416\n",
      "[0/10][725/938] Loss_D: 0.5486 Loss_G: 3.0550 D(x): 0.8910 D(G(z)): 0.3228 / 0.0621\n",
      "[0/10][726/938] Loss_D: 0.5454 Loss_G: 1.6148 D(x): 0.7032 D(G(z)): 0.1231 / 0.2362\n",
      "[0/10][727/938] Loss_D: 0.5647 Loss_G: 3.1129 D(x): 0.8804 D(G(z)): 0.3147 / 0.0566\n",
      "[0/10][728/938] Loss_D: 0.5962 Loss_G: 1.7003 D(x): 0.6813 D(G(z)): 0.1233 / 0.2199\n",
      "[0/10][729/938] Loss_D: 0.5604 Loss_G: 4.3736 D(x): 0.9190 D(G(z)): 0.3500 / 0.0168\n",
      "[0/10][730/938] Loss_D: 1.0495 Loss_G: 0.6957 D(x): 0.4133 D(G(z)): 0.0481 / 0.5643\n",
      "[0/10][731/938] Loss_D: 1.2810 Loss_G: 4.9736 D(x): 0.9409 D(G(z)): 0.6551 / 0.0128\n",
      "[0/10][732/938] Loss_D: 1.7359 Loss_G: 0.6827 D(x): 0.2506 D(G(z)): 0.0309 / 0.5614\n",
      "[0/10][733/938] Loss_D: 1.3339 Loss_G: 3.5357 D(x): 0.9502 D(G(z)): 0.6384 / 0.0492\n",
      "[0/10][734/938] Loss_D: 1.0612 Loss_G: 1.3424 D(x): 0.5055 D(G(z)): 0.1915 / 0.3035\n",
      "[0/10][735/938] Loss_D: 1.0053 Loss_G: 2.1945 D(x): 0.7480 D(G(z)): 0.4439 / 0.1376\n",
      "[0/10][736/938] Loss_D: 0.9431 Loss_G: 1.4843 D(x): 0.5724 D(G(z)): 0.2578 / 0.2778\n",
      "[0/10][737/938] Loss_D: 1.1224 Loss_G: 2.9418 D(x): 0.7655 D(G(z)): 0.4828 / 0.0772\n",
      "[0/10][738/938] Loss_D: 0.9901 Loss_G: 0.9716 D(x): 0.5017 D(G(z)): 0.1374 / 0.4316\n",
      "[0/10][739/938] Loss_D: 1.0365 Loss_G: 3.5537 D(x): 0.8956 D(G(z)): 0.5292 / 0.0450\n",
      "[0/10][740/938] Loss_D: 0.8192 Loss_G: 1.7727 D(x): 0.5801 D(G(z)): 0.1554 / 0.2241\n",
      "[0/10][741/938] Loss_D: 0.7950 Loss_G: 2.0440 D(x): 0.7399 D(G(z)): 0.3157 / 0.1665\n",
      "[0/10][742/938] Loss_D: 0.6803 Loss_G: 2.9351 D(x): 0.7693 D(G(z)): 0.2857 / 0.0737\n",
      "[0/10][743/938] Loss_D: 0.6890 Loss_G: 3.0795 D(x): 0.7497 D(G(z)): 0.2925 / 0.0549\n",
      "[0/10][744/938] Loss_D: 0.6499 Loss_G: 1.1454 D(x): 0.6117 D(G(z)): 0.0899 / 0.3591\n",
      "[0/10][745/938] Loss_D: 0.8824 Loss_G: 4.1573 D(x): 0.9161 D(G(z)): 0.5017 / 0.0229\n",
      "[0/10][746/938] Loss_D: 0.8745 Loss_G: 1.3046 D(x): 0.5327 D(G(z)): 0.1251 / 0.3461\n",
      "[0/10][747/938] Loss_D: 0.7506 Loss_G: 3.4875 D(x): 0.8721 D(G(z)): 0.3928 / 0.0445\n",
      "[0/10][748/938] Loss_D: 0.4494 Loss_G: 2.5469 D(x): 0.7748 D(G(z)): 0.1480 / 0.0944\n",
      "[0/10][749/938] Loss_D: 0.5040 Loss_G: 2.2736 D(x): 0.7950 D(G(z)): 0.2141 / 0.1219\n",
      "[0/10][750/938] Loss_D: 0.4809 Loss_G: 2.6241 D(x): 0.7993 D(G(z)): 0.1952 / 0.0944\n",
      "[0/10][751/938] Loss_D: 0.6369 Loss_G: 2.7472 D(x): 0.7450 D(G(z)): 0.2359 / 0.0867\n",
      "[0/10][752/938] Loss_D: 0.5836 Loss_G: 2.9869 D(x): 0.7786 D(G(z)): 0.2449 / 0.0708\n",
      "[0/10][753/938] Loss_D: 0.4148 Loss_G: 2.2109 D(x): 0.7698 D(G(z)): 0.1018 / 0.1378\n",
      "[0/10][754/938] Loss_D: 0.3603 Loss_G: 4.2628 D(x): 0.9327 D(G(z)): 0.2356 / 0.0190\n",
      "[0/10][755/938] Loss_D: 0.5365 Loss_G: 1.3647 D(x): 0.6822 D(G(z)): 0.0856 / 0.3008\n",
      "[0/10][756/938] Loss_D: 0.6635 Loss_G: 4.9586 D(x): 0.9541 D(G(z)): 0.4116 / 0.0096\n",
      "[0/10][757/938] Loss_D: 0.6948 Loss_G: 1.8907 D(x): 0.5700 D(G(z)): 0.0377 / 0.1992\n",
      "[0/10][758/938] Loss_D: 0.9192 Loss_G: 4.8581 D(x): 0.8800 D(G(z)): 0.4794 / 0.0119\n",
      "[0/10][759/938] Loss_D: 1.7653 Loss_G: 0.7194 D(x): 0.2535 D(G(z)): 0.0536 / 0.5553\n",
      "[0/10][760/938] Loss_D: 1.1691 Loss_G: 5.4805 D(x): 0.9170 D(G(z)): 0.6056 / 0.0058\n",
      "[0/10][761/938] Loss_D: 2.4105 Loss_G: 0.3967 D(x): 0.1496 D(G(z)): 0.0253 / 0.7227\n",
      "[0/10][762/938] Loss_D: 2.4783 Loss_G: 2.7538 D(x): 0.9802 D(G(z)): 0.8558 / 0.1153\n",
      "[0/10][763/938] Loss_D: 1.0766 Loss_G: 2.2803 D(x): 0.5651 D(G(z)): 0.2725 / 0.1383\n",
      "[0/10][764/938] Loss_D: 1.1522 Loss_G: 0.8094 D(x): 0.5125 D(G(z)): 0.2682 / 0.4790\n",
      "[0/10][765/938] Loss_D: 1.4390 Loss_G: 3.2482 D(x): 0.7813 D(G(z)): 0.6236 / 0.0794\n",
      "[0/10][766/938] Loss_D: 1.6662 Loss_G: 0.6874 D(x): 0.3456 D(G(z)): 0.1540 / 0.5677\n",
      "[0/10][767/938] Loss_D: 1.2904 Loss_G: 2.1777 D(x): 0.7907 D(G(z)): 0.5664 / 0.1463\n",
      "[0/10][768/938] Loss_D: 1.2853 Loss_G: 1.2919 D(x): 0.4908 D(G(z)): 0.3067 / 0.3272\n",
      "[0/10][769/938] Loss_D: 1.1201 Loss_G: 2.3518 D(x): 0.6880 D(G(z)): 0.4390 / 0.1527\n",
      "[0/10][770/938] Loss_D: 1.0683 Loss_G: 1.1439 D(x): 0.5549 D(G(z)): 0.2551 / 0.3675\n",
      "[0/10][771/938] Loss_D: 1.2963 Loss_G: 3.5411 D(x): 0.7864 D(G(z)): 0.5851 / 0.0415\n",
      "[0/10][772/938] Loss_D: 1.3929 Loss_G: 0.7207 D(x): 0.3307 D(G(z)): 0.0764 / 0.5284\n",
      "[0/10][773/938] Loss_D: 1.3452 Loss_G: 3.1238 D(x): 0.9340 D(G(z)): 0.6669 / 0.0693\n",
      "[0/10][774/938] Loss_D: 1.4068 Loss_G: 1.3566 D(x): 0.4376 D(G(z)): 0.2689 / 0.3099\n",
      "[0/10][775/938] Loss_D: 0.7677 Loss_G: 1.6811 D(x): 0.7458 D(G(z)): 0.3296 / 0.2154\n",
      "[0/10][776/938] Loss_D: 1.1042 Loss_G: 1.3226 D(x): 0.5977 D(G(z)): 0.3754 / 0.3045\n",
      "[0/10][777/938] Loss_D: 0.9736 Loss_G: 2.2273 D(x): 0.7322 D(G(z)): 0.4293 / 0.1385\n",
      "[0/10][778/938] Loss_D: 0.8700 Loss_G: 1.6563 D(x): 0.5998 D(G(z)): 0.2366 / 0.2465\n",
      "[0/10][779/938] Loss_D: 0.8873 Loss_G: 1.8422 D(x): 0.7041 D(G(z)): 0.3392 / 0.1958\n",
      "[0/10][780/938] Loss_D: 0.7328 Loss_G: 2.5298 D(x): 0.7792 D(G(z)): 0.3470 / 0.1037\n",
      "[0/10][781/938] Loss_D: 0.7871 Loss_G: 1.5863 D(x): 0.6283 D(G(z)): 0.2103 / 0.2487\n",
      "[0/10][782/938] Loss_D: 0.7487 Loss_G: 2.1994 D(x): 0.8001 D(G(z)): 0.3559 / 0.1439\n",
      "[0/10][783/938] Loss_D: 0.6808 Loss_G: 2.0566 D(x): 0.6984 D(G(z)): 0.2359 / 0.1579\n",
      "[0/10][784/938] Loss_D: 0.8134 Loss_G: 2.0843 D(x): 0.7245 D(G(z)): 0.3303 / 0.1465\n",
      "[0/10][785/938] Loss_D: 0.6240 Loss_G: 1.8643 D(x): 0.7331 D(G(z)): 0.2271 / 0.1802\n",
      "[0/10][786/938] Loss_D: 0.7520 Loss_G: 1.9855 D(x): 0.7404 D(G(z)): 0.3112 / 0.1685\n",
      "[0/10][787/938] Loss_D: 0.6994 Loss_G: 2.1103 D(x): 0.7206 D(G(z)): 0.2692 / 0.1476\n",
      "[0/10][788/938] Loss_D: 0.6021 Loss_G: 2.3582 D(x): 0.7700 D(G(z)): 0.2515 / 0.1147\n",
      "[0/10][789/938] Loss_D: 0.5953 Loss_G: 2.0032 D(x): 0.7353 D(G(z)): 0.2097 / 0.1709\n",
      "[0/10][790/938] Loss_D: 0.5965 Loss_G: 1.3600 D(x): 0.7144 D(G(z)): 0.1928 / 0.2992\n",
      "[0/10][791/938] Loss_D: 0.9177 Loss_G: 4.9234 D(x): 0.8963 D(G(z)): 0.5006 / 0.0107\n",
      "[0/10][792/938] Loss_D: 1.7825 Loss_G: 0.4044 D(x): 0.2307 D(G(z)): 0.0282 / 0.7217\n",
      "[0/10][793/938] Loss_D: 1.9828 Loss_G: 4.0478 D(x): 0.9726 D(G(z)): 0.8059 / 0.0352\n",
      "[0/10][794/938] Loss_D: 1.5215 Loss_G: 0.7785 D(x): 0.3565 D(G(z)): 0.1418 / 0.5074\n",
      "[0/10][795/938] Loss_D: 1.2759 Loss_G: 2.7509 D(x): 0.8441 D(G(z)): 0.5857 / 0.0911\n",
      "[0/10][796/938] Loss_D: 0.8282 Loss_G: 1.5116 D(x): 0.5795 D(G(z)): 0.1583 / 0.2604\n",
      "[0/10][797/938] Loss_D: 0.7823 Loss_G: 2.6247 D(x): 0.8110 D(G(z)): 0.3881 / 0.0931\n",
      "[0/10][798/938] Loss_D: 0.6487 Loss_G: 1.6885 D(x): 0.6509 D(G(z)): 0.1431 / 0.2126\n",
      "[0/10][799/938] Loss_D: 0.6391 Loss_G: 2.2548 D(x): 0.8076 D(G(z)): 0.2984 / 0.1286\n",
      "[0/10][800/938] Loss_D: 0.6349 Loss_G: 1.5938 D(x): 0.6985 D(G(z)): 0.1973 / 0.2419\n",
      "[0/10][801/938] Loss_D: 0.5994 Loss_G: 3.3238 D(x): 0.8932 D(G(z)): 0.3472 / 0.0471\n",
      "[0/10][802/938] Loss_D: 0.8477 Loss_G: 1.1789 D(x): 0.5509 D(G(z)): 0.1273 / 0.3591\n",
      "[0/10][803/938] Loss_D: 0.6940 Loss_G: 3.3746 D(x): 0.9270 D(G(z)): 0.4154 / 0.0441\n",
      "[0/10][804/938] Loss_D: 0.6538 Loss_G: 1.6130 D(x): 0.6227 D(G(z)): 0.0871 / 0.2274\n",
      "[0/10][805/938] Loss_D: 0.8687 Loss_G: 3.8374 D(x): 0.8801 D(G(z)): 0.4692 / 0.0299\n",
      "[0/10][806/938] Loss_D: 0.9999 Loss_G: 1.0740 D(x): 0.4701 D(G(z)): 0.1033 / 0.4078\n",
      "[0/10][807/938] Loss_D: 0.9530 Loss_G: 4.4065 D(x): 0.9316 D(G(z)): 0.5183 / 0.0174\n",
      "[0/10][808/938] Loss_D: 1.0694 Loss_G: 1.2773 D(x): 0.4372 D(G(z)): 0.0597 / 0.3240\n",
      "[0/10][809/938] Loss_D: 0.8291 Loss_G: 3.7805 D(x): 0.9451 D(G(z)): 0.4915 / 0.0312\n",
      "[0/10][810/938] Loss_D: 0.8800 Loss_G: 0.9547 D(x): 0.5232 D(G(z)): 0.1027 / 0.4345\n",
      "[0/10][811/938] Loss_D: 1.0929 Loss_G: 4.9749 D(x): 0.9309 D(G(z)): 0.5898 / 0.0098\n",
      "[0/10][812/938] Loss_D: 1.0670 Loss_G: 1.4523 D(x): 0.4360 D(G(z)): 0.0411 / 0.3003\n",
      "[0/10][813/938] Loss_D: 0.6448 Loss_G: 3.0325 D(x): 0.9055 D(G(z)): 0.3805 / 0.0650\n",
      "[0/10][814/938] Loss_D: 0.7961 Loss_G: 2.6212 D(x): 0.6753 D(G(z)): 0.2705 / 0.0918\n",
      "[0/10][815/938] Loss_D: 0.6947 Loss_G: 1.9433 D(x): 0.6863 D(G(z)): 0.2039 / 0.1781\n",
      "[0/10][816/938] Loss_D: 0.6306 Loss_G: 3.0199 D(x): 0.8022 D(G(z)): 0.2981 / 0.0637\n",
      "[0/10][817/938] Loss_D: 0.5603 Loss_G: 2.1448 D(x): 0.7342 D(G(z)): 0.1806 / 0.1523\n",
      "[0/10][818/938] Loss_D: 0.5174 Loss_G: 3.3686 D(x): 0.8649 D(G(z)): 0.2764 / 0.0489\n",
      "[0/10][819/938] Loss_D: 0.6852 Loss_G: 1.1535 D(x): 0.6264 D(G(z)): 0.1284 / 0.3538\n",
      "[0/10][820/938] Loss_D: 0.7067 Loss_G: 3.9917 D(x): 0.9277 D(G(z)): 0.4216 / 0.0248\n",
      "[0/10][821/938] Loss_D: 0.7660 Loss_G: 1.8154 D(x): 0.5972 D(G(z)): 0.1549 / 0.1956\n",
      "[0/10][822/938] Loss_D: 0.5192 Loss_G: 3.4702 D(x): 0.8958 D(G(z)): 0.3010 / 0.0405\n",
      "[0/10][823/938] Loss_D: 0.4652 Loss_G: 2.0604 D(x): 0.6923 D(G(z)): 0.0590 / 0.1720\n",
      "[0/10][824/938] Loss_D: 0.5083 Loss_G: 3.5076 D(x): 0.9162 D(G(z)): 0.3041 / 0.0434\n",
      "[0/10][825/938] Loss_D: 0.5294 Loss_G: 1.5800 D(x): 0.6936 D(G(z)): 0.0960 / 0.2409\n",
      "[0/10][826/938] Loss_D: 0.6151 Loss_G: 4.2645 D(x): 0.9093 D(G(z)): 0.3776 / 0.0176\n",
      "[0/10][827/938] Loss_D: 0.8233 Loss_G: 1.3848 D(x): 0.5252 D(G(z)): 0.0418 / 0.3243\n",
      "[0/10][828/938] Loss_D: 0.8164 Loss_G: 4.0491 D(x): 0.9495 D(G(z)): 0.4734 / 0.0222\n",
      "[0/10][829/938] Loss_D: 0.5464 Loss_G: 2.0190 D(x): 0.6631 D(G(z)): 0.0867 / 0.1634\n",
      "[0/10][830/938] Loss_D: 0.6053 Loss_G: 2.9242 D(x): 0.8393 D(G(z)): 0.2945 / 0.0671\n",
      "[0/10][831/938] Loss_D: 0.5933 Loss_G: 2.3667 D(x): 0.7376 D(G(z)): 0.2072 / 0.1139\n",
      "[0/10][832/938] Loss_D: 0.4533 Loss_G: 3.4419 D(x): 0.8326 D(G(z)): 0.2121 / 0.0401\n",
      "[0/10][833/938] Loss_D: 0.2927 Loss_G: 2.5472 D(x): 0.8077 D(G(z)): 0.0645 / 0.1112\n",
      "[0/10][834/938] Loss_D: 0.4005 Loss_G: 3.8415 D(x): 0.9055 D(G(z)): 0.2270 / 0.0297\n",
      "[0/10][835/938] Loss_D: 0.4561 Loss_G: 1.8755 D(x): 0.7264 D(G(z)): 0.0880 / 0.1944\n",
      "[0/10][836/938] Loss_D: 0.7843 Loss_G: 5.2138 D(x): 0.9169 D(G(z)): 0.4536 / 0.0083\n",
      "[0/10][837/938] Loss_D: 1.2499 Loss_G: 0.6454 D(x): 0.3532 D(G(z)): 0.0291 / 0.5771\n",
      "[0/10][838/938] Loss_D: 1.3145 Loss_G: 6.7779 D(x): 0.9688 D(G(z)): 0.6590 / 0.0023\n",
      "[0/10][839/938] Loss_D: 3.1465 Loss_G: 0.9213 D(x): 0.0779 D(G(z)): 0.0078 / 0.5467\n",
      "[0/10][840/938] Loss_D: 1.7686 Loss_G: 2.9498 D(x): 0.9210 D(G(z)): 0.6943 / 0.0897\n",
      "[0/10][841/938] Loss_D: 0.8864 Loss_G: 2.3857 D(x): 0.6064 D(G(z)): 0.2244 / 0.1345\n",
      "[0/10][842/938] Loss_D: 1.0668 Loss_G: 1.3016 D(x): 0.5783 D(G(z)): 0.3186 / 0.3276\n",
      "[0/10][843/938] Loss_D: 1.5324 Loss_G: 3.8267 D(x): 0.7505 D(G(z)): 0.6317 / 0.0318\n",
      "[0/10][844/938] Loss_D: 1.9252 Loss_G: 0.4503 D(x): 0.1886 D(G(z)): 0.0633 / 0.6740\n",
      "[0/10][845/938] Loss_D: 1.6625 Loss_G: 3.8916 D(x): 0.9470 D(G(z)): 0.7339 / 0.0323\n",
      "[0/10][846/938] Loss_D: 1.0036 Loss_G: 1.7093 D(x): 0.4595 D(G(z)): 0.0930 / 0.2334\n",
      "[0/10][847/938] Loss_D: 0.9056 Loss_G: 2.9610 D(x): 0.9071 D(G(z)): 0.4681 / 0.0701\n",
      "[0/10][848/938] Loss_D: 0.9379 Loss_G: 2.0783 D(x): 0.6220 D(G(z)): 0.2961 / 0.1583\n",
      "[0/10][849/938] Loss_D: 1.0127 Loss_G: 1.1558 D(x): 0.5735 D(G(z)): 0.2690 / 0.3532\n",
      "[0/10][850/938] Loss_D: 1.2730 Loss_G: 4.3523 D(x): 0.8597 D(G(z)): 0.6086 / 0.0195\n",
      "[0/10][851/938] Loss_D: 1.1971 Loss_G: 1.3789 D(x): 0.3794 D(G(z)): 0.0741 / 0.3192\n",
      "[0/10][852/938] Loss_D: 0.9956 Loss_G: 4.2533 D(x): 0.9043 D(G(z)): 0.5108 / 0.0192\n",
      "[0/10][853/938] Loss_D: 1.3040 Loss_G: 0.5350 D(x): 0.4051 D(G(z)): 0.1640 / 0.6147\n",
      "[0/10][854/938] Loss_D: 1.7197 Loss_G: 5.5641 D(x): 0.9547 D(G(z)): 0.7752 / 0.0072\n",
      "[0/10][855/938] Loss_D: 2.1849 Loss_G: 1.5997 D(x): 0.1887 D(G(z)): 0.0358 / 0.3015\n",
      "[0/10][856/938] Loss_D: 0.8675 Loss_G: 1.1675 D(x): 0.7243 D(G(z)): 0.3139 / 0.3731\n",
      "[0/10][857/938] Loss_D: 0.7719 Loss_G: 3.3176 D(x): 0.9086 D(G(z)): 0.4333 / 0.0550\n",
      "[0/10][858/938] Loss_D: 0.7471 Loss_G: 2.4299 D(x): 0.6583 D(G(z)): 0.1841 / 0.1316\n",
      "[0/10][859/938] Loss_D: 0.8231 Loss_G: 1.6922 D(x): 0.7025 D(G(z)): 0.3132 / 0.2228\n",
      "[0/10][860/938] Loss_D: 0.8575 Loss_G: 2.1249 D(x): 0.7176 D(G(z)): 0.3496 / 0.1440\n",
      "[0/10][861/938] Loss_D: 0.8765 Loss_G: 2.0281 D(x): 0.6836 D(G(z)): 0.3461 / 0.1642\n",
      "[0/10][862/938] Loss_D: 0.8478 Loss_G: 1.7772 D(x): 0.6301 D(G(z)): 0.2726 / 0.2010\n",
      "[0/10][863/938] Loss_D: 0.7103 Loss_G: 3.0361 D(x): 0.8231 D(G(z)): 0.3394 / 0.0688\n",
      "[0/10][864/938] Loss_D: 0.7092 Loss_G: 1.8350 D(x): 0.6569 D(G(z)): 0.1791 / 0.2076\n",
      "[0/10][865/938] Loss_D: 0.5945 Loss_G: 2.3839 D(x): 0.8024 D(G(z)): 0.2790 / 0.1173\n",
      "[0/10][866/938] Loss_D: 0.6464 Loss_G: 3.1381 D(x): 0.8040 D(G(z)): 0.2954 / 0.0626\n",
      "[0/10][867/938] Loss_D: 0.6819 Loss_G: 1.2310 D(x): 0.6177 D(G(z)): 0.1056 / 0.3527\n",
      "[0/10][868/938] Loss_D: 1.0784 Loss_G: 4.3094 D(x): 0.9298 D(G(z)): 0.5692 / 0.0213\n",
      "[0/10][869/938] Loss_D: 1.1965 Loss_G: 0.9935 D(x): 0.3843 D(G(z)): 0.0424 / 0.4293\n",
      "[0/10][870/938] Loss_D: 1.0014 Loss_G: 3.3112 D(x): 0.9094 D(G(z)): 0.5390 / 0.0509\n",
      "[0/10][871/938] Loss_D: 1.0972 Loss_G: 1.2431 D(x): 0.4756 D(G(z)): 0.1241 / 0.3292\n",
      "[0/10][872/938] Loss_D: 0.9098 Loss_G: 2.7303 D(x): 0.8296 D(G(z)): 0.4743 / 0.0791\n",
      "[0/10][873/938] Loss_D: 0.8504 Loss_G: 1.8099 D(x): 0.6019 D(G(z)): 0.2223 / 0.2055\n",
      "[0/10][874/938] Loss_D: 0.7910 Loss_G: 1.8848 D(x): 0.7063 D(G(z)): 0.2945 / 0.1850\n",
      "[0/10][875/938] Loss_D: 0.5965 Loss_G: 2.7530 D(x): 0.7931 D(G(z)): 0.2766 / 0.0871\n",
      "[0/10][876/938] Loss_D: 0.4072 Loss_G: 2.7887 D(x): 0.8031 D(G(z)): 0.1399 / 0.0900\n",
      "[0/10][877/938] Loss_D: 0.6628 Loss_G: 2.3272 D(x): 0.7652 D(G(z)): 0.2757 / 0.1178\n",
      "[0/10][878/938] Loss_D: 0.6024 Loss_G: 1.9831 D(x): 0.7332 D(G(z)): 0.1974 / 0.1714\n",
      "[0/10][879/938] Loss_D: 0.7152 Loss_G: 3.8263 D(x): 0.8469 D(G(z)): 0.3669 / 0.0318\n",
      "[0/10][880/938] Loss_D: 0.7993 Loss_G: 1.1348 D(x): 0.5236 D(G(z)): 0.0655 / 0.3893\n",
      "[0/10][881/938] Loss_D: 0.8370 Loss_G: 4.4345 D(x): 0.9431 D(G(z)): 0.5062 / 0.0160\n",
      "[0/10][882/938] Loss_D: 1.3457 Loss_G: 0.7347 D(x): 0.3570 D(G(z)): 0.0565 / 0.5425\n",
      "[0/10][883/938] Loss_D: 1.3306 Loss_G: 3.6099 D(x): 0.9641 D(G(z)): 0.6527 / 0.0506\n",
      "[0/10][884/938] Loss_D: 0.9830 Loss_G: 1.5557 D(x): 0.4984 D(G(z)): 0.1064 / 0.2661\n",
      "[0/10][885/938] Loss_D: 1.0241 Loss_G: 2.8977 D(x): 0.8009 D(G(z)): 0.4963 / 0.0834\n",
      "[0/10][886/938] Loss_D: 0.9453 Loss_G: 2.4281 D(x): 0.6366 D(G(z)): 0.2863 / 0.1304\n",
      "[0/10][887/938] Loss_D: 1.0283 Loss_G: 1.3835 D(x): 0.5611 D(G(z)): 0.2318 / 0.2900\n",
      "[0/10][888/938] Loss_D: 0.7188 Loss_G: 4.1622 D(x): 0.8524 D(G(z)): 0.3760 / 0.0210\n",
      "[0/10][889/938] Loss_D: 0.6651 Loss_G: 1.6686 D(x): 0.6006 D(G(z)): 0.0730 / 0.2345\n",
      "[0/10][890/938] Loss_D: 0.4966 Loss_G: 3.8872 D(x): 0.9484 D(G(z)): 0.3297 / 0.0289\n",
      "[0/10][891/938] Loss_D: 0.5429 Loss_G: 2.8467 D(x): 0.7444 D(G(z)): 0.1778 / 0.0787\n",
      "[0/10][892/938] Loss_D: 0.6596 Loss_G: 1.8457 D(x): 0.7040 D(G(z)): 0.1924 / 0.2040\n",
      "[0/10][893/938] Loss_D: 0.5434 Loss_G: 4.4833 D(x): 0.9348 D(G(z)): 0.3385 / 0.0162\n",
      "[0/10][894/938] Loss_D: 0.7707 Loss_G: 0.8612 D(x): 0.5648 D(G(z)): 0.0878 / 0.4815\n",
      "[0/10][895/938] Loss_D: 1.2294 Loss_G: 5.6976 D(x): 0.9601 D(G(z)): 0.6183 / 0.0058\n",
      "[0/10][896/938] Loss_D: 1.9106 Loss_G: 1.0003 D(x): 0.2069 D(G(z)): 0.0205 / 0.4340\n",
      "[0/10][897/938] Loss_D: 1.0520 Loss_G: 3.9481 D(x): 0.9388 D(G(z)): 0.5665 / 0.0279\n",
      "[0/10][898/938] Loss_D: 1.0204 Loss_G: 0.8870 D(x): 0.4529 D(G(z)): 0.0831 / 0.4662\n",
      "[0/10][899/938] Loss_D: 1.0822 Loss_G: 3.9364 D(x): 0.9296 D(G(z)): 0.5672 / 0.0267\n",
      "[0/10][900/938] Loss_D: 0.7738 Loss_G: 1.6585 D(x): 0.5683 D(G(z)): 0.0976 / 0.2360\n",
      "[0/10][901/938] Loss_D: 0.8971 Loss_G: 3.3216 D(x): 0.8565 D(G(z)): 0.4511 / 0.0581\n",
      "[0/10][902/938] Loss_D: 0.8871 Loss_G: 1.3448 D(x): 0.5495 D(G(z)): 0.1616 / 0.3211\n",
      "[0/10][903/938] Loss_D: 0.8161 Loss_G: 4.0915 D(x): 0.8770 D(G(z)): 0.4273 / 0.0239\n",
      "[0/10][904/938] Loss_D: 0.7229 Loss_G: 1.4050 D(x): 0.5944 D(G(z)): 0.1021 / 0.2992\n",
      "[0/10][905/938] Loss_D: 0.8430 Loss_G: 4.9297 D(x): 0.9218 D(G(z)): 0.4682 / 0.0114\n",
      "[0/10][906/938] Loss_D: 0.7676 Loss_G: 1.5754 D(x): 0.5417 D(G(z)): 0.0377 / 0.3034\n",
      "[0/10][907/938] Loss_D: 0.7598 Loss_G: 3.7682 D(x): 0.9137 D(G(z)): 0.4133 / 0.0390\n",
      "[0/10][908/938] Loss_D: 0.4530 Loss_G: 2.4456 D(x): 0.7278 D(G(z)): 0.0795 / 0.1148\n",
      "[0/10][909/938] Loss_D: 0.7729 Loss_G: 4.2326 D(x): 0.8895 D(G(z)): 0.4185 / 0.0213\n",
      "[0/10][910/938] Loss_D: 0.5960 Loss_G: 2.3875 D(x): 0.6197 D(G(z)): 0.0398 / 0.1528\n",
      "[0/10][911/938] Loss_D: 0.5316 Loss_G: 3.1016 D(x): 0.8632 D(G(z)): 0.2641 / 0.0634\n",
      "[0/10][912/938] Loss_D: 0.4412 Loss_G: 3.0765 D(x): 0.8094 D(G(z)): 0.1767 / 0.0632\n",
      "[0/10][913/938] Loss_D: 0.5747 Loss_G: 2.2958 D(x): 0.7632 D(G(z)): 0.1996 / 0.1390\n",
      "[0/10][914/938] Loss_D: 0.6698 Loss_G: 3.8117 D(x): 0.8365 D(G(z)): 0.3445 / 0.0305\n",
      "[0/10][915/938] Loss_D: 0.6323 Loss_G: 1.8074 D(x): 0.6210 D(G(z)): 0.0817 / 0.2201\n",
      "[0/10][916/938] Loss_D: 0.6303 Loss_G: 4.7508 D(x): 0.9522 D(G(z)): 0.3986 / 0.0124\n",
      "[0/10][917/938] Loss_D: 0.9690 Loss_G: 1.0187 D(x): 0.4803 D(G(z)): 0.0818 / 0.4209\n",
      "[0/10][918/938] Loss_D: 0.9215 Loss_G: 5.3976 D(x): 0.9635 D(G(z)): 0.5426 / 0.0063\n",
      "[0/10][919/938] Loss_D: 0.9719 Loss_G: 1.1948 D(x): 0.4577 D(G(z)): 0.0651 / 0.3954\n",
      "[0/10][920/938] Loss_D: 1.1260 Loss_G: 5.6599 D(x): 0.9361 D(G(z)): 0.5586 / 0.0051\n",
      "[0/10][921/938] Loss_D: 1.7273 Loss_G: 0.6115 D(x): 0.2537 D(G(z)): 0.0220 / 0.5884\n",
      "[0/10][922/938] Loss_D: 1.3053 Loss_G: 4.3174 D(x): 0.9507 D(G(z)): 0.6439 / 0.0209\n",
      "[0/10][923/938] Loss_D: 0.7665 Loss_G: 2.3202 D(x): 0.5863 D(G(z)): 0.0960 / 0.1447\n",
      "[0/10][924/938] Loss_D: 0.9188 Loss_G: 2.0362 D(x): 0.7438 D(G(z)): 0.3794 / 0.1678\n",
      "[0/10][925/938] Loss_D: 1.1659 Loss_G: 2.7325 D(x): 0.6613 D(G(z)): 0.4525 / 0.0821\n",
      "[0/10][926/938] Loss_D: 0.7978 Loss_G: 1.3159 D(x): 0.5787 D(G(z)): 0.1333 / 0.3105\n",
      "[0/10][927/938] Loss_D: 0.9186 Loss_G: 3.8264 D(x): 0.8777 D(G(z)): 0.4940 / 0.0276\n",
      "[0/10][928/938] Loss_D: 0.7608 Loss_G: 1.8265 D(x): 0.5643 D(G(z)): 0.0940 / 0.2237\n",
      "[0/10][929/938] Loss_D: 0.5373 Loss_G: 2.5947 D(x): 0.8679 D(G(z)): 0.2853 / 0.0984\n",
      "[0/10][930/938] Loss_D: 0.6063 Loss_G: 3.7055 D(x): 0.8423 D(G(z)): 0.3041 / 0.0311\n",
      "[0/10][931/938] Loss_D: 0.6074 Loss_G: 1.4686 D(x): 0.6325 D(G(z)): 0.0893 / 0.2713\n",
      "[0/10][932/938] Loss_D: 0.9954 Loss_G: 4.7179 D(x): 0.9063 D(G(z)): 0.5306 / 0.0135\n",
      "[0/10][933/938] Loss_D: 1.4043 Loss_G: 0.8017 D(x): 0.3560 D(G(z)): 0.0567 / 0.5160\n",
      "[0/10][934/938] Loss_D: 0.8844 Loss_G: 4.1233 D(x): 0.9193 D(G(z)): 0.4862 / 0.0284\n",
      "[0/10][935/938] Loss_D: 0.8459 Loss_G: 1.8676 D(x): 0.5784 D(G(z)): 0.1360 / 0.2164\n",
      "[0/10][936/938] Loss_D: 0.6448 Loss_G: 2.9455 D(x): 0.8573 D(G(z)): 0.3432 / 0.0694\n",
      "[0/10][937/938] Loss_D: 1.0896 Loss_G: 1.4458 D(x): 0.5137 D(G(z)): 0.1878 / 0.2972\n"
     ]
    }
   ],
   "source": [
    "# This is the engine of the code base - explicitly taking the objects created above (The generator, discrimator and the dataset) and connecting them together to learn.\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        \n",
    "        # Set the descrimator to forget any gradients.\n",
    "        netD.zero_grad()\n",
    "        # Get a sample of real handwritten digits and label them as 1 - all real\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=real_cpu.dtype, device=device)\n",
    "        # Pass the sample through the discrimator\n",
    "        output = netD(real_cpu)\n",
    "        # measure the error\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate the gradients of each layer of the network\n",
    "        errD_real.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        # pass the noise through the generator layers\n",
    "        fake = netG(noise)\n",
    "        # set the labels to all 0 - fake\n",
    "        label.fill_(fake_label)\n",
    "        # ask the discrimator to judge the fake images\n",
    "        output = netD(fake.detach())\n",
    "        # measure the error\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients \n",
    "        errD_fake.backward()\n",
    "        # Get the average output across the batch again\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Get the error\n",
    "        errD = errD_real + errD_fake\n",
    "        # Run the optimizer to update the weights\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        # Set the gradients of the generator to zero\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # get the judgements from the discrimator of the generator output is fake\n",
    "        output = netD(fake)\n",
    "        # calculate the error\n",
    "        errG = criterion(output, label)\n",
    "        # update the gradients\n",
    "        errG.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # update the weights\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, 1, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        # every 100 steps save a real sample and a fake sample for comparison\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,'real_samples.png',normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),'fake_samples_epoch_%03d.png' % epoch, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
