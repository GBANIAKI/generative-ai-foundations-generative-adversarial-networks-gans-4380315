{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/generative-ai-foundations-generative-adversarial-networks-gans-4380315/.venv/bin/python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.8.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /workspaces/generative-ai-foundations-generative-adversarial-networks-gans-4380315/.venv/lib/python3.12/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, setuptools, sympy, triton, typing-extensions\n",
      "Required-by: torchaudio, torchvision\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "# For this example we will use pytorch to manage the construction of the neural networks and the training\n",
    "# torchvision is a module that is part of pytorch that supports vision datasets and it will be where we will source the mnist - handwritten digits - data\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "!which python\n",
    "!pip show torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  3139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7970a439f530>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a seed will determine which data elements are selected. To replicate results keep the same seed.\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a check if there is a gpu available for training. At the moment we are assuming that it is not available.\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the GPU is not available means we will set the device to cpu and set up some parameters\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\")\n",
    "ngpu = 0\n",
    "#This is the width of the latent space matrix\n",
    "nz = 100\n",
    "# This is the generator matrix shape\n",
    "ngf = 64\n",
    "# This is the descrimator matrix shape\n",
    "ndf = 64\n",
    "# This is the number of color channels - other datasets may have 3 if they are color\n",
    "nc = 1\n",
    "# The number of sample to process per pass\n",
    "batch_size = 64\n",
    "# the number of CPU workers to work on the dataset\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.MNIST(root='data', download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5,), (0.5,)),\n",
    "                      ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "# The weights will need to be initialised based on the layer type to some value before training. These could be imported from past training steps.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Generator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Discrimator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function from pytorches established modules\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Set up the initial noise of the latent space to sample from.\n",
    "# Set the label of a real and fake sample to 0,1\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Create the optimiser which will dynamically change the parameters of the learning function over time to imporve the training process\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0005, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/938] Loss_D: 1.7669 Loss_G: 1.6955 D(x): 0.3097 D(G(z)): 0.2745 / 0.2033\n",
      "[0/1][1/938] Loss_D: 9.4350 Loss_G: 9.4008 D(x): 1.0000 D(G(z)): 0.9998 / 0.0002\n",
      "[0/1][2/938] Loss_D: 3.1866 Loss_G: 16.4184 D(x): 1.0000 D(G(z)): 0.9192 / 0.0000\n",
      "[0/1][3/938] Loss_D: 0.4638 Loss_G: 12.5407 D(x): 0.9115 D(G(z)): 0.1151 / 0.0000\n",
      "[0/1][4/938] Loss_D: 2.0822 Loss_G: 20.1405 D(x): 0.6662 D(G(z)): 0.6464 / 0.0000\n",
      "[0/1][5/938] Loss_D: 0.0659 Loss_G: 12.6683 D(x): 0.9624 D(G(z)): 0.0062 / 0.0000\n",
      "[0/1][6/938] Loss_D: 3.7557 Loss_G: 27.4345 D(x): 0.9978 D(G(z)): 0.9536 / 0.0000\n",
      "[0/1][7/938] Loss_D: 0.0797 Loss_G: 30.8790 D(x): 0.9414 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][8/938] Loss_D: 0.4121 Loss_G: 29.2573 D(x): 0.8167 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][9/938] Loss_D: 0.0259 Loss_G: 19.9763 D(x): 0.9806 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][10/938] Loss_D: 0.1931 Loss_G: 15.3894 D(x): 0.9973 D(G(z)): 0.1364 / 0.0000\n",
      "[0/1][11/938] Loss_D: 4.5625 Loss_G: 34.5962 D(x): 0.9973 D(G(z)): 0.9794 / 0.0000\n",
      "[0/1][12/938] Loss_D: 2.1600 Loss_G: 37.5844 D(x): 0.3393 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][13/938] Loss_D: 0.0046 Loss_G: 38.3170 D(x): 0.9956 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][14/938] Loss_D: 0.0049 Loss_G: 38.3813 D(x): 0.9954 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][15/938] Loss_D: 0.0037 Loss_G: 38.5001 D(x): 0.9965 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][16/938] Loss_D: 0.0072 Loss_G: 38.4019 D(x): 0.9942 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][17/938] Loss_D: 0.0016 Loss_G: 38.4392 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][18/938] Loss_D: 0.0001 Loss_G: 38.3847 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][19/938] Loss_D: 0.0001 Loss_G: 38.5042 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][20/938] Loss_D: 0.0002 Loss_G: 38.5675 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][21/938] Loss_D: 0.0005 Loss_G: 38.3909 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][22/938] Loss_D: 0.0005 Loss_G: 38.3873 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][23/938] Loss_D: 0.0001 Loss_G: 38.3283 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][24/938] Loss_D: 0.0000 Loss_G: 38.3920 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][25/938] Loss_D: 0.0001 Loss_G: 38.3769 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][26/938] Loss_D: 0.0000 Loss_G: 38.4087 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][27/938] Loss_D: 0.0001 Loss_G: 38.3564 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][28/938] Loss_D: 0.0003 Loss_G: 38.3597 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][29/938] Loss_D: 0.0020 Loss_G: 38.4971 D(x): 0.9981 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][30/938] Loss_D: 0.0001 Loss_G: 38.2577 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][31/938] Loss_D: 0.0000 Loss_G: 38.5260 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][32/938] Loss_D: 0.0009 Loss_G: 38.3599 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][33/938] Loss_D: 0.0001 Loss_G: 38.4680 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][34/938] Loss_D: 0.0002 Loss_G: 38.3325 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][35/938] Loss_D: 0.0003 Loss_G: 38.4689 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][36/938] Loss_D: 0.0001 Loss_G: 38.2340 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][37/938] Loss_D: 0.0000 Loss_G: 38.3738 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][38/938] Loss_D: 0.0002 Loss_G: 38.3604 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][39/938] Loss_D: 0.0001 Loss_G: 38.3334 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][40/938] Loss_D: 0.0001 Loss_G: 38.1229 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][41/938] Loss_D: 0.0019 Loss_G: 38.1637 D(x): 0.9982 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][42/938] Loss_D: 0.0001 Loss_G: 38.4008 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][43/938] Loss_D: 0.0160 Loss_G: 38.2063 D(x): 0.9900 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][44/938] Loss_D: 0.0007 Loss_G: 38.1098 D(x): 0.9993 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][45/938] Loss_D: 0.0000 Loss_G: 38.0547 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][46/938] Loss_D: 0.0000 Loss_G: 38.1682 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][47/938] Loss_D: 0.0000 Loss_G: 38.0443 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][48/938] Loss_D: 0.0000 Loss_G: 38.0663 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][49/938] Loss_D: 0.0001 Loss_G: 38.0669 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][50/938] Loss_D: 0.0000 Loss_G: 37.9288 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][51/938] Loss_D: 0.0000 Loss_G: 38.0505 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][52/938] Loss_D: 0.0052 Loss_G: 38.1274 D(x): 0.9955 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][53/938] Loss_D: 0.0004 Loss_G: 37.9450 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][54/938] Loss_D: 0.0000 Loss_G: 37.9211 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][55/938] Loss_D: 0.0000 Loss_G: 37.8346 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][56/938] Loss_D: 0.0000 Loss_G: 37.8268 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][57/938] Loss_D: 0.0001 Loss_G: 37.9192 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][58/938] Loss_D: 0.0001 Loss_G: 37.7661 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][59/938] Loss_D: 0.0001 Loss_G: 37.7352 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][60/938] Loss_D: 0.0000 Loss_G: 37.8064 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][61/938] Loss_D: 0.0001 Loss_G: 37.8440 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][62/938] Loss_D: 0.0016 Loss_G: 37.7169 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][63/938] Loss_D: 0.0013 Loss_G: 37.7526 D(x): 0.9987 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][64/938] Loss_D: 0.0000 Loss_G: 37.6253 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][65/938] Loss_D: 0.0000 Loss_G: 37.6116 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][66/938] Loss_D: 0.0000 Loss_G: 37.5426 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][67/938] Loss_D: 0.0000 Loss_G: 37.5556 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][68/938] Loss_D: 0.0002 Loss_G: 37.5661 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][69/938] Loss_D: 0.0000 Loss_G: 37.4439 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][70/938] Loss_D: 0.0000 Loss_G: 37.4272 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][71/938] Loss_D: 0.0000 Loss_G: 37.3171 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][72/938] Loss_D: 0.0006 Loss_G: 37.2893 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][73/938] Loss_D: 0.0000 Loss_G: 37.2912 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][74/938] Loss_D: 0.0000 Loss_G: 37.3182 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][75/938] Loss_D: 0.0000 Loss_G: 37.1873 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][76/938] Loss_D: 0.0001 Loss_G: 37.1626 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][77/938] Loss_D: 0.0000 Loss_G: 37.0122 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][78/938] Loss_D: 0.0000 Loss_G: 36.9016 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][79/938] Loss_D: 0.0000 Loss_G: 36.7983 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][80/938] Loss_D: 0.0000 Loss_G: 36.3530 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][81/938] Loss_D: 0.0000 Loss_G: 36.3553 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][82/938] Loss_D: 0.0000 Loss_G: 36.0937 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][83/938] Loss_D: 0.0000 Loss_G: 35.9246 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][84/938] Loss_D: 0.0000 Loss_G: 35.3969 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][85/938] Loss_D: 0.0000 Loss_G: 34.8479 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][86/938] Loss_D: 0.0000 Loss_G: 34.2956 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][87/938] Loss_D: 0.0003 Loss_G: 32.6225 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][88/938] Loss_D: 0.0001 Loss_G: 22.4950 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][89/938] Loss_D: 65.5133 Loss_G: 27.3710 D(x): 1.0000 D(G(z)): 1.0000 / 0.0000\n",
      "[0/1][90/938] Loss_D: 1.5113 Loss_G: 18.4087 D(x): 0.4677 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][91/938] Loss_D: 0.0179 Loss_G: 2.7771 D(x): 0.9999 D(G(z)): 0.0175 / 0.0912\n",
      "[0/1][92/938] Loss_D: 49.6778 Loss_G: 27.2430 D(x): 1.0000 D(G(z)): 1.0000 / 0.0000\n",
      "[0/1][93/938] Loss_D: 1.0271 Loss_G: 23.8099 D(x): 0.6348 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][94/938] Loss_D: 0.0013 Loss_G: 16.8078 D(x): 0.9987 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][95/938] Loss_D: 0.2421 Loss_G: 13.6252 D(x): 0.9751 D(G(z)): 0.1389 / 0.0000\n",
      "[0/1][96/938] Loss_D: 18.1287 Loss_G: 35.4969 D(x): 0.9816 D(G(z)): 0.9999 / 0.0000\n",
      "[0/1][97/938] Loss_D: 15.6889 Loss_G: 26.6650 D(x): 0.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][98/938] Loss_D: 0.8219 Loss_G: 15.0557 D(x): 0.7700 D(G(z)): 0.0000 / 0.0000\n",
      "[0/1][99/938] Loss_D: 0.8693 Loss_G: 9.4334 D(x): 0.9435 D(G(z)): 0.2452 / 0.0012\n",
      "[0/1][100/938] Loss_D: 4.6249 Loss_G: 15.7645 D(x): 0.9228 D(G(z)): 0.7492 / 0.0000\n",
      "[0/1][101/938] Loss_D: 3.2154 Loss_G: 7.1481 D(x): 0.3600 D(G(z)): 0.0018 / 0.0027\n",
      "[0/1][102/938] Loss_D: 1.6548 Loss_G: 8.4129 D(x): 0.7664 D(G(z)): 0.3484 / 0.0008\n",
      "[0/1][103/938] Loss_D: 0.7506 Loss_G: 6.7764 D(x): 0.7632 D(G(z)): 0.1298 / 0.0029\n",
      "[0/1][104/938] Loss_D: 1.6271 Loss_G: 14.0214 D(x): 0.9145 D(G(z)): 0.5947 / 0.0001\n",
      "[0/1][105/938] Loss_D: 4.7820 Loss_G: 5.0679 D(x): 0.1063 D(G(z)): 0.0013 / 0.0279\n",
      "[0/1][106/938] Loss_D: 3.3565 Loss_G: 13.1660 D(x): 0.9006 D(G(z)): 0.8433 / 0.0002\n",
      "[0/1][107/938] Loss_D: 1.2526 Loss_G: 11.0266 D(x): 0.6347 D(G(z)): 0.0024 / 0.0010\n",
      "[0/1][108/938] Loss_D: 0.9313 Loss_G: 3.5780 D(x): 0.6838 D(G(z)): 0.0482 / 0.0967\n",
      "[0/1][109/938] Loss_D: 2.7625 Loss_G: 6.8312 D(x): 0.9673 D(G(z)): 0.8053 / 0.0117\n",
      "[0/1][110/938] Loss_D: 0.6055 Loss_G: 6.7416 D(x): 0.7024 D(G(z)): 0.0420 / 0.0067\n",
      "[0/1][111/938] Loss_D: 0.5993 Loss_G: 2.7981 D(x): 0.7250 D(G(z)): 0.0855 / 0.0900\n",
      "[0/1][112/938] Loss_D: 1.0561 Loss_G: 7.0784 D(x): 0.9284 D(G(z)): 0.5487 / 0.0052\n",
      "[0/1][113/938] Loss_D: 0.7283 Loss_G: 4.3818 D(x): 0.6208 D(G(z)): 0.0215 / 0.0245\n",
      "[0/1][114/938] Loss_D: 0.2556 Loss_G: 2.8202 D(x): 0.9108 D(G(z)): 0.1279 / 0.0793\n",
      "[0/1][115/938] Loss_D: 0.7459 Loss_G: 6.9767 D(x): 0.9334 D(G(z)): 0.4505 / 0.0029\n",
      "[0/1][116/938] Loss_D: 0.7225 Loss_G: 4.4403 D(x): 0.5964 D(G(z)): 0.0168 / 0.0328\n",
      "[0/1][117/938] Loss_D: 0.3128 Loss_G: 3.4251 D(x): 0.9129 D(G(z)): 0.1512 / 0.0485\n",
      "[0/1][118/938] Loss_D: 0.6185 Loss_G: 7.3098 D(x): 0.9512 D(G(z)): 0.3902 / 0.0027\n",
      "[0/1][119/938] Loss_D: 0.7009 Loss_G: 4.8749 D(x): 0.5994 D(G(z)): 0.0134 / 0.0172\n",
      "[0/1][120/938] Loss_D: 0.1773 Loss_G: 3.3556 D(x): 0.9674 D(G(z)): 0.1271 / 0.0481\n",
      "[0/1][121/938] Loss_D: 0.5138 Loss_G: 5.5127 D(x): 0.9637 D(G(z)): 0.3322 / 0.0086\n",
      "[0/1][122/938] Loss_D: 0.3790 Loss_G: 4.5976 D(x): 0.8084 D(G(z)): 0.0753 / 0.0232\n",
      "[0/1][123/938] Loss_D: 0.4448 Loss_G: 4.6709 D(x): 0.8971 D(G(z)): 0.2503 / 0.0187\n",
      "[0/1][124/938] Loss_D: 0.9101 Loss_G: 2.5885 D(x): 0.6130 D(G(z)): 0.2008 / 0.1066\n",
      "[0/1][125/938] Loss_D: 1.6302 Loss_G: 11.7987 D(x): 0.9010 D(G(z)): 0.6999 / 0.0001\n",
      "[0/1][126/938] Loss_D: 4.0914 Loss_G: 5.3426 D(x): 0.0564 D(G(z)): 0.0007 / 0.0147\n",
      "[0/1][127/938] Loss_D: 0.2760 Loss_G: 1.7179 D(x): 0.9232 D(G(z)): 0.1399 / 0.2614\n",
      "[0/1][128/938] Loss_D: 1.0613 Loss_G: 5.8177 D(x): 0.9927 D(G(z)): 0.5814 / 0.0079\n",
      "[0/1][129/938] Loss_D: 0.2342 Loss_G: 5.7622 D(x): 0.8527 D(G(z)): 0.0498 / 0.0082\n",
      "[0/1][130/938] Loss_D: 0.5358 Loss_G: 2.8232 D(x): 0.7388 D(G(z)): 0.0894 / 0.0904\n",
      "[0/1][131/938] Loss_D: 0.8940 Loss_G: 4.9387 D(x): 0.8826 D(G(z)): 0.4564 / 0.0121\n",
      "[0/1][132/938] Loss_D: 0.8131 Loss_G: 3.3548 D(x): 0.6288 D(G(z)): 0.0690 / 0.0529\n",
      "[0/1][133/938] Loss_D: 0.7337 Loss_G: 4.5671 D(x): 0.8531 D(G(z)): 0.3692 / 0.0150\n",
      "[0/1][134/938] Loss_D: 0.7704 Loss_G: 5.0837 D(x): 0.7627 D(G(z)): 0.3120 / 0.0102\n",
      "[0/1][135/938] Loss_D: 0.9502 Loss_G: 4.2832 D(x): 0.6082 D(G(z)): 0.2363 / 0.0226\n",
      "[0/1][136/938] Loss_D: 0.4749 Loss_G: 5.6826 D(x): 0.8766 D(G(z)): 0.2460 / 0.0061\n",
      "[0/1][137/938] Loss_D: 0.3560 Loss_G: 3.5810 D(x): 0.7815 D(G(z)): 0.0450 / 0.0391\n",
      "[0/1][138/938] Loss_D: 0.8475 Loss_G: 9.0716 D(x): 0.9200 D(G(z)): 0.4812 / 0.0003\n",
      "[0/1][139/938] Loss_D: 1.5504 Loss_G: 2.0648 D(x): 0.3215 D(G(z)): 0.0051 / 0.1755\n",
      "[0/1][140/938] Loss_D: 1.4589 Loss_G: 7.2523 D(x): 0.9446 D(G(z)): 0.6788 / 0.0022\n",
      "[0/1][141/938] Loss_D: 0.6843 Loss_G: 5.4761 D(x): 0.6813 D(G(z)): 0.0183 / 0.0066\n",
      "[0/1][142/938] Loss_D: 0.6106 Loss_G: 2.9056 D(x): 0.7977 D(G(z)): 0.1324 / 0.0862\n",
      "[0/1][143/938] Loss_D: 0.9776 Loss_G: 6.9876 D(x): 0.9258 D(G(z)): 0.5047 / 0.0021\n",
      "[0/1][144/938] Loss_D: 1.2363 Loss_G: 2.8398 D(x): 0.4312 D(G(z)): 0.0107 / 0.0838\n",
      "[0/1][145/938] Loss_D: 0.6427 Loss_G: 5.0713 D(x): 0.9470 D(G(z)): 0.4133 / 0.0087\n",
      "[0/1][146/938] Loss_D: 0.3657 Loss_G: 4.4754 D(x): 0.8326 D(G(z)): 0.1025 / 0.0176\n",
      "[0/1][147/938] Loss_D: 0.7949 Loss_G: 4.1580 D(x): 0.7407 D(G(z)): 0.2988 / 0.0230\n",
      "[0/1][148/938] Loss_D: 0.8839 Loss_G: 4.6697 D(x): 0.7342 D(G(z)): 0.3102 / 0.0199\n",
      "[0/1][149/938] Loss_D: 0.7245 Loss_G: 3.5791 D(x): 0.7126 D(G(z)): 0.2117 / 0.0484\n",
      "[0/1][150/938] Loss_D: 0.8535 Loss_G: 4.4841 D(x): 0.7552 D(G(z)): 0.3230 / 0.0191\n",
      "[0/1][151/938] Loss_D: 0.4248 Loss_G: 3.9708 D(x): 0.7953 D(G(z)): 0.1246 / 0.0277\n",
      "[0/1][152/938] Loss_D: 0.4118 Loss_G: 4.6230 D(x): 0.8593 D(G(z)): 0.1813 / 0.0141\n",
      "[0/1][153/938] Loss_D: 0.3612 Loss_G: 4.6135 D(x): 0.8550 D(G(z)): 0.1583 / 0.0133\n",
      "[0/1][154/938] Loss_D: 0.4353 Loss_G: 3.5114 D(x): 0.8046 D(G(z)): 0.1525 / 0.0457\n",
      "[0/1][155/938] Loss_D: 0.6808 Loss_G: 6.9550 D(x): 0.8636 D(G(z)): 0.3505 / 0.0016\n",
      "[0/1][156/938] Loss_D: 1.5366 Loss_G: 0.5429 D(x): 0.3540 D(G(z)): 0.0132 / 0.6221\n",
      "[0/1][157/938] Loss_D: 1.7704 Loss_G: 8.0874 D(x): 0.9836 D(G(z)): 0.7698 / 0.0026\n",
      "[0/1][158/938] Loss_D: 1.3229 Loss_G: 4.6249 D(x): 0.3946 D(G(z)): 0.0204 / 0.0905\n",
      "[0/1][159/938] Loss_D: 0.5926 Loss_G: 3.5913 D(x): 0.8884 D(G(z)): 0.2777 / 0.0492\n",
      "[0/1][160/938] Loss_D: 0.8700 Loss_G: 6.4061 D(x): 0.8920 D(G(z)): 0.4152 / 0.0036\n",
      "[0/1][161/938] Loss_D: 1.4585 Loss_G: 1.4140 D(x): 0.4315 D(G(z)): 0.1027 / 0.3297\n",
      "[0/1][162/938] Loss_D: 1.2121 Loss_G: 7.5587 D(x): 0.9468 D(G(z)): 0.5885 / 0.0008\n",
      "[0/1][163/938] Loss_D: 1.3638 Loss_G: 4.4200 D(x): 0.4099 D(G(z)): 0.0032 / 0.0174\n",
      "[0/1][164/938] Loss_D: 0.3168 Loss_G: 2.5532 D(x): 0.9049 D(G(z)): 0.1475 / 0.1037\n",
      "[0/1][165/938] Loss_D: 1.1116 Loss_G: 8.8464 D(x): 0.9795 D(G(z)): 0.6030 / 0.0027\n",
      "[0/1][166/938] Loss_D: 1.0672 Loss_G: 5.4657 D(x): 0.5203 D(G(z)): 0.0289 / 0.0319\n",
      "[0/1][167/938] Loss_D: 0.5143 Loss_G: 3.3580 D(x): 0.8653 D(G(z)): 0.2467 / 0.0649\n",
      "[0/1][168/938] Loss_D: 0.5491 Loss_G: 5.5837 D(x): 0.8929 D(G(z)): 0.3108 / 0.0072\n",
      "[0/1][169/938] Loss_D: 0.7487 Loss_G: 2.4584 D(x): 0.6278 D(G(z)): 0.0505 / 0.1360\n",
      "[0/1][170/938] Loss_D: 1.3526 Loss_G: 10.0839 D(x): 0.9402 D(G(z)): 0.6405 / 0.0001\n",
      "[0/1][171/938] Loss_D: 2.2120 Loss_G: 4.3011 D(x): 0.2764 D(G(z)): 0.0014 / 0.0299\n",
      "[0/1][172/938] Loss_D: 0.4581 Loss_G: 1.8863 D(x): 0.8393 D(G(z)): 0.1973 / 0.2064\n",
      "[0/1][173/938] Loss_D: 1.0133 Loss_G: 6.5041 D(x): 0.9636 D(G(z)): 0.5733 / 0.0039\n",
      "[0/1][174/938] Loss_D: 1.1362 Loss_G: 3.2738 D(x): 0.4395 D(G(z)): 0.0146 / 0.0731\n",
      "[0/1][175/938] Loss_D: 0.6292 Loss_G: 4.2709 D(x): 0.8976 D(G(z)): 0.3440 / 0.0267\n",
      "[0/1][176/938] Loss_D: 0.5831 Loss_G: 5.6917 D(x): 0.8679 D(G(z)): 0.2831 / 0.0060\n",
      "[0/1][177/938] Loss_D: 0.6201 Loss_G: 2.8727 D(x): 0.6545 D(G(z)): 0.0486 / 0.0962\n",
      "[0/1][178/938] Loss_D: 0.7224 Loss_G: 5.5576 D(x): 0.9016 D(G(z)): 0.3891 / 0.0085\n",
      "[0/1][179/938] Loss_D: 0.5347 Loss_G: 3.9952 D(x): 0.7007 D(G(z)): 0.0353 / 0.0505\n",
      "[0/1][180/938] Loss_D: 0.3532 Loss_G: 4.0910 D(x): 0.9359 D(G(z)): 0.2054 / 0.0295\n",
      "[0/1][181/938] Loss_D: 0.5343 Loss_G: 4.5695 D(x): 0.8648 D(G(z)): 0.2472 / 0.0155\n",
      "[0/1][182/938] Loss_D: 0.5068 Loss_G: 2.3587 D(x): 0.7311 D(G(z)): 0.1219 / 0.1329\n",
      "[0/1][183/938] Loss_D: 0.7884 Loss_G: 6.5003 D(x): 0.9005 D(G(z)): 0.4125 / 0.0030\n",
      "[0/1][184/938] Loss_D: 1.3401 Loss_G: 0.8930 D(x): 0.4097 D(G(z)): 0.0237 / 0.4801\n",
      "[0/1][185/938] Loss_D: 1.4163 Loss_G: 7.3575 D(x): 0.9496 D(G(z)): 0.6518 / 0.0011\n",
      "[0/1][186/938] Loss_D: 1.6051 Loss_G: 3.5113 D(x): 0.3324 D(G(z)): 0.0145 / 0.0774\n",
      "[0/1][187/938] Loss_D: 0.5945 Loss_G: 4.3304 D(x): 0.9687 D(G(z)): 0.3471 / 0.0383\n",
      "[0/1][188/938] Loss_D: 0.3611 Loss_G: 4.2954 D(x): 0.8452 D(G(z)): 0.1284 / 0.0354\n",
      "[0/1][189/938] Loss_D: 0.5057 Loss_G: 3.3530 D(x): 0.8004 D(G(z)): 0.1714 / 0.0659\n",
      "[0/1][190/938] Loss_D: 0.7397 Loss_G: 4.3991 D(x): 0.8329 D(G(z)): 0.3281 / 0.0219\n",
      "[0/1][191/938] Loss_D: 0.7539 Loss_G: 2.9618 D(x): 0.6686 D(G(z)): 0.1778 / 0.0899\n",
      "[0/1][192/938] Loss_D: 0.6611 Loss_G: 5.2472 D(x): 0.8788 D(G(z)): 0.3423 / 0.0076\n",
      "[0/1][193/938] Loss_D: 0.5472 Loss_G: 3.2278 D(x): 0.6736 D(G(z)): 0.0415 / 0.0659\n",
      "[0/1][194/938] Loss_D: 0.3747 Loss_G: 3.8028 D(x): 0.9378 D(G(z)): 0.2172 / 0.0344\n",
      "[0/1][195/938] Loss_D: 0.3530 Loss_G: 2.7715 D(x): 0.8497 D(G(z)): 0.1287 / 0.1021\n",
      "[0/1][196/938] Loss_D: 0.4890 Loss_G: 4.8402 D(x): 0.8914 D(G(z)): 0.2686 / 0.0133\n",
      "[0/1][197/938] Loss_D: 0.6476 Loss_G: 1.3401 D(x): 0.6626 D(G(z)): 0.0902 / 0.3827\n",
      "[0/1][198/938] Loss_D: 1.3574 Loss_G: 15.8034 D(x): 0.9401 D(G(z)): 0.6117 / 0.0000\n",
      "[0/1][199/938] Loss_D: 11.4811 Loss_G: 6.2070 D(x): 0.0001 D(G(z)): 0.0001 / 0.0068\n",
      "[0/1][200/938] Loss_D: 2.3311 Loss_G: 0.6899 D(x): 0.1662 D(G(z)): 0.0087 / 0.5960\n",
      "[0/1][201/938] Loss_D: 2.2238 Loss_G: 1.7930 D(x): 0.9789 D(G(z)): 0.7438 / 0.2308\n",
      "[0/1][202/938] Loss_D: 0.9948 Loss_G: 3.8902 D(x): 0.8196 D(G(z)): 0.4698 / 0.0429\n",
      "[0/1][203/938] Loss_D: 1.3176 Loss_G: 1.3821 D(x): 0.4155 D(G(z)): 0.1340 / 0.3300\n",
      "[0/1][204/938] Loss_D: 1.3793 Loss_G: 2.1520 D(x): 0.7775 D(G(z)): 0.6021 / 0.1665\n",
      "[0/1][205/938] Loss_D: 1.1835 Loss_G: 1.8779 D(x): 0.5912 D(G(z)): 0.3785 / 0.1988\n",
      "[0/1][206/938] Loss_D: 1.1833 Loss_G: 1.6574 D(x): 0.5912 D(G(z)): 0.3824 / 0.2516\n",
      "[0/1][207/938] Loss_D: 1.0665 Loss_G: 1.1544 D(x): 0.5784 D(G(z)): 0.3277 / 0.3713\n",
      "[0/1][208/938] Loss_D: 1.2275 Loss_G: 3.2521 D(x): 0.7704 D(G(z)): 0.5444 / 0.0534\n",
      "[0/1][209/938] Loss_D: 1.8410 Loss_G: 0.5571 D(x): 0.2507 D(G(z)): 0.0819 / 0.6457\n",
      "[0/1][210/938] Loss_D: 2.1116 Loss_G: 2.8085 D(x): 0.9430 D(G(z)): 0.7717 / 0.0931\n",
      "[0/1][211/938] Loss_D: 1.2707 Loss_G: 1.9579 D(x): 0.4051 D(G(z)): 0.1599 / 0.1887\n",
      "[0/1][212/938] Loss_D: 1.0169 Loss_G: 1.2169 D(x): 0.6047 D(G(z)): 0.3000 / 0.3616\n",
      "[0/1][213/938] Loss_D: 0.8296 Loss_G: 2.6398 D(x): 0.8375 D(G(z)): 0.4090 / 0.1012\n",
      "[0/1][214/938] Loss_D: 0.8427 Loss_G: 1.9387 D(x): 0.6488 D(G(z)): 0.2566 / 0.1813\n",
      "[0/1][215/938] Loss_D: 0.9030 Loss_G: 1.2902 D(x): 0.6103 D(G(z)): 0.2625 / 0.3164\n",
      "[0/1][216/938] Loss_D: 0.8425 Loss_G: 2.5477 D(x): 0.7789 D(G(z)): 0.3908 / 0.0950\n",
      "[0/1][217/938] Loss_D: 0.7479 Loss_G: 1.9577 D(x): 0.6249 D(G(z)): 0.1823 / 0.1743\n",
      "[0/1][218/938] Loss_D: 0.7788 Loss_G: 2.2775 D(x): 0.7460 D(G(z)): 0.3264 / 0.1165\n",
      "[0/1][219/938] Loss_D: 0.6839 Loss_G: 2.7010 D(x): 0.7561 D(G(z)): 0.2889 / 0.0885\n",
      "[0/1][220/938] Loss_D: 0.6320 Loss_G: 2.6220 D(x): 0.7520 D(G(z)): 0.2422 / 0.0937\n",
      "[0/1][221/938] Loss_D: 0.6858 Loss_G: 2.2597 D(x): 0.6970 D(G(z)): 0.2039 / 0.1371\n",
      "[0/1][222/938] Loss_D: 1.0412 Loss_G: 4.8783 D(x): 0.8430 D(G(z)): 0.5131 / 0.0192\n",
      "[0/1][223/938] Loss_D: 1.1046 Loss_G: 1.7027 D(x): 0.4383 D(G(z)): 0.0923 / 0.2686\n",
      "[0/1][224/938] Loss_D: 1.5838 Loss_G: 4.1147 D(x): 0.8364 D(G(z)): 0.6895 / 0.0319\n",
      "[0/1][225/938] Loss_D: 1.1372 Loss_G: 2.3091 D(x): 0.4591 D(G(z)): 0.1084 / 0.1864\n",
      "[0/1][226/938] Loss_D: 0.9576 Loss_G: 3.5514 D(x): 0.8428 D(G(z)): 0.3976 / 0.0466\n",
      "[0/1][227/938] Loss_D: 0.5912 Loss_G: 3.4948 D(x): 0.7918 D(G(z)): 0.2647 / 0.0499\n",
      "[0/1][228/938] Loss_D: 1.0447 Loss_G: 2.1038 D(x): 0.5672 D(G(z)): 0.2514 / 0.1674\n",
      "[0/1][229/938] Loss_D: 0.8698 Loss_G: 3.2991 D(x): 0.7612 D(G(z)): 0.3811 / 0.0472\n",
      "[0/1][230/938] Loss_D: 0.6713 Loss_G: 2.2487 D(x): 0.6638 D(G(z)): 0.1503 / 0.1420\n",
      "[0/1][231/938] Loss_D: 0.8531 Loss_G: 2.4566 D(x): 0.7360 D(G(z)): 0.2827 / 0.1148\n",
      "[0/1][232/938] Loss_D: 0.7373 Loss_G: 2.6280 D(x): 0.7351 D(G(z)): 0.2689 / 0.0961\n",
      "[0/1][233/938] Loss_D: 0.7014 Loss_G: 2.4630 D(x): 0.7375 D(G(z)): 0.2510 / 0.1196\n",
      "[0/1][234/938] Loss_D: 0.7951 Loss_G: 3.3530 D(x): 0.7870 D(G(z)): 0.3431 / 0.0552\n",
      "[0/1][235/938] Loss_D: 0.6747 Loss_G: 1.4080 D(x): 0.6215 D(G(z)): 0.1133 / 0.2947\n",
      "[0/1][236/938] Loss_D: 0.8595 Loss_G: 4.2670 D(x): 0.8812 D(G(z)): 0.4606 / 0.0198\n",
      "[0/1][237/938] Loss_D: 0.9009 Loss_G: 1.6894 D(x): 0.5020 D(G(z)): 0.0508 / 0.2637\n",
      "[0/1][238/938] Loss_D: 0.9437 Loss_G: 3.4267 D(x): 0.8861 D(G(z)): 0.4670 / 0.0511\n",
      "[0/1][239/938] Loss_D: 0.5733 Loss_G: 2.6058 D(x): 0.7073 D(G(z)): 0.1494 / 0.0971\n",
      "[0/1][240/938] Loss_D: 0.5933 Loss_G: 1.5108 D(x): 0.7000 D(G(z)): 0.1523 / 0.2721\n",
      "[0/1][241/938] Loss_D: 0.7901 Loss_G: 4.1980 D(x): 0.9124 D(G(z)): 0.4617 / 0.0211\n",
      "[0/1][242/938] Loss_D: 1.0216 Loss_G: 1.4706 D(x): 0.4482 D(G(z)): 0.0334 / 0.3003\n",
      "[0/1][243/938] Loss_D: 0.7013 Loss_G: 3.1630 D(x): 0.9342 D(G(z)): 0.4210 / 0.0596\n",
      "[0/1][244/938] Loss_D: 0.7921 Loss_G: 1.6483 D(x): 0.6284 D(G(z)): 0.1893 / 0.2633\n",
      "[0/1][245/938] Loss_D: 0.6137 Loss_G: 2.5494 D(x): 0.8410 D(G(z)): 0.2959 / 0.0991\n",
      "[0/1][246/938] Loss_D: 0.4569 Loss_G: 2.3709 D(x): 0.7734 D(G(z)): 0.1472 / 0.1208\n",
      "[0/1][247/938] Loss_D: 0.4644 Loss_G: 2.4076 D(x): 0.8292 D(G(z)): 0.2062 / 0.1140\n",
      "[0/1][248/938] Loss_D: 0.4674 Loss_G: 2.7011 D(x): 0.8336 D(G(z)): 0.2055 / 0.0884\n",
      "[0/1][249/938] Loss_D: 0.4696 Loss_G: 2.3642 D(x): 0.7881 D(G(z)): 0.1760 / 0.1216\n",
      "[0/1][250/938] Loss_D: 0.6060 Loss_G: 1.5497 D(x): 0.7347 D(G(z)): 0.2039 / 0.2545\n",
      "[0/1][251/938] Loss_D: 0.6247 Loss_G: 4.3313 D(x): 0.9056 D(G(z)): 0.3530 / 0.0299\n",
      "[0/1][252/938] Loss_D: 1.0071 Loss_G: 0.9005 D(x): 0.4656 D(G(z)): 0.0632 / 0.4777\n",
      "[0/1][253/938] Loss_D: 1.1772 Loss_G: 5.1683 D(x): 0.9642 D(G(z)): 0.6162 / 0.0120\n",
      "[0/1][254/938] Loss_D: 1.1732 Loss_G: 1.7243 D(x): 0.4016 D(G(z)): 0.0340 / 0.2657\n",
      "[0/1][255/938] Loss_D: 0.9642 Loss_G: 3.4609 D(x): 0.9181 D(G(z)): 0.4372 / 0.0588\n",
      "[0/1][256/938] Loss_D: 0.6692 Loss_G: 1.7426 D(x): 0.6722 D(G(z)): 0.1458 / 0.2539\n",
      "[0/1][257/938] Loss_D: 0.5910 Loss_G: 2.8871 D(x): 0.8761 D(G(z)): 0.3077 / 0.0797\n",
      "[0/1][258/938] Loss_D: 0.5335 Loss_G: 2.5570 D(x): 0.7532 D(G(z)): 0.1741 / 0.1141\n",
      "[0/1][259/938] Loss_D: 0.3874 Loss_G: 2.5298 D(x): 0.8314 D(G(z)): 0.1670 / 0.1019\n",
      "[0/1][260/938] Loss_D: 0.6959 Loss_G: 3.0751 D(x): 0.7917 D(G(z)): 0.2950 / 0.0625\n",
      "[0/1][261/938] Loss_D: 0.5929 Loss_G: 1.4913 D(x): 0.6755 D(G(z)): 0.1040 / 0.2721\n",
      "[0/1][262/938] Loss_D: 0.5810 Loss_G: 4.1168 D(x): 0.9320 D(G(z)): 0.3563 / 0.0257\n",
      "[0/1][263/938] Loss_D: 0.6437 Loss_G: 1.5909 D(x): 0.6174 D(G(z)): 0.0652 / 0.2683\n",
      "[0/1][264/938] Loss_D: 0.9321 Loss_G: 4.9754 D(x): 0.9070 D(G(z)): 0.4851 / 0.0117\n",
      "[0/1][265/938] Loss_D: 1.5810 Loss_G: 0.9328 D(x): 0.3022 D(G(z)): 0.0350 / 0.4614\n",
      "[0/1][266/938] Loss_D: 1.2594 Loss_G: 7.3357 D(x): 0.9513 D(G(z)): 0.6126 / 0.0016\n",
      "[0/1][267/938] Loss_D: 4.4598 Loss_G: 0.6658 D(x): 0.0293 D(G(z)): 0.0034 / 0.6146\n",
      "[0/1][268/938] Loss_D: 1.7672 Loss_G: 3.5669 D(x): 0.9432 D(G(z)): 0.7066 / 0.0657\n",
      "[0/1][269/938] Loss_D: 0.8852 Loss_G: 3.0853 D(x): 0.6574 D(G(z)): 0.1509 / 0.1139\n",
      "[0/1][270/938] Loss_D: 1.0283 Loss_G: 2.1474 D(x): 0.7087 D(G(z)): 0.3423 / 0.1640\n",
      "[0/1][271/938] Loss_D: 0.9996 Loss_G: 2.7505 D(x): 0.7302 D(G(z)): 0.4117 / 0.1069\n",
      "[0/1][272/938] Loss_D: 1.1287 Loss_G: 1.0546 D(x): 0.4790 D(G(z)): 0.1702 / 0.4128\n",
      "[0/1][273/938] Loss_D: 1.1389 Loss_G: 3.8309 D(x): 0.8745 D(G(z)): 0.5443 / 0.0385\n",
      "[0/1][274/938] Loss_D: 1.2163 Loss_G: 1.8418 D(x): 0.4911 D(G(z)): 0.1551 / 0.2245\n",
      "[0/1][275/938] Loss_D: 0.9727 Loss_G: 2.2960 D(x): 0.7752 D(G(z)): 0.4063 / 0.1585\n",
      "[0/1][276/938] Loss_D: 0.8154 Loss_G: 3.1594 D(x): 0.7840 D(G(z)): 0.3754 / 0.0599\n",
      "[0/1][277/938] Loss_D: 1.1810 Loss_G: 1.2013 D(x): 0.4469 D(G(z)): 0.1852 / 0.3722\n",
      "[0/1][278/938] Loss_D: 1.0123 Loss_G: 3.0196 D(x): 0.8343 D(G(z)): 0.4912 / 0.0710\n",
      "[0/1][279/938] Loss_D: 0.6388 Loss_G: 2.2843 D(x): 0.6519 D(G(z)): 0.1243 / 0.1296\n",
      "[0/1][280/938] Loss_D: 0.4954 Loss_G: 1.9097 D(x): 0.7937 D(G(z)): 0.1967 / 0.1839\n",
      "[0/1][281/938] Loss_D: 0.6201 Loss_G: 3.1574 D(x): 0.8797 D(G(z)): 0.3419 / 0.0677\n",
      "[0/1][282/938] Loss_D: 0.6431 Loss_G: 2.1102 D(x): 0.6911 D(G(z)): 0.1793 / 0.1690\n",
      "[0/1][283/938] Loss_D: 0.6131 Loss_G: 2.2654 D(x): 0.7993 D(G(z)): 0.2726 / 0.1384\n",
      "[0/1][284/938] Loss_D: 0.6600 Loss_G: 2.3300 D(x): 0.7357 D(G(z)): 0.2481 / 0.1324\n",
      "[0/1][285/938] Loss_D: 0.6118 Loss_G: 1.8384 D(x): 0.7206 D(G(z)): 0.2133 / 0.1965\n",
      "[0/1][286/938] Loss_D: 0.6912 Loss_G: 3.1007 D(x): 0.8160 D(G(z)): 0.3262 / 0.0620\n",
      "[0/1][287/938] Loss_D: 0.7714 Loss_G: 1.1316 D(x): 0.5859 D(G(z)): 0.1340 / 0.3883\n",
      "[0/1][288/938] Loss_D: 1.0539 Loss_G: 4.6956 D(x): 0.9294 D(G(z)): 0.5451 / 0.0162\n",
      "[0/1][289/938] Loss_D: 1.1797 Loss_G: 1.5422 D(x): 0.4166 D(G(z)): 0.0380 / 0.2935\n",
      "[0/1][290/938] Loss_D: 0.7176 Loss_G: 2.2853 D(x): 0.8904 D(G(z)): 0.3761 / 0.1418\n",
      "[0/1][291/938] Loss_D: 0.4421 Loss_G: 3.1219 D(x): 0.8501 D(G(z)): 0.2158 / 0.0582\n",
      "[0/1][292/938] Loss_D: 0.6023 Loss_G: 1.7477 D(x): 0.6961 D(G(z)): 0.1518 / 0.2093\n",
      "[0/1][293/938] Loss_D: 0.5473 Loss_G: 3.0250 D(x): 0.8849 D(G(z)): 0.3120 / 0.0661\n",
      "[0/1][294/938] Loss_D: 0.5385 Loss_G: 1.9151 D(x): 0.7099 D(G(z)): 0.1325 / 0.1904\n",
      "[0/1][295/938] Loss_D: 0.7254 Loss_G: 2.6866 D(x): 0.8183 D(G(z)): 0.3539 / 0.0983\n",
      "[0/1][296/938] Loss_D: 0.5343 Loss_G: 1.8375 D(x): 0.7157 D(G(z)): 0.1370 / 0.1942\n",
      "[0/1][297/938] Loss_D: 0.5724 Loss_G: 3.0954 D(x): 0.8720 D(G(z)): 0.3078 / 0.0653\n",
      "[0/1][298/938] Loss_D: 0.4843 Loss_G: 2.3914 D(x): 0.7430 D(G(z)): 0.1363 / 0.1357\n",
      "[0/1][299/938] Loss_D: 0.3904 Loss_G: 2.2817 D(x): 0.8418 D(G(z)): 0.1731 / 0.1367\n",
      "[0/1][300/938] Loss_D: 0.5122 Loss_G: 2.8126 D(x): 0.8479 D(G(z)): 0.2540 / 0.0820\n",
      "[0/1][301/938] Loss_D: 0.4673 Loss_G: 2.1098 D(x): 0.7774 D(G(z)): 0.1574 / 0.1531\n",
      "[0/1][302/938] Loss_D: 0.5380 Loss_G: 2.6373 D(x): 0.8275 D(G(z)): 0.2521 / 0.0960\n",
      "[0/1][303/938] Loss_D: 0.4076 Loss_G: 2.2401 D(x): 0.7969 D(G(z)): 0.1392 / 0.1333\n",
      "[0/1][304/938] Loss_D: 0.4419 Loss_G: 3.8021 D(x): 0.9158 D(G(z)): 0.2692 / 0.0343\n",
      "[0/1][305/938] Loss_D: 0.5814 Loss_G: 1.3752 D(x): 0.6307 D(G(z)): 0.0541 / 0.3073\n",
      "[0/1][306/938] Loss_D: 0.6261 Loss_G: 3.4705 D(x): 0.8747 D(G(z)): 0.3619 / 0.0428\n",
      "[0/1][307/938] Loss_D: 1.0289 Loss_G: 0.2104 D(x): 0.4552 D(G(z)): 0.0816 / 0.8322\n",
      "[0/1][308/938] Loss_D: 2.3411 Loss_G: 6.6078 D(x): 0.9959 D(G(z)): 0.8329 / 0.0023\n",
      "[0/1][309/938] Loss_D: 3.5111 Loss_G: 1.1022 D(x): 0.0503 D(G(z)): 0.0064 / 0.4093\n",
      "[0/1][310/938] Loss_D: 0.8719 Loss_G: 2.1464 D(x): 0.8627 D(G(z)): 0.4407 / 0.1624\n",
      "[0/1][311/938] Loss_D: 0.6552 Loss_G: 2.8468 D(x): 0.8115 D(G(z)): 0.3044 / 0.0878\n",
      "[0/1][312/938] Loss_D: 1.0210 Loss_G: 1.0259 D(x): 0.5275 D(G(z)): 0.1504 / 0.4169\n",
      "[0/1][313/938] Loss_D: 0.8663 Loss_G: 2.4622 D(x): 0.9017 D(G(z)): 0.4825 / 0.1218\n",
      "[0/1][314/938] Loss_D: 0.7174 Loss_G: 2.0529 D(x): 0.6869 D(G(z)): 0.2061 / 0.1648\n",
      "[0/1][315/938] Loss_D: 0.6480 Loss_G: 1.8094 D(x): 0.7278 D(G(z)): 0.2366 / 0.2070\n",
      "[0/1][316/938] Loss_D: 0.7870 Loss_G: 2.8272 D(x): 0.8013 D(G(z)): 0.3660 / 0.0759\n",
      "[0/1][317/938] Loss_D: 0.9761 Loss_G: 0.8475 D(x): 0.5155 D(G(z)): 0.1655 / 0.4658\n",
      "[0/1][318/938] Loss_D: 0.9439 Loss_G: 3.7407 D(x): 0.9074 D(G(z)): 0.5334 / 0.0298\n",
      "[0/1][319/938] Loss_D: 1.0793 Loss_G: 0.8919 D(x): 0.4122 D(G(z)): 0.0739 / 0.4520\n",
      "[0/1][320/938] Loss_D: 0.9715 Loss_G: 2.9680 D(x): 0.9155 D(G(z)): 0.5232 / 0.0769\n",
      "[0/1][321/938] Loss_D: 0.6906 Loss_G: 1.9112 D(x): 0.6475 D(G(z)): 0.1500 / 0.1875\n",
      "[0/1][322/938] Loss_D: 0.6288 Loss_G: 2.2403 D(x): 0.8027 D(G(z)): 0.2752 / 0.1346\n",
      "[0/1][323/938] Loss_D: 0.7214 Loss_G: 1.9576 D(x): 0.6914 D(G(z)): 0.2387 / 0.1704\n",
      "[0/1][324/938] Loss_D: 0.7760 Loss_G: 2.7820 D(x): 0.7669 D(G(z)): 0.3467 / 0.0828\n",
      "[0/1][325/938] Loss_D: 0.6631 Loss_G: 1.8469 D(x): 0.6746 D(G(z)): 0.1693 / 0.2016\n",
      "[0/1][326/938] Loss_D: 0.6097 Loss_G: 3.5622 D(x): 0.8759 D(G(z)): 0.3453 / 0.0371\n",
      "[0/1][327/938] Loss_D: 0.8132 Loss_G: 1.1059 D(x): 0.5577 D(G(z)): 0.1125 / 0.3879\n",
      "[0/1][328/938] Loss_D: 1.0480 Loss_G: 5.2550 D(x): 0.9375 D(G(z)): 0.5475 / 0.0072\n",
      "[0/1][329/938] Loss_D: 2.0755 Loss_G: 0.2876 D(x): 0.1829 D(G(z)): 0.0213 / 0.7849\n",
      "[0/1][330/938] Loss_D: 2.1129 Loss_G: 4.4890 D(x): 0.9818 D(G(z)): 0.8173 / 0.0332\n",
      "[0/1][331/938] Loss_D: 1.3379 Loss_G: 1.6507 D(x): 0.3853 D(G(z)): 0.0690 / 0.2736\n",
      "[0/1][332/938] Loss_D: 0.8116 Loss_G: 2.5258 D(x): 0.8433 D(G(z)): 0.3985 / 0.1221\n",
      "[0/1][333/938] Loss_D: 0.7472 Loss_G: 2.1276 D(x): 0.6885 D(G(z)): 0.2137 / 0.1860\n",
      "[0/1][334/938] Loss_D: 0.7657 Loss_G: 2.5297 D(x): 0.7738 D(G(z)): 0.3184 / 0.1063\n",
      "[0/1][335/938] Loss_D: 0.5426 Loss_G: 2.7852 D(x): 0.8083 D(G(z)): 0.2326 / 0.0892\n",
      "[0/1][336/938] Loss_D: 0.7471 Loss_G: 1.5679 D(x): 0.6695 D(G(z)): 0.2207 / 0.2562\n",
      "[0/1][337/938] Loss_D: 0.5410 Loss_G: 2.0776 D(x): 0.8126 D(G(z)): 0.2490 / 0.1473\n",
      "[0/1][338/938] Loss_D: 0.5093 Loss_G: 2.7698 D(x): 0.8297 D(G(z)): 0.2508 / 0.0791\n",
      "[0/1][339/938] Loss_D: 0.5000 Loss_G: 1.7145 D(x): 0.7221 D(G(z)): 0.1265 / 0.2189\n",
      "[0/1][340/938] Loss_D: 0.7200 Loss_G: 3.5457 D(x): 0.8748 D(G(z)): 0.3970 / 0.0387\n",
      "[0/1][341/938] Loss_D: 0.6262 Loss_G: 1.4190 D(x): 0.6234 D(G(z)): 0.0843 / 0.2903\n",
      "[0/1][342/938] Loss_D: 0.6347 Loss_G: 3.4765 D(x): 0.9072 D(G(z)): 0.3763 / 0.0409\n",
      "[0/1][343/938] Loss_D: 0.5986 Loss_G: 1.7737 D(x): 0.6458 D(G(z)): 0.0957 / 0.2080\n",
      "[0/1][344/938] Loss_D: 0.5531 Loss_G: 3.1329 D(x): 0.8634 D(G(z)): 0.2920 / 0.0602\n",
      "[0/1][345/938] Loss_D: 0.6677 Loss_G: 1.3781 D(x): 0.6558 D(G(z)): 0.1512 / 0.3156\n",
      "[0/1][346/938] Loss_D: 0.8560 Loss_G: 5.3120 D(x): 0.9214 D(G(z)): 0.4685 / 0.0067\n",
      "[0/1][347/938] Loss_D: 2.5969 Loss_G: 0.4625 D(x): 0.1065 D(G(z)): 0.0168 / 0.6798\n",
      "[0/1][348/938] Loss_D: 1.5648 Loss_G: 6.2540 D(x): 0.9679 D(G(z)): 0.7307 / 0.0031\n",
      "[0/1][349/938] Loss_D: 3.6508 Loss_G: 0.3127 D(x): 0.0540 D(G(z)): 0.0153 / 0.7820\n",
      "[0/1][350/938] Loss_D: 2.5214 Loss_G: 2.2079 D(x): 0.9439 D(G(z)): 0.8033 / 0.1801\n",
      "[0/1][351/938] Loss_D: 1.2566 Loss_G: 2.3435 D(x): 0.5576 D(G(z)): 0.3385 / 0.1314\n",
      "[0/1][352/938] Loss_D: 1.1183 Loss_G: 0.9021 D(x): 0.5219 D(G(z)): 0.2637 / 0.4492\n",
      "[0/1][353/938] Loss_D: 1.0951 Loss_G: 2.0642 D(x): 0.7718 D(G(z)): 0.4957 / 0.1755\n",
      "[0/1][354/938] Loss_D: 1.1586 Loss_G: 2.1099 D(x): 0.6420 D(G(z)): 0.4195 / 0.1614\n",
      "[0/1][355/938] Loss_D: 1.2610 Loss_G: 1.0794 D(x): 0.4852 D(G(z)): 0.2852 / 0.3958\n",
      "[0/1][356/938] Loss_D: 1.2103 Loss_G: 2.8268 D(x): 0.8048 D(G(z)): 0.5661 / 0.0823\n",
      "[0/1][357/938] Loss_D: 1.0945 Loss_G: 1.0605 D(x): 0.4283 D(G(z)): 0.1264 / 0.4140\n",
      "[0/1][358/938] Loss_D: 1.2106 Loss_G: 3.0303 D(x): 0.8577 D(G(z)): 0.5818 / 0.0631\n",
      "[0/1][359/938] Loss_D: 1.0802 Loss_G: 1.4907 D(x): 0.4794 D(G(z)): 0.1810 / 0.2683\n",
      "[0/1][360/938] Loss_D: 0.9036 Loss_G: 1.6882 D(x): 0.7237 D(G(z)): 0.3864 / 0.2100\n",
      "[0/1][361/938] Loss_D: 0.7105 Loss_G: 1.7777 D(x): 0.7004 D(G(z)): 0.2632 / 0.1924\n",
      "[0/1][362/938] Loss_D: 0.7990 Loss_G: 2.5907 D(x): 0.7882 D(G(z)): 0.3539 / 0.1051\n",
      "[0/1][363/938] Loss_D: 0.9216 Loss_G: 0.8533 D(x): 0.5380 D(G(z)): 0.1748 / 0.4624\n",
      "[0/1][364/938] Loss_D: 0.8831 Loss_G: 3.0615 D(x): 0.8578 D(G(z)): 0.4806 / 0.0601\n",
      "[0/1][365/938] Loss_D: 1.1516 Loss_G: 0.6004 D(x): 0.4285 D(G(z)): 0.1160 / 0.5932\n",
      "[0/1][366/938] Loss_D: 1.5408 Loss_G: 4.2206 D(x): 0.9287 D(G(z)): 0.7052 / 0.0234\n",
      "[0/1][367/938] Loss_D: 1.8482 Loss_G: 0.7818 D(x): 0.2498 D(G(z)): 0.0372 / 0.4924\n",
      "[0/1][368/938] Loss_D: 1.0064 Loss_G: 2.2644 D(x): 0.9027 D(G(z)): 0.5571 / 0.1336\n",
      "[0/1][369/938] Loss_D: 0.7510 Loss_G: 2.4914 D(x): 0.7218 D(G(z)): 0.2983 / 0.1095\n",
      "[0/1][370/938] Loss_D: 0.9124 Loss_G: 0.9663 D(x): 0.5321 D(G(z)): 0.1593 / 0.4210\n",
      "[0/1][371/938] Loss_D: 1.0371 Loss_G: 3.0911 D(x): 0.9075 D(G(z)): 0.5478 / 0.0656\n",
      "[0/1][372/938] Loss_D: 0.8814 Loss_G: 1.2436 D(x): 0.5080 D(G(z)): 0.0913 / 0.3166\n",
      "[0/1][373/938] Loss_D: 0.9660 Loss_G: 3.1612 D(x): 0.8785 D(G(z)): 0.5218 / 0.0536\n",
      "[0/1][374/938] Loss_D: 0.9427 Loss_G: 1.2147 D(x): 0.5138 D(G(z)): 0.1261 / 0.3490\n",
      "[0/1][375/938] Loss_D: 0.7322 Loss_G: 2.3383 D(x): 0.8448 D(G(z)): 0.3768 / 0.1296\n",
      "[0/1][376/938] Loss_D: 0.7331 Loss_G: 1.4015 D(x): 0.6297 D(G(z)): 0.1904 / 0.2891\n",
      "[0/1][377/938] Loss_D: 0.7645 Loss_G: 3.0133 D(x): 0.8673 D(G(z)): 0.4236 / 0.0602\n",
      "[0/1][378/938] Loss_D: 0.9347 Loss_G: 0.7822 D(x): 0.4746 D(G(z)): 0.0872 / 0.5135\n",
      "[0/1][379/938] Loss_D: 1.4386 Loss_G: 4.2515 D(x): 0.9414 D(G(z)): 0.6883 / 0.0250\n",
      "[0/1][380/938] Loss_D: 1.3713 Loss_G: 1.4037 D(x): 0.3390 D(G(z)): 0.0457 / 0.3221\n",
      "[0/1][381/938] Loss_D: 0.7226 Loss_G: 2.5755 D(x): 0.8793 D(G(z)): 0.4082 / 0.0960\n",
      "[0/1][382/938] Loss_D: 0.7964 Loss_G: 1.7641 D(x): 0.6663 D(G(z)): 0.2486 / 0.2061\n",
      "[0/1][383/938] Loss_D: 0.8244 Loss_G: 1.2422 D(x): 0.6614 D(G(z)): 0.2751 / 0.3409\n",
      "[0/1][384/938] Loss_D: 0.7835 Loss_G: 3.6682 D(x): 0.8904 D(G(z)): 0.4450 / 0.0353\n",
      "[0/1][385/938] Loss_D: 0.9921 Loss_G: 0.8157 D(x): 0.4509 D(G(z)): 0.0697 / 0.4756\n",
      "[0/1][386/938] Loss_D: 1.2209 Loss_G: 3.7768 D(x): 0.9226 D(G(z)): 0.6366 / 0.0395\n",
      "[0/1][387/938] Loss_D: 1.3725 Loss_G: 0.7032 D(x): 0.3237 D(G(z)): 0.0615 / 0.5426\n",
      "[0/1][388/938] Loss_D: 1.2562 Loss_G: 3.6393 D(x): 0.9552 D(G(z)): 0.6364 / 0.0408\n",
      "[0/1][389/938] Loss_D: 0.8527 Loss_G: 1.6668 D(x): 0.5535 D(G(z)): 0.1315 / 0.2349\n",
      "[0/1][390/938] Loss_D: 0.6114 Loss_G: 2.0087 D(x): 0.8200 D(G(z)): 0.2960 / 0.1767\n",
      "[0/1][391/938] Loss_D: 0.6146 Loss_G: 2.0006 D(x): 0.7453 D(G(z)): 0.2437 / 0.1688\n",
      "[0/1][392/938] Loss_D: 0.5733 Loss_G: 2.4504 D(x): 0.7891 D(G(z)): 0.2481 / 0.1176\n",
      "[0/1][393/938] Loss_D: 0.6404 Loss_G: 1.7689 D(x): 0.7137 D(G(z)): 0.2066 / 0.2115\n",
      "[0/1][394/938] Loss_D: 0.4861 Loss_G: 3.3003 D(x): 0.8687 D(G(z)): 0.2663 / 0.0601\n",
      "[0/1][395/938] Loss_D: 0.4857 Loss_G: 2.3383 D(x): 0.7593 D(G(z)): 0.1492 / 0.1434\n",
      "[0/1][396/938] Loss_D: 0.7186 Loss_G: 2.2490 D(x): 0.7384 D(G(z)): 0.2758 / 0.1329\n",
      "[0/1][397/938] Loss_D: 0.4002 Loss_G: 2.5213 D(x): 0.8343 D(G(z)): 0.1778 / 0.1022\n",
      "[0/1][398/938] Loss_D: 0.5147 Loss_G: 1.8408 D(x): 0.7600 D(G(z)): 0.1805 / 0.1873\n",
      "[0/1][399/938] Loss_D: 0.5348 Loss_G: 2.7921 D(x): 0.8303 D(G(z)): 0.2458 / 0.0822\n",
      "[0/1][400/938] Loss_D: 0.3749 Loss_G: 2.7052 D(x): 0.8300 D(G(z)): 0.1510 / 0.0967\n",
      "[0/1][401/938] Loss_D: 0.3674 Loss_G: 2.5567 D(x): 0.8288 D(G(z)): 0.1426 / 0.1012\n",
      "[0/1][402/938] Loss_D: 0.5197 Loss_G: 3.3207 D(x): 0.8318 D(G(z)): 0.2448 / 0.0492\n",
      "[0/1][403/938] Loss_D: 0.7864 Loss_G: 1.1781 D(x): 0.5727 D(G(z)): 0.1081 / 0.3497\n",
      "[0/1][404/938] Loss_D: 0.7882 Loss_G: 4.4321 D(x): 0.8346 D(G(z)): 0.4049 / 0.0188\n",
      "[0/1][405/938] Loss_D: 2.5116 Loss_G: 0.0201 D(x): 0.1286 D(G(z)): 0.0556 / 0.9815\n",
      "[0/1][406/938] Loss_D: 5.9601 Loss_G: 3.6397 D(x): 0.9989 D(G(z)): 0.9910 / 0.0739\n",
      "[0/1][407/938] Loss_D: 1.0018 Loss_G: 3.4045 D(x): 0.5627 D(G(z)): 0.1462 / 0.0648\n",
      "[0/1][408/938] Loss_D: 1.1854 Loss_G: 0.4837 D(x): 0.4433 D(G(z)): 0.1582 / 0.6623\n",
      "[0/1][409/938] Loss_D: 1.8385 Loss_G: 3.3891 D(x): 0.9238 D(G(z)): 0.7468 / 0.0546\n",
      "[0/1][410/938] Loss_D: 1.5052 Loss_G: 0.9227 D(x): 0.3221 D(G(z)): 0.1178 / 0.4427\n",
      "[0/1][411/938] Loss_D: 0.8821 Loss_G: 1.5438 D(x): 0.7950 D(G(z)): 0.4226 / 0.2620\n",
      "[0/1][412/938] Loss_D: 0.9499 Loss_G: 2.8949 D(x): 0.7992 D(G(z)): 0.4504 / 0.0753\n",
      "[0/1][413/938] Loss_D: 1.0798 Loss_G: 1.0322 D(x): 0.4337 D(G(z)): 0.1060 / 0.3950\n",
      "[0/1][414/938] Loss_D: 0.9578 Loss_G: 2.3327 D(x): 0.8369 D(G(z)): 0.4885 / 0.1228\n",
      "[0/1][415/938] Loss_D: 1.0381 Loss_G: 0.8038 D(x): 0.4761 D(G(z)): 0.1917 / 0.4899\n",
      "[0/1][416/938] Loss_D: 1.0414 Loss_G: 2.5274 D(x): 0.8674 D(G(z)): 0.5314 / 0.1072\n",
      "[0/1][417/938] Loss_D: 0.9367 Loss_G: 1.2071 D(x): 0.5447 D(G(z)): 0.2204 / 0.3367\n",
      "[0/1][418/938] Loss_D: 0.6550 Loss_G: 1.5865 D(x): 0.7531 D(G(z)): 0.2782 / 0.2440\n",
      "[0/1][419/938] Loss_D: 0.7811 Loss_G: 2.5526 D(x): 0.8058 D(G(z)): 0.3861 / 0.1041\n",
      "[0/1][420/938] Loss_D: 0.7733 Loss_G: 1.2225 D(x): 0.6074 D(G(z)): 0.1740 / 0.3272\n",
      "[0/1][421/938] Loss_D: 0.8507 Loss_G: 2.1735 D(x): 0.7716 D(G(z)): 0.3914 / 0.1312\n",
      "[0/1][422/938] Loss_D: 0.6960 Loss_G: 1.4306 D(x): 0.6821 D(G(z)): 0.2352 / 0.2795\n",
      "[0/1][423/938] Loss_D: 0.5282 Loss_G: 2.2359 D(x): 0.8409 D(G(z)): 0.2741 / 0.1244\n",
      "[0/1][424/938] Loss_D: 0.7140 Loss_G: 1.6378 D(x): 0.6777 D(G(z)): 0.2335 / 0.2323\n",
      "[0/1][425/938] Loss_D: 0.6910 Loss_G: 2.9563 D(x): 0.8513 D(G(z)): 0.3641 / 0.0676\n",
      "[0/1][426/938] Loss_D: 0.9009 Loss_G: 1.6503 D(x): 0.5005 D(G(z)): 0.1038 / 0.2339\n",
      "[0/1][427/938] Loss_D: 0.8001 Loss_G: 1.3012 D(x): 0.7174 D(G(z)): 0.3128 / 0.3093\n",
      "[0/1][428/938] Loss_D: 0.8804 Loss_G: 2.9436 D(x): 0.7770 D(G(z)): 0.4219 / 0.0690\n",
      "[0/1][429/938] Loss_D: 1.2170 Loss_G: 0.2454 D(x): 0.3894 D(G(z)): 0.1322 / 0.7984\n",
      "[0/1][430/938] Loss_D: 2.2427 Loss_G: 4.4547 D(x): 0.9695 D(G(z)): 0.8417 / 0.0156\n",
      "[0/1][431/938] Loss_D: 1.7112 Loss_G: 1.0663 D(x): 0.2385 D(G(z)): 0.0504 / 0.4137\n",
      "[0/1][432/938] Loss_D: 1.1513 Loss_G: 2.0168 D(x): 0.8107 D(G(z)): 0.5337 / 0.1712\n",
      "[0/1][433/938] Loss_D: 0.9355 Loss_G: 1.6084 D(x): 0.6210 D(G(z)): 0.2490 / 0.2503\n",
      "[0/1][434/938] Loss_D: 0.7404 Loss_G: 1.9206 D(x): 0.7529 D(G(z)): 0.3199 / 0.1731\n",
      "[0/1][435/938] Loss_D: 0.8446 Loss_G: 1.1213 D(x): 0.6373 D(G(z)): 0.2486 / 0.3546\n",
      "[0/1][436/938] Loss_D: 0.8090 Loss_G: 3.4496 D(x): 0.8709 D(G(z)): 0.4560 / 0.0430\n",
      "[0/1][437/938] Loss_D: 0.8193 Loss_G: 1.0582 D(x): 0.5201 D(G(z)): 0.0826 / 0.3885\n",
      "[0/1][438/938] Loss_D: 0.8977 Loss_G: 3.0391 D(x): 0.8676 D(G(z)): 0.4834 / 0.0688\n",
      "[0/1][439/938] Loss_D: 1.0992 Loss_G: 0.6120 D(x): 0.4586 D(G(z)): 0.1670 / 0.5878\n",
      "[0/1][440/938] Loss_D: 1.0565 Loss_G: 4.2763 D(x): 0.9342 D(G(z)): 0.5809 / 0.0181\n",
      "[0/1][441/938] Loss_D: 1.9216 Loss_G: 0.4250 D(x): 0.1999 D(G(z)): 0.0388 / 0.6894\n",
      "[0/1][442/938] Loss_D: 1.6439 Loss_G: 3.5045 D(x): 0.9639 D(G(z)): 0.7374 / 0.0503\n",
      "[0/1][443/938] Loss_D: 1.2166 Loss_G: 1.4004 D(x): 0.4326 D(G(z)): 0.1092 / 0.3108\n",
      "[0/1][444/938] Loss_D: 0.7596 Loss_G: 1.7643 D(x): 0.8003 D(G(z)): 0.3708 / 0.2018\n",
      "[0/1][445/938] Loss_D: 0.6177 Loss_G: 2.3183 D(x): 0.7836 D(G(z)): 0.2828 / 0.1243\n",
      "[0/1][446/938] Loss_D: 0.7333 Loss_G: 1.6061 D(x): 0.6626 D(G(z)): 0.2081 / 0.2485\n",
      "[0/1][447/938] Loss_D: 0.5636 Loss_G: 2.2179 D(x): 0.8268 D(G(z)): 0.2900 / 0.1276\n",
      "[0/1][448/938] Loss_D: 0.6070 Loss_G: 2.0448 D(x): 0.7503 D(G(z)): 0.2392 / 0.1653\n",
      "[0/1][449/938] Loss_D: 0.5564 Loss_G: 1.6725 D(x): 0.7409 D(G(z)): 0.1887 / 0.2098\n",
      "[0/1][450/938] Loss_D: 0.5463 Loss_G: 2.7177 D(x): 0.8520 D(G(z)): 0.2963 / 0.0837\n",
      "[0/1][451/938] Loss_D: 0.6803 Loss_G: 1.3516 D(x): 0.6290 D(G(z)): 0.1315 / 0.3044\n",
      "[0/1][452/938] Loss_D: 0.7092 Loss_G: 3.8454 D(x): 0.8962 D(G(z)): 0.4007 / 0.0307\n",
      "[0/1][453/938] Loss_D: 1.3205 Loss_G: 0.3926 D(x): 0.3470 D(G(z)): 0.0688 / 0.7072\n",
      "[0/1][454/938] Loss_D: 1.6547 Loss_G: 4.2788 D(x): 0.9760 D(G(z)): 0.7191 / 0.0266\n",
      "[0/1][455/938] Loss_D: 1.9734 Loss_G: 0.5303 D(x): 0.1960 D(G(z)): 0.0538 / 0.6417\n",
      "[0/1][456/938] Loss_D: 1.3480 Loss_G: 2.6518 D(x): 0.9083 D(G(z)): 0.6521 / 0.0949\n",
      "[0/1][457/938] Loss_D: 1.2005 Loss_G: 1.2424 D(x): 0.4620 D(G(z)): 0.2342 / 0.3277\n",
      "[0/1][458/938] Loss_D: 0.9109 Loss_G: 1.3990 D(x): 0.6633 D(G(z)): 0.3361 / 0.2960\n",
      "[0/1][459/938] Loss_D: 0.9801 Loss_G: 2.5426 D(x): 0.7769 D(G(z)): 0.4559 / 0.1114\n",
      "[0/1][460/938] Loss_D: 1.1288 Loss_G: 1.0515 D(x): 0.4850 D(G(z)): 0.1927 / 0.4079\n",
      "[0/1][461/938] Loss_D: 0.9647 Loss_G: 2.6181 D(x): 0.8547 D(G(z)): 0.4982 / 0.0940\n",
      "[0/1][462/938] Loss_D: 0.8893 Loss_G: 1.0673 D(x): 0.5122 D(G(z)): 0.1352 / 0.3755\n",
      "[0/1][463/938] Loss_D: 0.7224 Loss_G: 2.5199 D(x): 0.8974 D(G(z)): 0.4206 / 0.1059\n",
      "[0/1][464/938] Loss_D: 0.7016 Loss_G: 1.7910 D(x): 0.6522 D(G(z)): 0.1899 / 0.2153\n",
      "[0/1][465/938] Loss_D: 0.6952 Loss_G: 1.7940 D(x): 0.7681 D(G(z)): 0.3053 / 0.2014\n",
      "[0/1][466/938] Loss_D: 0.7930 Loss_G: 2.1308 D(x): 0.7364 D(G(z)): 0.3331 / 0.1461\n",
      "[0/1][467/938] Loss_D: 0.6466 Loss_G: 1.2359 D(x): 0.6621 D(G(z)): 0.1609 / 0.3363\n",
      "[0/1][468/938] Loss_D: 0.7000 Loss_G: 3.3400 D(x): 0.8949 D(G(z)): 0.4070 / 0.0480\n",
      "[0/1][469/938] Loss_D: 0.9973 Loss_G: 0.7108 D(x): 0.4667 D(G(z)): 0.0981 / 0.5302\n",
      "[0/1][470/938] Loss_D: 1.0822 Loss_G: 3.8669 D(x): 0.9127 D(G(z)): 0.5893 / 0.0371\n",
      "[0/1][471/938] Loss_D: 1.4935 Loss_G: 0.6715 D(x): 0.3169 D(G(z)): 0.0579 / 0.5809\n",
      "[0/1][472/938] Loss_D: 1.3369 Loss_G: 3.0750 D(x): 0.9417 D(G(z)): 0.6500 / 0.0629\n",
      "[0/1][473/938] Loss_D: 0.7146 Loss_G: 2.1660 D(x): 0.6204 D(G(z)): 0.1348 / 0.1438\n",
      "[0/1][474/938] Loss_D: 0.6815 Loss_G: 1.2023 D(x): 0.6662 D(G(z)): 0.1715 / 0.3451\n",
      "[0/1][475/938] Loss_D: 0.8399 Loss_G: 2.7860 D(x): 0.8726 D(G(z)): 0.4595 / 0.0806\n",
      "[0/1][476/938] Loss_D: 0.8383 Loss_G: 1.2556 D(x): 0.5668 D(G(z)): 0.1610 / 0.3133\n",
      "[0/1][477/938] Loss_D: 0.6718 Loss_G: 1.7406 D(x): 0.7995 D(G(z)): 0.3200 / 0.2087\n",
      "[0/1][478/938] Loss_D: 0.6736 Loss_G: 2.9029 D(x): 0.8294 D(G(z)): 0.3557 / 0.0673\n",
      "[0/1][479/938] Loss_D: 0.6216 Loss_G: 1.3445 D(x): 0.6094 D(G(z)): 0.0795 / 0.3082\n",
      "[0/1][480/938] Loss_D: 0.7220 Loss_G: 2.9495 D(x): 0.9057 D(G(z)): 0.4142 / 0.0717\n",
      "[0/1][481/938] Loss_D: 0.9899 Loss_G: 1.2662 D(x): 0.4962 D(G(z)): 0.1167 / 0.3227\n",
      "[0/1][482/938] Loss_D: 0.7577 Loss_G: 3.2259 D(x): 0.8825 D(G(z)): 0.4145 / 0.0529\n",
      "[0/1][483/938] Loss_D: 1.3327 Loss_G: 0.2974 D(x): 0.3523 D(G(z)): 0.1174 / 0.7597\n",
      "[0/1][484/938] Loss_D: 1.6037 Loss_G: 3.8693 D(x): 0.9587 D(G(z)): 0.7177 / 0.0389\n",
      "[0/1][485/938] Loss_D: 1.3749 Loss_G: 1.3963 D(x): 0.3878 D(G(z)): 0.1270 / 0.3254\n",
      "[0/1][486/938] Loss_D: 0.9867 Loss_G: 2.3876 D(x): 0.8281 D(G(z)): 0.5022 / 0.1173\n",
      "[0/1][487/938] Loss_D: 1.1109 Loss_G: 0.7675 D(x): 0.4541 D(G(z)): 0.1378 / 0.5027\n",
      "[0/1][488/938] Loss_D: 1.0133 Loss_G: 3.0010 D(x): 0.9304 D(G(z)): 0.5591 / 0.0693\n",
      "[0/1][489/938] Loss_D: 0.8825 Loss_G: 1.2700 D(x): 0.5189 D(G(z)): 0.1225 / 0.3376\n",
      "[0/1][490/938] Loss_D: 0.7733 Loss_G: 2.5163 D(x): 0.8725 D(G(z)): 0.4233 / 0.1027\n",
      "[0/1][491/938] Loss_D: 0.7692 Loss_G: 1.4609 D(x): 0.6118 D(G(z)): 0.1822 / 0.2752\n",
      "[0/1][492/938] Loss_D: 0.7428 Loss_G: 2.8113 D(x): 0.8457 D(G(z)): 0.4028 / 0.0762\n",
      "[0/1][493/938] Loss_D: 0.8944 Loss_G: 0.8664 D(x): 0.4952 D(G(z)): 0.0985 / 0.4623\n",
      "[0/1][494/938] Loss_D: 0.8256 Loss_G: 2.9932 D(x): 0.9168 D(G(z)): 0.4697 / 0.0684\n",
      "[0/1][495/938] Loss_D: 0.9087 Loss_G: 1.1233 D(x): 0.5062 D(G(z)): 0.1126 / 0.3775\n",
      "[0/1][496/938] Loss_D: 0.9248 Loss_G: 2.7627 D(x): 0.8852 D(G(z)): 0.4934 / 0.0883\n",
      "[0/1][497/938] Loss_D: 0.9532 Loss_G: 1.3084 D(x): 0.5357 D(G(z)): 0.1568 / 0.3311\n",
      "[0/1][498/938] Loss_D: 0.7244 Loss_G: 2.4429 D(x): 0.8584 D(G(z)): 0.3981 / 0.1112\n",
      "[0/1][499/938] Loss_D: 0.6437 Loss_G: 1.9911 D(x): 0.6983 D(G(z)): 0.2026 / 0.1660\n",
      "[0/1][500/938] Loss_D: 0.5830 Loss_G: 1.5259 D(x): 0.7418 D(G(z)): 0.2143 / 0.2492\n",
      "[0/1][501/938] Loss_D: 0.7255 Loss_G: 3.1421 D(x): 0.8676 D(G(z)): 0.3774 / 0.0623\n",
      "[0/1][502/938] Loss_D: 0.7717 Loss_G: 1.1560 D(x): 0.5793 D(G(z)): 0.0906 / 0.3672\n",
      "[0/1][503/938] Loss_D: 0.7494 Loss_G: 2.8737 D(x): 0.8942 D(G(z)): 0.4307 / 0.0726\n",
      "[0/1][504/938] Loss_D: 0.7404 Loss_G: 1.3216 D(x): 0.5822 D(G(z)): 0.1185 / 0.3090\n",
      "[0/1][505/938] Loss_D: 0.6952 Loss_G: 3.5333 D(x): 0.9039 D(G(z)): 0.3965 / 0.0388\n",
      "[0/1][506/938] Loss_D: 1.1318 Loss_G: 0.5178 D(x): 0.3938 D(G(z)): 0.0758 / 0.6331\n",
      "[0/1][507/938] Loss_D: 1.2456 Loss_G: 4.6431 D(x): 0.9714 D(G(z)): 0.6618 / 0.0160\n",
      "[0/1][508/938] Loss_D: 1.9514 Loss_G: 0.6568 D(x): 0.2217 D(G(z)): 0.0468 / 0.5890\n",
      "[0/1][509/938] Loss_D: 1.1369 Loss_G: 3.0328 D(x): 0.9427 D(G(z)): 0.5906 / 0.0753\n",
      "[0/1][510/938] Loss_D: 0.9640 Loss_G: 1.3534 D(x): 0.5100 D(G(z)): 0.1326 / 0.3142\n",
      "[0/1][511/938] Loss_D: 0.7713 Loss_G: 2.5458 D(x): 0.8689 D(G(z)): 0.4226 / 0.1151\n",
      "[0/1][512/938] Loss_D: 0.9701 Loss_G: 1.1069 D(x): 0.5464 D(G(z)): 0.1902 / 0.3784\n",
      "[0/1][513/938] Loss_D: 0.9339 Loss_G: 2.4293 D(x): 0.8431 D(G(z)): 0.4819 / 0.1123\n",
      "[0/1][514/938] Loss_D: 0.6441 Loss_G: 1.9786 D(x): 0.6758 D(G(z)): 0.1728 / 0.1819\n",
      "[0/1][515/938] Loss_D: 0.7074 Loss_G: 1.2159 D(x): 0.6856 D(G(z)): 0.2346 / 0.3375\n",
      "[0/1][516/938] Loss_D: 0.7131 Loss_G: 3.3250 D(x): 0.8869 D(G(z)): 0.4087 / 0.0471\n",
      "[0/1][517/938] Loss_D: 0.8857 Loss_G: 0.9771 D(x): 0.5196 D(G(z)): 0.1268 / 0.4267\n",
      "[0/1][518/938] Loss_D: 0.7678 Loss_G: 2.4948 D(x): 0.8408 D(G(z)): 0.3959 / 0.1169\n",
      "[0/1][519/938] Loss_D: 0.4798 Loss_G: 2.4290 D(x): 0.7826 D(G(z)): 0.1768 / 0.1061\n",
      "[0/1][520/938] Loss_D: 0.5720 Loss_G: 1.2567 D(x): 0.6819 D(G(z)): 0.1246 / 0.3446\n",
      "[0/1][521/938] Loss_D: 0.7267 Loss_G: 3.5203 D(x): 0.9164 D(G(z)): 0.4085 / 0.0398\n",
      "[0/1][522/938] Loss_D: 1.0258 Loss_G: 0.8834 D(x): 0.4785 D(G(z)): 0.1093 / 0.4681\n",
      "[0/1][523/938] Loss_D: 0.7540 Loss_G: 4.0964 D(x): 0.9482 D(G(z)): 0.4382 / 0.0262\n",
      "[0/1][524/938] Loss_D: 1.7527 Loss_G: 0.2729 D(x): 0.2826 D(G(z)): 0.0973 / 0.7864\n",
      "[0/1][525/938] Loss_D: 2.3964 Loss_G: 3.6210 D(x): 0.9725 D(G(z)): 0.8287 / 0.0498\n",
      "[0/1][526/938] Loss_D: 1.2828 Loss_G: 1.4996 D(x): 0.3792 D(G(z)): 0.0976 / 0.3060\n",
      "[0/1][527/938] Loss_D: 0.7030 Loss_G: 2.0459 D(x): 0.8317 D(G(z)): 0.3504 / 0.1676\n",
      "[0/1][528/938] Loss_D: 0.7962 Loss_G: 2.2549 D(x): 0.7118 D(G(z)): 0.2879 / 0.1355\n",
      "[0/1][529/938] Loss_D: 0.8514 Loss_G: 1.4987 D(x): 0.6259 D(G(z)): 0.2546 / 0.2617\n",
      "[0/1][530/938] Loss_D: 0.7337 Loss_G: 2.9524 D(x): 0.8209 D(G(z)): 0.3774 / 0.0680\n",
      "[0/1][531/938] Loss_D: 1.2834 Loss_G: 0.6553 D(x): 0.4143 D(G(z)): 0.1128 / 0.5677\n",
      "[0/1][532/938] Loss_D: 0.9639 Loss_G: 3.4407 D(x): 0.9357 D(G(z)): 0.5245 / 0.0460\n",
      "[0/1][533/938] Loss_D: 1.1282 Loss_G: 0.6226 D(x): 0.4174 D(G(z)): 0.0874 / 0.5829\n",
      "[0/1][534/938] Loss_D: 1.5467 Loss_G: 3.9113 D(x): 0.9132 D(G(z)): 0.7004 / 0.0379\n",
      "[0/1][535/938] Loss_D: 1.6531 Loss_G: 0.8151 D(x): 0.3308 D(G(z)): 0.0879 / 0.4900\n",
      "[0/1][536/938] Loss_D: 1.0146 Loss_G: 2.3787 D(x): 0.8660 D(G(z)): 0.5339 / 0.1334\n",
      "[0/1][537/938] Loss_D: 0.9708 Loss_G: 1.5528 D(x): 0.5545 D(G(z)): 0.2072 / 0.2591\n",
      "[0/1][538/938] Loss_D: 0.7787 Loss_G: 2.0253 D(x): 0.7920 D(G(z)): 0.3552 / 0.1599\n",
      "[0/1][539/938] Loss_D: 0.8438 Loss_G: 1.4507 D(x): 0.6187 D(G(z)): 0.2355 / 0.2767\n",
      "[0/1][540/938] Loss_D: 0.8233 Loss_G: 2.7370 D(x): 0.8530 D(G(z)): 0.4386 / 0.0905\n",
      "[0/1][541/938] Loss_D: 0.6651 Loss_G: 1.7011 D(x): 0.6353 D(G(z)): 0.1321 / 0.2244\n",
      "[0/1][542/938] Loss_D: 0.6094 Loss_G: 2.1510 D(x): 0.8283 D(G(z)): 0.3044 / 0.1401\n",
      "[0/1][543/938] Loss_D: 0.6941 Loss_G: 1.8001 D(x): 0.7234 D(G(z)): 0.2649 / 0.1958\n",
      "[0/1][544/938] Loss_D: 0.5973 Loss_G: 2.3025 D(x): 0.7810 D(G(z)): 0.2625 / 0.1209\n",
      "[0/1][545/938] Loss_D: 0.5662 Loss_G: 1.8277 D(x): 0.7337 D(G(z)): 0.1970 / 0.2012\n",
      "[0/1][546/938] Loss_D: 0.4860 Loss_G: 2.2553 D(x): 0.8162 D(G(z)): 0.2170 / 0.1307\n",
      "[0/1][547/938] Loss_D: 0.4738 Loss_G: 2.4115 D(x): 0.8126 D(G(z)): 0.2060 / 0.1209\n",
      "[0/1][548/938] Loss_D: 0.5651 Loss_G: 1.7186 D(x): 0.7367 D(G(z)): 0.1727 / 0.2163\n",
      "[0/1][549/938] Loss_D: 0.5037 Loss_G: 3.0718 D(x): 0.8808 D(G(z)): 0.2887 / 0.0626\n",
      "[0/1][550/938] Loss_D: 0.6819 Loss_G: 1.2578 D(x): 0.5943 D(G(z)): 0.0867 / 0.3502\n",
      "[0/1][551/938] Loss_D: 0.7942 Loss_G: 4.3501 D(x): 0.8911 D(G(z)): 0.4549 / 0.0171\n",
      "[0/1][552/938] Loss_D: 1.5373 Loss_G: 0.2390 D(x): 0.2965 D(G(z)): 0.0679 / 0.8058\n",
      "[0/1][553/938] Loss_D: 2.0231 Loss_G: 5.0475 D(x): 0.9789 D(G(z)): 0.7883 / 0.0117\n",
      "[0/1][554/938] Loss_D: 1.8729 Loss_G: 0.3460 D(x): 0.2478 D(G(z)): 0.0447 / 0.7355\n",
      "[0/1][555/938] Loss_D: 1.9382 Loss_G: 4.2175 D(x): 0.9562 D(G(z)): 0.7753 / 0.0240\n",
      "[0/1][556/938] Loss_D: 1.8351 Loss_G: 1.0820 D(x): 0.2476 D(G(z)): 0.0703 / 0.4035\n",
      "[0/1][557/938] Loss_D: 1.0776 Loss_G: 1.1884 D(x): 0.7526 D(G(z)): 0.4504 / 0.3596\n",
      "[0/1][558/938] Loss_D: 1.0548 Loss_G: 2.4197 D(x): 0.8070 D(G(z)): 0.4943 / 0.1246\n",
      "[0/1][559/938] Loss_D: 1.1288 Loss_G: 1.0910 D(x): 0.4818 D(G(z)): 0.2208 / 0.3711\n",
      "[0/1][560/938] Loss_D: 0.6984 Loss_G: 1.8750 D(x): 0.8330 D(G(z)): 0.3561 / 0.1877\n",
      "[0/1][561/938] Loss_D: 0.8117 Loss_G: 2.0879 D(x): 0.7090 D(G(z)): 0.3084 / 0.1597\n",
      "[0/1][562/938] Loss_D: 0.6669 Loss_G: 1.5355 D(x): 0.6911 D(G(z)): 0.2033 / 0.2603\n",
      "[0/1][563/938] Loss_D: 0.8520 Loss_G: 2.0547 D(x): 0.7728 D(G(z)): 0.3865 / 0.1644\n",
      "[0/1][564/938] Loss_D: 0.8201 Loss_G: 1.4425 D(x): 0.6288 D(G(z)): 0.2371 / 0.2657\n",
      "[0/1][565/938] Loss_D: 0.7644 Loss_G: 2.0073 D(x): 0.7681 D(G(z)): 0.3410 / 0.1660\n",
      "[0/1][566/938] Loss_D: 0.7358 Loss_G: 1.7712 D(x): 0.6882 D(G(z)): 0.2339 / 0.2083\n",
      "[0/1][567/938] Loss_D: 0.5888 Loss_G: 1.6703 D(x): 0.7553 D(G(z)): 0.2274 / 0.2390\n",
      "[0/1][568/938] Loss_D: 0.7675 Loss_G: 2.3702 D(x): 0.7935 D(G(z)): 0.3691 / 0.1090\n",
      "[0/1][569/938] Loss_D: 0.6617 Loss_G: 1.8859 D(x): 0.6912 D(G(z)): 0.2052 / 0.1866\n",
      "[0/1][570/938] Loss_D: 0.6764 Loss_G: 1.4366 D(x): 0.6823 D(G(z)): 0.2029 / 0.2705\n",
      "[0/1][571/938] Loss_D: 0.7898 Loss_G: 2.6316 D(x): 0.8148 D(G(z)): 0.3931 / 0.0975\n",
      "[0/1][572/938] Loss_D: 0.7286 Loss_G: 1.4422 D(x): 0.6327 D(G(z)): 0.1860 / 0.2789\n",
      "[0/1][573/938] Loss_D: 0.6123 Loss_G: 3.1827 D(x): 0.8604 D(G(z)): 0.3277 / 0.0554\n",
      "[0/1][574/938] Loss_D: 0.7511 Loss_G: 1.0672 D(x): 0.5849 D(G(z)): 0.1212 / 0.3862\n",
      "[0/1][575/938] Loss_D: 0.8514 Loss_G: 3.2871 D(x): 0.8871 D(G(z)): 0.4733 / 0.0500\n",
      "[0/1][576/938] Loss_D: 1.2347 Loss_G: 0.6417 D(x): 0.4131 D(G(z)): 0.1320 / 0.5722\n",
      "[0/1][577/938] Loss_D: 1.0691 Loss_G: 3.1701 D(x): 0.8916 D(G(z)): 0.5689 / 0.0597\n",
      "[0/1][578/938] Loss_D: 1.1006 Loss_G: 0.9932 D(x): 0.4471 D(G(z)): 0.1191 / 0.4380\n",
      "[0/1][579/938] Loss_D: 0.7880 Loss_G: 2.9096 D(x): 0.8921 D(G(z)): 0.4235 / 0.0773\n",
      "[0/1][580/938] Loss_D: 0.6484 Loss_G: 1.9923 D(x): 0.6713 D(G(z)): 0.1652 / 0.1659\n",
      "[0/1][581/938] Loss_D: 0.5757 Loss_G: 2.4154 D(x): 0.8108 D(G(z)): 0.2774 / 0.1164\n",
      "[0/1][582/938] Loss_D: 0.6607 Loss_G: 1.0362 D(x): 0.6463 D(G(z)): 0.1513 / 0.3999\n",
      "[0/1][583/938] Loss_D: 0.9484 Loss_G: 3.8441 D(x): 0.9140 D(G(z)): 0.5265 / 0.0292\n",
      "[0/1][584/938] Loss_D: 1.2629 Loss_G: 0.7284 D(x): 0.3886 D(G(z)): 0.0615 / 0.5212\n",
      "[0/1][585/938] Loss_D: 1.2469 Loss_G: 4.0795 D(x): 0.9315 D(G(z)): 0.6387 / 0.0247\n",
      "[0/1][586/938] Loss_D: 1.7454 Loss_G: 0.5859 D(x): 0.2532 D(G(z)): 0.0474 / 0.5937\n",
      "[0/1][587/938] Loss_D: 1.2062 Loss_G: 2.9089 D(x): 0.9357 D(G(z)): 0.6251 / 0.0747\n",
      "[0/1][588/938] Loss_D: 0.7368 Loss_G: 2.4677 D(x): 0.6681 D(G(z)): 0.2145 / 0.1098\n",
      "[0/1][589/938] Loss_D: 0.9612 Loss_G: 1.0139 D(x): 0.5337 D(G(z)): 0.1830 / 0.4112\n",
      "[0/1][590/938] Loss_D: 1.0289 Loss_G: 3.7629 D(x): 0.9065 D(G(z)): 0.5411 / 0.0315\n",
      "[0/1][591/938] Loss_D: 1.1930 Loss_G: 0.7888 D(x): 0.3838 D(G(z)): 0.0655 / 0.5039\n",
      "[0/1][592/938] Loss_D: 0.9114 Loss_G: 3.2666 D(x): 0.9504 D(G(z)): 0.5281 / 0.0508\n",
      "[0/1][593/938] Loss_D: 0.7960 Loss_G: 1.7206 D(x): 0.5946 D(G(z)): 0.1536 / 0.2227\n",
      "[0/1][594/938] Loss_D: 0.8453 Loss_G: 1.5451 D(x): 0.6937 D(G(z)): 0.3259 / 0.2523\n",
      "[0/1][595/938] Loss_D: 0.6616 Loss_G: 2.2802 D(x): 0.7826 D(G(z)): 0.2864 / 0.1435\n",
      "[0/1][596/938] Loss_D: 0.7834 Loss_G: 2.2207 D(x): 0.7270 D(G(z)): 0.3169 / 0.1328\n",
      "[0/1][597/938] Loss_D: 0.6572 Loss_G: 1.8366 D(x): 0.6963 D(G(z)): 0.2015 / 0.2106\n",
      "[0/1][598/938] Loss_D: 0.5538 Loss_G: 2.3189 D(x): 0.8288 D(G(z)): 0.2803 / 0.1208\n",
      "[0/1][599/938] Loss_D: 0.6336 Loss_G: 2.1663 D(x): 0.7485 D(G(z)): 0.2597 / 0.1407\n",
      "[0/1][600/938] Loss_D: 0.5445 Loss_G: 1.8347 D(x): 0.7534 D(G(z)): 0.1970 / 0.1902\n",
      "[0/1][601/938] Loss_D: 0.4583 Loss_G: 2.2854 D(x): 0.8267 D(G(z)): 0.2049 / 0.1310\n",
      "[0/1][602/938] Loss_D: 0.5471 Loss_G: 2.7868 D(x): 0.8157 D(G(z)): 0.2529 / 0.0831\n",
      "[0/1][603/938] Loss_D: 0.7224 Loss_G: 1.0884 D(x): 0.6103 D(G(z)): 0.1339 / 0.3805\n",
      "[0/1][604/938] Loss_D: 0.7769 Loss_G: 3.7923 D(x): 0.9055 D(G(z)): 0.4400 / 0.0305\n",
      "[0/1][605/938] Loss_D: 1.2111 Loss_G: 0.4607 D(x): 0.3672 D(G(z)): 0.0621 / 0.6695\n",
      "[0/1][606/938] Loss_D: 1.5307 Loss_G: 4.5436 D(x): 0.9562 D(G(z)): 0.7141 / 0.0148\n",
      "[0/1][607/938] Loss_D: 1.9073 Loss_G: 0.5503 D(x): 0.2387 D(G(z)): 0.0760 / 0.6141\n",
      "[0/1][608/938] Loss_D: 1.1542 Loss_G: 4.3457 D(x): 0.9565 D(G(z)): 0.6054 / 0.0260\n",
      "[0/1][609/938] Loss_D: 1.4822 Loss_G: 0.6103 D(x): 0.3374 D(G(z)): 0.0739 / 0.5936\n",
      "[0/1][610/938] Loss_D: 1.3005 Loss_G: 3.1438 D(x): 0.9246 D(G(z)): 0.6242 / 0.0785\n",
      "[0/1][611/938] Loss_D: 0.8509 Loss_G: 1.9149 D(x): 0.5872 D(G(z)): 0.1891 / 0.1856\n",
      "[0/1][612/938] Loss_D: 1.0376 Loss_G: 1.7424 D(x): 0.6656 D(G(z)): 0.3692 / 0.2339\n",
      "[0/1][613/938] Loss_D: 0.7434 Loss_G: 2.3371 D(x): 0.7778 D(G(z)): 0.3266 / 0.1297\n",
      "[0/1][614/938] Loss_D: 0.7732 Loss_G: 1.5410 D(x): 0.6182 D(G(z)): 0.1867 / 0.2637\n",
      "[0/1][615/938] Loss_D: 0.7309 Loss_G: 2.3830 D(x): 0.8149 D(G(z)): 0.3566 / 0.1197\n",
      "[0/1][616/938] Loss_D: 0.8002 Loss_G: 1.4016 D(x): 0.5986 D(G(z)): 0.1965 / 0.3081\n",
      "[0/1][617/938] Loss_D: 0.7279 Loss_G: 3.2534 D(x): 0.8829 D(G(z)): 0.4116 / 0.0515\n",
      "[0/1][618/938] Loss_D: 0.8431 Loss_G: 1.2988 D(x): 0.5480 D(G(z)): 0.1452 / 0.3278\n",
      "[0/1][619/938] Loss_D: 0.7157 Loss_G: 2.8122 D(x): 0.8887 D(G(z)): 0.4073 / 0.0846\n",
      "[0/1][620/938] Loss_D: 0.7984 Loss_G: 1.2643 D(x): 0.5905 D(G(z)): 0.1654 / 0.3424\n",
      "[0/1][621/938] Loss_D: 0.8672 Loss_G: 1.8213 D(x): 0.7551 D(G(z)): 0.3701 / 0.1967\n",
      "[0/1][622/938] Loss_D: 0.6831 Loss_G: 2.1616 D(x): 0.7256 D(G(z)): 0.2688 / 0.1421\n",
      "[0/1][623/938] Loss_D: 0.6506 Loss_G: 2.7395 D(x): 0.7714 D(G(z)): 0.2750 / 0.0848\n",
      "[0/1][624/938] Loss_D: 0.7506 Loss_G: 1.1749 D(x): 0.6107 D(G(z)): 0.1634 / 0.3635\n",
      "[0/1][625/938] Loss_D: 0.7481 Loss_G: 3.4398 D(x): 0.9017 D(G(z)): 0.4306 / 0.0411\n",
      "[0/1][626/938] Loss_D: 1.0391 Loss_G: 0.6624 D(x): 0.4451 D(G(z)): 0.1001 / 0.5481\n",
      "[0/1][627/938] Loss_D: 1.0193 Loss_G: 4.2658 D(x): 0.9541 D(G(z)): 0.5770 / 0.0201\n",
      "[0/1][628/938] Loss_D: 0.8477 Loss_G: 1.5405 D(x): 0.5251 D(G(z)): 0.0716 / 0.2852\n",
      "[0/1][629/938] Loss_D: 0.8856 Loss_G: 2.8427 D(x): 0.8487 D(G(z)): 0.4380 / 0.0892\n",
      "[0/1][630/938] Loss_D: 0.8367 Loss_G: 1.4052 D(x): 0.5908 D(G(z)): 0.1728 / 0.2894\n",
      "[0/1][631/938] Loss_D: 0.8670 Loss_G: 3.1432 D(x): 0.8420 D(G(z)): 0.4325 / 0.0733\n",
      "[0/1][632/938] Loss_D: 0.9542 Loss_G: 0.9521 D(x): 0.4949 D(G(z)): 0.1300 / 0.4737\n",
      "[0/1][633/938] Loss_D: 0.9785 Loss_G: 3.6054 D(x): 0.9042 D(G(z)): 0.5008 / 0.0470\n",
      "[0/1][634/938] Loss_D: 0.8114 Loss_G: 1.6215 D(x): 0.5819 D(G(z)): 0.1130 / 0.2722\n",
      "[0/1][635/938] Loss_D: 0.8123 Loss_G: 2.8765 D(x): 0.8163 D(G(z)): 0.3807 / 0.0792\n",
      "[0/1][636/938] Loss_D: 1.1328 Loss_G: 0.6161 D(x): 0.4684 D(G(z)): 0.1498 / 0.5852\n",
      "[0/1][637/938] Loss_D: 1.4084 Loss_G: 4.4332 D(x): 0.9531 D(G(z)): 0.6751 / 0.0158\n",
      "[0/1][638/938] Loss_D: 2.0304 Loss_G: 0.6239 D(x): 0.1819 D(G(z)): 0.0400 / 0.6070\n",
      "[0/1][639/938] Loss_D: 1.1986 Loss_G: 3.1107 D(x): 0.9240 D(G(z)): 0.5967 / 0.0676\n",
      "[0/1][640/938] Loss_D: 1.0133 Loss_G: 1.4329 D(x): 0.5147 D(G(z)): 0.1630 / 0.2897\n",
      "[0/1][641/938] Loss_D: 0.8258 Loss_G: 2.6861 D(x): 0.8314 D(G(z)): 0.4210 / 0.1020\n",
      "[0/1][642/938] Loss_D: 0.9464 Loss_G: 0.9114 D(x): 0.5375 D(G(z)): 0.1923 / 0.4351\n",
      "[0/1][643/938] Loss_D: 0.9890 Loss_G: 3.4453 D(x): 0.8848 D(G(z)): 0.5359 / 0.0394\n",
      "[0/1][644/938] Loss_D: 1.1534 Loss_G: 0.8532 D(x): 0.4170 D(G(z)): 0.1039 / 0.4979\n",
      "[0/1][645/938] Loss_D: 0.8378 Loss_G: 2.9416 D(x): 0.9188 D(G(z)): 0.4703 / 0.0738\n",
      "[0/1][646/938] Loss_D: 0.6569 Loss_G: 1.9234 D(x): 0.6722 D(G(z)): 0.1573 / 0.1746\n",
      "[0/1][647/938] Loss_D: 0.6901 Loss_G: 1.8991 D(x): 0.7692 D(G(z)): 0.3055 / 0.1783\n",
      "[0/1][648/938] Loss_D: 0.8201 Loss_G: 2.1920 D(x): 0.7314 D(G(z)): 0.3302 / 0.1387\n",
      "[0/1][649/938] Loss_D: 0.5988 Loss_G: 2.1745 D(x): 0.7212 D(G(z)): 0.2070 / 0.1459\n",
      "[0/1][650/938] Loss_D: 0.6050 Loss_G: 2.3340 D(x): 0.7641 D(G(z)): 0.2378 / 0.1265\n",
      "[0/1][651/938] Loss_D: 0.5807 Loss_G: 2.3384 D(x): 0.7956 D(G(z)): 0.2587 / 0.1252\n",
      "[0/1][652/938] Loss_D: 0.7468 Loss_G: 1.9148 D(x): 0.7005 D(G(z)): 0.2709 / 0.1757\n",
      "[0/1][653/938] Loss_D: 0.5215 Loss_G: 1.8391 D(x): 0.7529 D(G(z)): 0.1850 / 0.1946\n",
      "[0/1][654/938] Loss_D: 0.8012 Loss_G: 3.7042 D(x): 0.8249 D(G(z)): 0.4082 / 0.0323\n",
      "[0/1][655/938] Loss_D: 1.2209 Loss_G: 0.4489 D(x): 0.3833 D(G(z)): 0.0956 / 0.6818\n",
      "[0/1][656/938] Loss_D: 1.2363 Loss_G: 6.1950 D(x): 0.9657 D(G(z)): 0.6534 / 0.0030\n",
      "[0/1][657/938] Loss_D: 3.1819 Loss_G: 0.4940 D(x): 0.0642 D(G(z)): 0.0071 / 0.6704\n",
      "[0/1][658/938] Loss_D: 1.8113 Loss_G: 3.1302 D(x): 0.9506 D(G(z)): 0.7440 / 0.0861\n",
      "[0/1][659/938] Loss_D: 1.2257 Loss_G: 1.7399 D(x): 0.5117 D(G(z)): 0.2299 / 0.2350\n",
      "[0/1][660/938] Loss_D: 0.8697 Loss_G: 1.2356 D(x): 0.6454 D(G(z)): 0.2757 / 0.3492\n",
      "[0/1][661/938] Loss_D: 0.9855 Loss_G: 2.9147 D(x): 0.8719 D(G(z)): 0.5171 / 0.0825\n",
      "[0/1][662/938] Loss_D: 1.2371 Loss_G: 0.8121 D(x): 0.4100 D(G(z)): 0.1681 / 0.4960\n",
      "[0/1][663/938] Loss_D: 1.0919 Loss_G: 2.4439 D(x): 0.8358 D(G(z)): 0.5412 / 0.1223\n",
      "[0/1][664/938] Loss_D: 0.9175 Loss_G: 1.4316 D(x): 0.5834 D(G(z)): 0.2230 / 0.2784\n",
      "[0/1][665/938] Loss_D: 0.8171 Loss_G: 1.5527 D(x): 0.7121 D(G(z)): 0.3279 / 0.2387\n",
      "[0/1][666/938] Loss_D: 0.9086 Loss_G: 2.5752 D(x): 0.7633 D(G(z)): 0.4194 / 0.1023\n",
      "[0/1][667/938] Loss_D: 0.9945 Loss_G: 0.9958 D(x): 0.5249 D(G(z)): 0.2120 / 0.4194\n",
      "[0/1][668/938] Loss_D: 0.8524 Loss_G: 2.2654 D(x): 0.8065 D(G(z)): 0.4251 / 0.1284\n",
      "[0/1][669/938] Loss_D: 0.7815 Loss_G: 1.7458 D(x): 0.6672 D(G(z)): 0.2624 / 0.2047\n",
      "[0/1][670/938] Loss_D: 0.8731 Loss_G: 1.0075 D(x): 0.6016 D(G(z)): 0.2329 / 0.4058\n",
      "[0/1][671/938] Loss_D: 0.9460 Loss_G: 3.1548 D(x): 0.8691 D(G(z)): 0.5139 / 0.0631\n",
      "[0/1][672/938] Loss_D: 0.7339 Loss_G: 1.8333 D(x): 0.5543 D(G(z)): 0.0854 / 0.2327\n",
      "[0/1][673/938] Loss_D: 0.7728 Loss_G: 1.6982 D(x): 0.7818 D(G(z)): 0.3538 / 0.2260\n",
      "[0/1][674/938] Loss_D: 0.8158 Loss_G: 1.4675 D(x): 0.6876 D(G(z)): 0.2878 / 0.2736\n",
      "[0/1][675/938] Loss_D: 0.7606 Loss_G: 3.2399 D(x): 0.8376 D(G(z)): 0.3937 / 0.0533\n",
      "[0/1][676/938] Loss_D: 1.0494 Loss_G: 1.0438 D(x): 0.4741 D(G(z)): 0.1136 / 0.4046\n",
      "[0/1][677/938] Loss_D: 0.7166 Loss_G: 3.0112 D(x): 0.8904 D(G(z)): 0.4244 / 0.0572\n",
      "[0/1][678/938] Loss_D: 1.2892 Loss_G: 0.2773 D(x): 0.3662 D(G(z)): 0.1193 / 0.7802\n",
      "[0/1][679/938] Loss_D: 2.0532 Loss_G: 4.6732 D(x): 0.9686 D(G(z)): 0.8034 / 0.0155\n",
      "[0/1][680/938] Loss_D: 1.4118 Loss_G: 1.7291 D(x): 0.3538 D(G(z)): 0.0640 / 0.2737\n",
      "[0/1][681/938] Loss_D: 0.7341 Loss_G: 2.1891 D(x): 0.8338 D(G(z)): 0.3308 / 0.1676\n",
      "[0/1][682/938] Loss_D: 0.7295 Loss_G: 2.4308 D(x): 0.7553 D(G(z)): 0.2943 / 0.1106\n",
      "[0/1][683/938] Loss_D: 0.8450 Loss_G: 1.2493 D(x): 0.6263 D(G(z)): 0.2184 / 0.3458\n",
      "[0/1][684/938] Loss_D: 0.8777 Loss_G: 3.5091 D(x): 0.8802 D(G(z)): 0.4783 / 0.0460\n",
      "[0/1][685/938] Loss_D: 0.9768 Loss_G: 1.3203 D(x): 0.4906 D(G(z)): 0.1135 / 0.3240\n",
      "[0/1][686/938] Loss_D: 0.7775 Loss_G: 2.5403 D(x): 0.8545 D(G(z)): 0.4199 / 0.0974\n",
      "[0/1][687/938] Loss_D: 0.7639 Loss_G: 1.6159 D(x): 0.6357 D(G(z)): 0.2131 / 0.2467\n",
      "[0/1][688/938] Loss_D: 0.6299 Loss_G: 2.2440 D(x): 0.7992 D(G(z)): 0.2894 / 0.1370\n",
      "[0/1][689/938] Loss_D: 0.7104 Loss_G: 1.5518 D(x): 0.6761 D(G(z)): 0.2086 / 0.2630\n",
      "[0/1][690/938] Loss_D: 0.6837 Loss_G: 2.9103 D(x): 0.8476 D(G(z)): 0.3599 / 0.0668\n",
      "[0/1][691/938] Loss_D: 0.7265 Loss_G: 1.1676 D(x): 0.6223 D(G(z)): 0.1433 / 0.3636\n",
      "[0/1][692/938] Loss_D: 0.6654 Loss_G: 3.0312 D(x): 0.9113 D(G(z)): 0.3890 / 0.0621\n",
      "[0/1][693/938] Loss_D: 0.7911 Loss_G: 1.9969 D(x): 0.5735 D(G(z)): 0.1266 / 0.1770\n",
      "[0/1][694/938] Loss_D: 0.6587 Loss_G: 2.0230 D(x): 0.7846 D(G(z)): 0.2908 / 0.1576\n",
      "[0/1][695/938] Loss_D: 0.7600 Loss_G: 1.1384 D(x): 0.6695 D(G(z)): 0.2243 / 0.3822\n",
      "[0/1][696/938] Loss_D: 0.9540 Loss_G: 4.8195 D(x): 0.9033 D(G(z)): 0.5348 / 0.0109\n",
      "[0/1][697/938] Loss_D: 1.6075 Loss_G: 0.5538 D(x): 0.2482 D(G(z)): 0.0382 / 0.6318\n",
      "[0/1][698/938] Loss_D: 1.7093 Loss_G: 4.5500 D(x): 0.9643 D(G(z)): 0.7459 / 0.0142\n",
      "[0/1][699/938] Loss_D: 1.9705 Loss_G: 0.5750 D(x): 0.1986 D(G(z)): 0.0402 / 0.6057\n",
      "[0/1][700/938] Loss_D: 1.3149 Loss_G: 3.2656 D(x): 0.9261 D(G(z)): 0.6419 / 0.0565\n",
      "[0/1][701/938] Loss_D: 1.3017 Loss_G: 0.8883 D(x): 0.3845 D(G(z)): 0.1663 / 0.4705\n",
      "[0/1][702/938] Loss_D: 0.9787 Loss_G: 1.9597 D(x): 0.8098 D(G(z)): 0.4656 / 0.1784\n",
      "[0/1][703/938] Loss_D: 0.9499 Loss_G: 1.4423 D(x): 0.5902 D(G(z)): 0.2465 / 0.2855\n",
      "[0/1][704/938] Loss_D: 0.9695 Loss_G: 2.1729 D(x): 0.7688 D(G(z)): 0.4394 / 0.1584\n",
      "[0/1][705/938] Loss_D: 0.8209 Loss_G: 1.5719 D(x): 0.6354 D(G(z)): 0.2410 / 0.2694\n",
      "[0/1][706/938] Loss_D: 1.0763 Loss_G: 1.4385 D(x): 0.6532 D(G(z)): 0.4031 / 0.2664\n",
      "[0/1][707/938] Loss_D: 0.7203 Loss_G: 1.8897 D(x): 0.7253 D(G(z)): 0.2825 / 0.1771\n",
      "[0/1][708/938] Loss_D: 0.8300 Loss_G: 1.4635 D(x): 0.6530 D(G(z)): 0.2746 / 0.2783\n",
      "[0/1][709/938] Loss_D: 0.7786 Loss_G: 2.2300 D(x): 0.7702 D(G(z)): 0.3525 / 0.1408\n",
      "[0/1][710/938] Loss_D: 0.6061 Loss_G: 1.7524 D(x): 0.7187 D(G(z)): 0.1998 / 0.2240\n",
      "[0/1][711/938] Loss_D: 0.6786 Loss_G: 1.8981 D(x): 0.7794 D(G(z)): 0.3019 / 0.1741\n",
      "[0/1][712/938] Loss_D: 0.6986 Loss_G: 2.1575 D(x): 0.7379 D(G(z)): 0.2875 / 0.1414\n",
      "[0/1][713/938] Loss_D: 0.6844 Loss_G: 1.1512 D(x): 0.6324 D(G(z)): 0.1609 / 0.3672\n",
      "[0/1][714/938] Loss_D: 0.8117 Loss_G: 3.3249 D(x): 0.8988 D(G(z)): 0.4647 / 0.0553\n",
      "[0/1][715/938] Loss_D: 1.1938 Loss_G: 0.8905 D(x): 0.4188 D(G(z)): 0.1157 / 0.4904\n",
      "[0/1][716/938] Loss_D: 0.9222 Loss_G: 3.4333 D(x): 0.9010 D(G(z)): 0.4890 / 0.0446\n",
      "[0/1][717/938] Loss_D: 1.0675 Loss_G: 0.6579 D(x): 0.4617 D(G(z)): 0.1231 / 0.5564\n",
      "[0/1][718/938] Loss_D: 1.1933 Loss_G: 3.6819 D(x): 0.9476 D(G(z)): 0.6240 / 0.0348\n",
      "[0/1][719/938] Loss_D: 1.3861 Loss_G: 0.9682 D(x): 0.3481 D(G(z)): 0.0993 / 0.4380\n",
      "[0/1][720/938] Loss_D: 0.9574 Loss_G: 3.1106 D(x): 0.9104 D(G(z)): 0.5100 / 0.0652\n",
      "[0/1][721/938] Loss_D: 1.0575 Loss_G: 0.9255 D(x): 0.4703 D(G(z)): 0.1334 / 0.4878\n",
      "[0/1][722/938] Loss_D: 1.4000 Loss_G: 3.8268 D(x): 0.9164 D(G(z)): 0.6414 / 0.0380\n",
      "[0/1][723/938] Loss_D: 1.2795 Loss_G: 0.8719 D(x): 0.3497 D(G(z)): 0.0735 / 0.4590\n",
      "[0/1][724/938] Loss_D: 1.0091 Loss_G: 3.3528 D(x): 0.9237 D(G(z)): 0.5425 / 0.0479\n",
      "[0/1][725/938] Loss_D: 1.0436 Loss_G: 0.9928 D(x): 0.4608 D(G(z)): 0.0884 / 0.4157\n",
      "[0/1][726/938] Loss_D: 0.8492 Loss_G: 2.5432 D(x): 0.9051 D(G(z)): 0.4802 / 0.0952\n",
      "[0/1][727/938] Loss_D: 0.7581 Loss_G: 1.8992 D(x): 0.6312 D(G(z)): 0.2066 / 0.1824\n",
      "[0/1][728/938] Loss_D: 0.6307 Loss_G: 1.5390 D(x): 0.7141 D(G(z)): 0.2124 / 0.2500\n",
      "[0/1][729/938] Loss_D: 0.8344 Loss_G: 2.5789 D(x): 0.8121 D(G(z)): 0.4169 / 0.0911\n",
      "[0/1][730/938] Loss_D: 0.8936 Loss_G: 1.5493 D(x): 0.5861 D(G(z)): 0.2109 / 0.2605\n",
      "[0/1][731/938] Loss_D: 0.5919 Loss_G: 1.9925 D(x): 0.7929 D(G(z)): 0.2615 / 0.1662\n",
      "[0/1][732/938] Loss_D: 0.6919 Loss_G: 2.1231 D(x): 0.7739 D(G(z)): 0.2943 / 0.1473\n",
      "[0/1][733/938] Loss_D: 0.5264 Loss_G: 2.3840 D(x): 0.8085 D(G(z)): 0.2366 / 0.1238\n",
      "[0/1][734/938] Loss_D: 0.6412 Loss_G: 1.4392 D(x): 0.6712 D(G(z)): 0.1564 / 0.2822\n",
      "[0/1][735/938] Loss_D: 0.8976 Loss_G: 4.1522 D(x): 0.9124 D(G(z)): 0.5099 / 0.0224\n",
      "[0/1][736/938] Loss_D: 1.2319 Loss_G: 1.1490 D(x): 0.3833 D(G(z)): 0.0442 / 0.3808\n",
      "[0/1][737/938] Loss_D: 0.5670 Loss_G: 3.6292 D(x): 0.9170 D(G(z)): 0.3471 / 0.0355\n",
      "[0/1][738/938] Loss_D: 0.9361 Loss_G: 0.4981 D(x): 0.5124 D(G(z)): 0.1577 / 0.6506\n",
      "[0/1][739/938] Loss_D: 1.6152 Loss_G: 4.7680 D(x): 0.8987 D(G(z)): 0.7111 / 0.0145\n",
      "[0/1][740/938] Loss_D: 2.1944 Loss_G: 0.5105 D(x): 0.1754 D(G(z)): 0.0556 / 0.6621\n",
      "[0/1][741/938] Loss_D: 1.8402 Loss_G: 3.0048 D(x): 0.9108 D(G(z)): 0.7408 / 0.0853\n",
      "[0/1][742/938] Loss_D: 0.9536 Loss_G: 1.8892 D(x): 0.5362 D(G(z)): 0.1771 / 0.2106\n",
      "[0/1][743/938] Loss_D: 0.8739 Loss_G: 1.7075 D(x): 0.7079 D(G(z)): 0.3241 / 0.2379\n",
      "[0/1][744/938] Loss_D: 0.8313 Loss_G: 2.8367 D(x): 0.8097 D(G(z)): 0.4019 / 0.0836\n",
      "[0/1][745/938] Loss_D: 1.0933 Loss_G: 1.3943 D(x): 0.4648 D(G(z)): 0.1617 / 0.3031\n",
      "[0/1][746/938] Loss_D: 0.8195 Loss_G: 2.8834 D(x): 0.8364 D(G(z)): 0.4273 / 0.0737\n",
      "[0/1][747/938] Loss_D: 1.0971 Loss_G: 0.6634 D(x): 0.4668 D(G(z)): 0.1695 / 0.5729\n",
      "[0/1][748/938] Loss_D: 1.4868 Loss_G: 3.4453 D(x): 0.8786 D(G(z)): 0.6806 / 0.0492\n",
      "[0/1][749/938] Loss_D: 1.3608 Loss_G: 1.2255 D(x): 0.3541 D(G(z)): 0.0825 / 0.3671\n",
      "[0/1][750/938] Loss_D: 0.7604 Loss_G: 2.1772 D(x): 0.8810 D(G(z)): 0.4184 / 0.1439\n",
      "[0/1][751/938] Loss_D: 0.7878 Loss_G: 2.5147 D(x): 0.7496 D(G(z)): 0.3344 / 0.1067\n",
      "[0/1][752/938] Loss_D: 0.8038 Loss_G: 1.0271 D(x): 0.5712 D(G(z)): 0.1429 / 0.4145\n",
      "[0/1][753/938] Loss_D: 1.0295 Loss_G: 3.3686 D(x): 0.8888 D(G(z)): 0.5255 / 0.0494\n",
      "[0/1][754/938] Loss_D: 0.9979 Loss_G: 1.0951 D(x): 0.4663 D(G(z)): 0.1019 / 0.3656\n",
      "[0/1][755/938] Loss_D: 0.6767 Loss_G: 2.3645 D(x): 0.8947 D(G(z)): 0.4059 / 0.1166\n",
      "[0/1][756/938] Loss_D: 0.5945 Loss_G: 1.9482 D(x): 0.7105 D(G(z)): 0.1790 / 0.1697\n",
      "[0/1][757/938] Loss_D: 0.6978 Loss_G: 1.7271 D(x): 0.7425 D(G(z)): 0.2849 / 0.2046\n",
      "[0/1][758/938] Loss_D: 0.6966 Loss_G: 3.1213 D(x): 0.8318 D(G(z)): 0.3555 / 0.0623\n",
      "[0/1][759/938] Loss_D: 1.1284 Loss_G: 0.6556 D(x): 0.4447 D(G(z)): 0.1272 / 0.5511\n",
      "[0/1][760/938] Loss_D: 0.8528 Loss_G: 3.7666 D(x): 0.9321 D(G(z)): 0.4891 / 0.0391\n",
      "[0/1][761/938] Loss_D: 0.9177 Loss_G: 1.5307 D(x): 0.5292 D(G(z)): 0.1217 / 0.2591\n",
      "[0/1][762/938] Loss_D: 0.8136 Loss_G: 2.8424 D(x): 0.8474 D(G(z)): 0.4184 / 0.0790\n",
      "[0/1][763/938] Loss_D: 0.7788 Loss_G: 1.4679 D(x): 0.6083 D(G(z)): 0.1877 / 0.2845\n",
      "[0/1][764/938] Loss_D: 0.8524 Loss_G: 3.0751 D(x): 0.8446 D(G(z)): 0.4313 / 0.0575\n",
      "[0/1][765/938] Loss_D: 0.9222 Loss_G: 0.8007 D(x): 0.5174 D(G(z)): 0.1295 / 0.4877\n",
      "[0/1][766/938] Loss_D: 1.0357 Loss_G: 3.7737 D(x): 0.9116 D(G(z)): 0.5420 / 0.0394\n",
      "[0/1][767/938] Loss_D: 1.1004 Loss_G: 1.4486 D(x): 0.4236 D(G(z)): 0.0608 / 0.3088\n",
      "[0/1][768/938] Loss_D: 0.7320 Loss_G: 3.7158 D(x): 0.9088 D(G(z)): 0.4106 / 0.0340\n",
      "[0/1][769/938] Loss_D: 1.1349 Loss_G: 0.5338 D(x): 0.4378 D(G(z)): 0.1219 / 0.6286\n",
      "[0/1][770/938] Loss_D: 1.3311 Loss_G: 3.4206 D(x): 0.9422 D(G(z)): 0.6581 / 0.0539\n",
      "[0/1][771/938] Loss_D: 0.9577 Loss_G: 1.4831 D(x): 0.5106 D(G(z)): 0.1061 / 0.2748\n",
      "[0/1][772/938] Loss_D: 0.8316 Loss_G: 2.2604 D(x): 0.8013 D(G(z)): 0.3885 / 0.1236\n",
      "[0/1][773/938] Loss_D: 0.7585 Loss_G: 1.3460 D(x): 0.6277 D(G(z)): 0.1863 / 0.2970\n",
      "[0/1][774/938] Loss_D: 0.6101 Loss_G: 2.7114 D(x): 0.8750 D(G(z)): 0.3393 / 0.0913\n",
      "[0/1][775/938] Loss_D: 0.4959 Loss_G: 2.2803 D(x): 0.7601 D(G(z)): 0.1691 / 0.1338\n",
      "[0/1][776/938] Loss_D: 0.5831 Loss_G: 1.9042 D(x): 0.7570 D(G(z)): 0.2179 / 0.1924\n",
      "[0/1][777/938] Loss_D: 0.5841 Loss_G: 2.4399 D(x): 0.8173 D(G(z)): 0.2855 / 0.1059\n",
      "[0/1][778/938] Loss_D: 0.6084 Loss_G: 1.6932 D(x): 0.7138 D(G(z)): 0.1741 / 0.2268\n",
      "[0/1][779/938] Loss_D: 0.7291 Loss_G: 3.0762 D(x): 0.8243 D(G(z)): 0.3441 / 0.0637\n",
      "[0/1][780/938] Loss_D: 0.5487 Loss_G: 1.6082 D(x): 0.6718 D(G(z)): 0.0854 / 0.2531\n",
      "[0/1][781/938] Loss_D: 0.9111 Loss_G: 2.6669 D(x): 0.7915 D(G(z)): 0.4388 / 0.0943\n",
      "[0/1][782/938] Loss_D: 0.7977 Loss_G: 1.3710 D(x): 0.5859 D(G(z)): 0.1460 / 0.3126\n",
      "[0/1][783/938] Loss_D: 0.9990 Loss_G: 5.2132 D(x): 0.9092 D(G(z)): 0.5287 / 0.0081\n",
      "[0/1][784/938] Loss_D: 2.5680 Loss_G: 0.2981 D(x): 0.1078 D(G(z)): 0.0185 / 0.7841\n",
      "[0/1][785/938] Loss_D: 2.3877 Loss_G: 4.1827 D(x): 0.9646 D(G(z)): 0.8700 / 0.0278\n",
      "[0/1][786/938] Loss_D: 1.7393 Loss_G: 0.6678 D(x): 0.2754 D(G(z)): 0.0718 / 0.5676\n",
      "[0/1][787/938] Loss_D: 1.4396 Loss_G: 3.7439 D(x): 0.9212 D(G(z)): 0.6686 / 0.0337\n",
      "[0/1][788/938] Loss_D: 1.6289 Loss_G: 0.7516 D(x): 0.2834 D(G(z)): 0.0695 / 0.5148\n",
      "[0/1][789/938] Loss_D: 1.4270 Loss_G: 2.8142 D(x): 0.9256 D(G(z)): 0.6504 / 0.1025\n",
      "[0/1][790/938] Loss_D: 1.2579 Loss_G: 1.0911 D(x): 0.4432 D(G(z)): 0.1710 / 0.3889\n",
      "[0/1][791/938] Loss_D: 1.0244 Loss_G: 2.0422 D(x): 0.8068 D(G(z)): 0.4794 / 0.1620\n",
      "[0/1][792/938] Loss_D: 0.8637 Loss_G: 1.7482 D(x): 0.6164 D(G(z)): 0.2287 / 0.2278\n",
      "[0/1][793/938] Loss_D: 0.8612 Loss_G: 1.3808 D(x): 0.6805 D(G(z)): 0.3157 / 0.2817\n",
      "[0/1][794/938] Loss_D: 0.7994 Loss_G: 1.9963 D(x): 0.7629 D(G(z)): 0.3711 / 0.1624\n",
      "[0/1][795/938] Loss_D: 0.7253 Loss_G: 2.1021 D(x): 0.7209 D(G(z)): 0.2817 / 0.1566\n",
      "[0/1][796/938] Loss_D: 0.8435 Loss_G: 0.9943 D(x): 0.5990 D(G(z)): 0.2186 / 0.4248\n",
      "[0/1][797/938] Loss_D: 0.9541 Loss_G: 2.7278 D(x): 0.8547 D(G(z)): 0.4738 / 0.0865\n",
      "[0/1][798/938] Loss_D: 1.0665 Loss_G: 1.3473 D(x): 0.4544 D(G(z)): 0.1378 / 0.3166\n",
      "[0/1][799/938] Loss_D: 0.7362 Loss_G: 1.9280 D(x): 0.8021 D(G(z)): 0.3479 / 0.1748\n",
      "[0/1][800/938] Loss_D: 1.0574 Loss_G: 1.0816 D(x): 0.5623 D(G(z)): 0.3044 / 0.3803\n",
      "[0/1][801/938] Loss_D: 0.7725 Loss_G: 2.6675 D(x): 0.8724 D(G(z)): 0.4399 / 0.0893\n",
      "[0/1][802/938] Loss_D: 0.9682 Loss_G: 0.9452 D(x): 0.5092 D(G(z)): 0.1589 / 0.4252\n",
      "[0/1][803/938] Loss_D: 1.0211 Loss_G: 2.5237 D(x): 0.8366 D(G(z)): 0.5164 / 0.0965\n",
      "[0/1][804/938] Loss_D: 0.7292 Loss_G: 1.7309 D(x): 0.6345 D(G(z)): 0.1771 / 0.2146\n",
      "[0/1][805/938] Loss_D: 0.6557 Loss_G: 1.8845 D(x): 0.7933 D(G(z)): 0.3010 / 0.1850\n",
      "[0/1][806/938] Loss_D: 0.6560 Loss_G: 2.6388 D(x): 0.7920 D(G(z)): 0.3058 / 0.0901\n",
      "[0/1][807/938] Loss_D: 0.8381 Loss_G: 1.4709 D(x): 0.5903 D(G(z)): 0.1917 / 0.2974\n",
      "[0/1][808/938] Loss_D: 0.6903 Loss_G: 2.4211 D(x): 0.8281 D(G(z)): 0.3592 / 0.1101\n",
      "[0/1][809/938] Loss_D: 0.7740 Loss_G: 2.5984 D(x): 0.7078 D(G(z)): 0.2573 / 0.0959\n",
      "[0/1][810/938] Loss_D: 0.6552 Loss_G: 1.7827 D(x): 0.6939 D(G(z)): 0.1961 / 0.2002\n",
      "[0/1][811/938] Loss_D: 0.4985 Loss_G: 1.8395 D(x): 0.8045 D(G(z)): 0.2185 / 0.1832\n",
      "[0/1][812/938] Loss_D: 0.5948 Loss_G: 1.7748 D(x): 0.7526 D(G(z)): 0.2384 / 0.1915\n",
      "[0/1][813/938] Loss_D: 0.6154 Loss_G: 3.8781 D(x): 0.9105 D(G(z)): 0.3757 / 0.0267\n",
      "[0/1][814/938] Loss_D: 1.3773 Loss_G: 0.5745 D(x): 0.3312 D(G(z)): 0.0602 / 0.5993\n",
      "[0/1][815/938] Loss_D: 1.2713 Loss_G: 5.3960 D(x): 0.9524 D(G(z)): 0.6458 / 0.0067\n",
      "[0/1][816/938] Loss_D: 2.6634 Loss_G: 0.4608 D(x): 0.1116 D(G(z)): 0.0323 / 0.6796\n",
      "[0/1][817/938] Loss_D: 1.5948 Loss_G: 4.1591 D(x): 0.9638 D(G(z)): 0.7297 / 0.0269\n",
      "[0/1][818/938] Loss_D: 1.8916 Loss_G: 0.7678 D(x): 0.2472 D(G(z)): 0.0806 / 0.5362\n",
      "[0/1][819/938] Loss_D: 1.4422 Loss_G: 1.8051 D(x): 0.8362 D(G(z)): 0.6283 / 0.2039\n",
      "[0/1][820/938] Loss_D: 1.0396 Loss_G: 1.9627 D(x): 0.6004 D(G(z)): 0.3399 / 0.1879\n",
      "[0/1][821/938] Loss_D: 1.0024 Loss_G: 0.9956 D(x): 0.5280 D(G(z)): 0.2306 / 0.4195\n",
      "[0/1][822/938] Loss_D: 1.4107 Loss_G: 2.1309 D(x): 0.7976 D(G(z)): 0.6268 / 0.1461\n",
      "[0/1][823/938] Loss_D: 0.9416 Loss_G: 1.6603 D(x): 0.5713 D(G(z)): 0.2326 / 0.2264\n",
      "[0/1][824/938] Loss_D: 0.8739 Loss_G: 1.3056 D(x): 0.6501 D(G(z)): 0.2997 / 0.3151\n",
      "[0/1][825/938] Loss_D: 0.9680 Loss_G: 1.7857 D(x): 0.7342 D(G(z)): 0.3949 / 0.2122\n",
      "[0/1][826/938] Loss_D: 1.0135 Loss_G: 1.5409 D(x): 0.6085 D(G(z)): 0.3363 / 0.2391\n",
      "[0/1][827/938] Loss_D: 0.7378 Loss_G: 1.7953 D(x): 0.7129 D(G(z)): 0.2862 / 0.1961\n",
      "[0/1][828/938] Loss_D: 0.9223 Loss_G: 1.7942 D(x): 0.6968 D(G(z)): 0.3766 / 0.2031\n",
      "[0/1][829/938] Loss_D: 0.8446 Loss_G: 1.7564 D(x): 0.6772 D(G(z)): 0.2890 / 0.2263\n",
      "[0/1][830/938] Loss_D: 0.7601 Loss_G: 2.0983 D(x): 0.7698 D(G(z)): 0.3376 / 0.1546\n",
      "[0/1][831/938] Loss_D: 0.6764 Loss_G: 1.4680 D(x): 0.6717 D(G(z)): 0.1855 / 0.2720\n",
      "[0/1][832/938] Loss_D: 0.8380 Loss_G: 2.2573 D(x): 0.7879 D(G(z)): 0.4108 / 0.1268\n",
      "[0/1][833/938] Loss_D: 0.6801 Loss_G: 1.3454 D(x): 0.6261 D(G(z)): 0.1407 / 0.3024\n",
      "[0/1][834/938] Loss_D: 0.9029 Loss_G: 2.8306 D(x): 0.8490 D(G(z)): 0.4753 / 0.0718\n",
      "[0/1][835/938] Loss_D: 1.1100 Loss_G: 0.8509 D(x): 0.4319 D(G(z)): 0.1463 / 0.4730\n",
      "[0/1][836/938] Loss_D: 0.9141 Loss_G: 3.4760 D(x): 0.9242 D(G(z)): 0.5165 / 0.0426\n",
      "[0/1][837/938] Loss_D: 0.9327 Loss_G: 1.1390 D(x): 0.5179 D(G(z)): 0.1530 / 0.3562\n",
      "[0/1][838/938] Loss_D: 0.9175 Loss_G: 2.7990 D(x): 0.8322 D(G(z)): 0.4644 / 0.0866\n",
      "[0/1][839/938] Loss_D: 0.8122 Loss_G: 1.2491 D(x): 0.5772 D(G(z)): 0.1520 / 0.3223\n",
      "[0/1][840/938] Loss_D: 0.6838 Loss_G: 2.5715 D(x): 0.8614 D(G(z)): 0.3752 / 0.0973\n",
      "[0/1][841/938] Loss_D: 0.6867 Loss_G: 1.9418 D(x): 0.6783 D(G(z)): 0.1957 / 0.1861\n",
      "[0/1][842/938] Loss_D: 0.5546 Loss_G: 2.5967 D(x): 0.8313 D(G(z)): 0.2711 / 0.0937\n",
      "[0/1][843/938] Loss_D: 0.7150 Loss_G: 1.4094 D(x): 0.6357 D(G(z)): 0.1625 / 0.3123\n",
      "[0/1][844/938] Loss_D: 0.8391 Loss_G: 3.7908 D(x): 0.8726 D(G(z)): 0.4404 / 0.0336\n",
      "[0/1][845/938] Loss_D: 1.1254 Loss_G: 0.5772 D(x): 0.4011 D(G(z)): 0.0808 / 0.6196\n",
      "[0/1][846/938] Loss_D: 1.2305 Loss_G: 4.2078 D(x): 0.9495 D(G(z)): 0.6110 / 0.0249\n",
      "[0/1][847/938] Loss_D: 1.8688 Loss_G: 0.2824 D(x): 0.2295 D(G(z)): 0.0617 / 0.7730\n",
      "[0/1][848/938] Loss_D: 1.9080 Loss_G: 3.7353 D(x): 0.9624 D(G(z)): 0.7857 / 0.0315\n",
      "[0/1][849/938] Loss_D: 1.2716 Loss_G: 1.1792 D(x): 0.3669 D(G(z)): 0.0868 / 0.3634\n",
      "[0/1][850/938] Loss_D: 0.8929 Loss_G: 1.8444 D(x): 0.7948 D(G(z)): 0.4277 / 0.2023\n",
      "[0/1][851/938] Loss_D: 0.8388 Loss_G: 2.1047 D(x): 0.7232 D(G(z)): 0.3201 / 0.1671\n",
      "[0/1][852/938] Loss_D: 0.7339 Loss_G: 1.4865 D(x): 0.6465 D(G(z)): 0.1986 / 0.2580\n",
      "[0/1][853/938] Loss_D: 0.8370 Loss_G: 2.5502 D(x): 0.8325 D(G(z)): 0.4429 / 0.0950\n",
      "[0/1][854/938] Loss_D: 0.8624 Loss_G: 1.2868 D(x): 0.5391 D(G(z)): 0.1600 / 0.3296\n",
      "[0/1][855/938] Loss_D: 0.7146 Loss_G: 2.7547 D(x): 0.8956 D(G(z)): 0.4120 / 0.0829\n",
      "[0/1][856/938] Loss_D: 0.9441 Loss_G: 1.2611 D(x): 0.5409 D(G(z)): 0.2002 / 0.3185\n",
      "[0/1][857/938] Loss_D: 0.6931 Loss_G: 2.3518 D(x): 0.8080 D(G(z)): 0.3424 / 0.1272\n",
      "[0/1][858/938] Loss_D: 0.6269 Loss_G: 1.6638 D(x): 0.6847 D(G(z)): 0.1652 / 0.2208\n",
      "[0/1][859/938] Loss_D: 0.8105 Loss_G: 3.1759 D(x): 0.8504 D(G(z)): 0.4285 / 0.0565\n",
      "[0/1][860/938] Loss_D: 0.8336 Loss_G: 1.0271 D(x): 0.5449 D(G(z)): 0.1153 / 0.4174\n",
      "[0/1][861/938] Loss_D: 1.2235 Loss_G: 3.8738 D(x): 0.8796 D(G(z)): 0.6007 / 0.0353\n",
      "[0/1][862/938] Loss_D: 1.4326 Loss_G: 0.8723 D(x): 0.3250 D(G(z)): 0.0832 / 0.4630\n",
      "[0/1][863/938] Loss_D: 0.9310 Loss_G: 2.7725 D(x): 0.8997 D(G(z)): 0.5118 / 0.0794\n",
      "[0/1][864/938] Loss_D: 0.8266 Loss_G: 2.0584 D(x): 0.6205 D(G(z)): 0.2162 / 0.1863\n",
      "[0/1][865/938] Loss_D: 0.7688 Loss_G: 1.6043 D(x): 0.6735 D(G(z)): 0.2355 / 0.2441\n",
      "[0/1][866/938] Loss_D: 0.9174 Loss_G: 2.6706 D(x): 0.7552 D(G(z)): 0.4122 / 0.0870\n",
      "[0/1][867/938] Loss_D: 0.7484 Loss_G: 1.1896 D(x): 0.6101 D(G(z)): 0.1395 / 0.3536\n",
      "[0/1][868/938] Loss_D: 0.8539 Loss_G: 3.5353 D(x): 0.9380 D(G(z)): 0.4982 / 0.0410\n",
      "[0/1][869/938] Loss_D: 0.8479 Loss_G: 1.3739 D(x): 0.5439 D(G(z)): 0.1089 / 0.3108\n",
      "[0/1][870/938] Loss_D: 0.7980 Loss_G: 3.2891 D(x): 0.8710 D(G(z)): 0.4368 / 0.0490\n",
      "[0/1][871/938] Loss_D: 0.7952 Loss_G: 1.2493 D(x): 0.5296 D(G(z)): 0.0925 / 0.3280\n",
      "[0/1][872/938] Loss_D: 0.7848 Loss_G: 3.2482 D(x): 0.8710 D(G(z)): 0.4351 / 0.0478\n",
      "[0/1][873/938] Loss_D: 0.6030 Loss_G: 1.6957 D(x): 0.6409 D(G(z)): 0.1003 / 0.2279\n",
      "[0/1][874/938] Loss_D: 0.9416 Loss_G: 3.6061 D(x): 0.8569 D(G(z)): 0.4871 / 0.0442\n",
      "[0/1][875/938] Loss_D: 1.0613 Loss_G: 1.1559 D(x): 0.4666 D(G(z)): 0.1072 / 0.3770\n",
      "[0/1][876/938] Loss_D: 0.6552 Loss_G: 3.3811 D(x): 0.9363 D(G(z)): 0.4078 / 0.0454\n",
      "[0/1][877/938] Loss_D: 0.6525 Loss_G: 1.6877 D(x): 0.6321 D(G(z)): 0.1160 / 0.2259\n",
      "[0/1][878/938] Loss_D: 0.6545 Loss_G: 3.2851 D(x): 0.8925 D(G(z)): 0.3807 / 0.0535\n",
      "[0/1][879/938] Loss_D: 0.7725 Loss_G: 1.4242 D(x): 0.5812 D(G(z)): 0.1246 / 0.3068\n",
      "[0/1][880/938] Loss_D: 0.6914 Loss_G: 3.3712 D(x): 0.8789 D(G(z)): 0.3812 / 0.0477\n",
      "[0/1][881/938] Loss_D: 0.9176 Loss_G: 1.0432 D(x): 0.5390 D(G(z)): 0.1263 / 0.4131\n",
      "[0/1][882/938] Loss_D: 1.0596 Loss_G: 4.7096 D(x): 0.9493 D(G(z)): 0.5614 / 0.0134\n",
      "[0/1][883/938] Loss_D: 1.3667 Loss_G: 0.8922 D(x): 0.3192 D(G(z)): 0.0329 / 0.4558\n",
      "[0/1][884/938] Loss_D: 0.8928 Loss_G: 3.5295 D(x): 0.9252 D(G(z)): 0.5029 / 0.0470\n",
      "[0/1][885/938] Loss_D: 0.9293 Loss_G: 1.5353 D(x): 0.5486 D(G(z)): 0.1673 / 0.2692\n",
      "[0/1][886/938] Loss_D: 0.8987 Loss_G: 2.9094 D(x): 0.8074 D(G(z)): 0.4433 / 0.0686\n",
      "[0/1][887/938] Loss_D: 1.1444 Loss_G: 0.7464 D(x): 0.4448 D(G(z)): 0.1922 / 0.5057\n",
      "[0/1][888/938] Loss_D: 1.0852 Loss_G: 3.8331 D(x): 0.9151 D(G(z)): 0.5687 / 0.0297\n"
     ]
    }
   ],
   "source": [
    "# This is the engine of the code base - explicitly taking the objects created above \n",
    "# (The generator, discrimator and the dataset) and connecting them together to learn.\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        \n",
    "        # Set the descrimator to forget any gradients.\n",
    "        netD.zero_grad()\n",
    "        # Get a sample of real handwritten digits and label them as 1 - all real\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=real_cpu.dtype, device=device)\n",
    "        # Pass the sample through the discrimator\n",
    "        output = netD(real_cpu)\n",
    "        # measure the error\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate the gradients of each layer of the network\n",
    "        errD_real.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        # pass the noise through the generator layers\n",
    "        fake = netG(noise)\n",
    "        # set the labels to all 0 - fake\n",
    "        label.fill_(fake_label)\n",
    "        # ask the discrimator to judge the fake images\n",
    "        output = netD(fake.detach())\n",
    "        # measure the error\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients \n",
    "        errD_fake.backward()\n",
    "        # Get the average output across the batch again\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Get the error\n",
    "        errD = errD_real + errD_fake\n",
    "        # Run the optimizer to update the weights\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        # Set the gradients of the generator to zero\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # get the judgements from the discrimator of the generator output is fake\n",
    "        output = netD(fake)\n",
    "        # calculate the error\n",
    "        errG = criterion(output, label)\n",
    "        # update the gradients\n",
    "        errG.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # update the weights\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, 1, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        # every 100 steps save a real sample and a fake sample for comparison\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,'real_samples.png',normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),'fake_samples_epoch_%03d.png' % epoch, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
